{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(app-lab)=\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **neural** package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the library tree is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "lib_nn\n",
    "└── neural\n",
    "    ├── __init__.py\n",
    "    ├── draw.py\n",
    "    └── func.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **func.py** module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "\"\"\"\n",
    "Contains functions repeatedy used in the lecture\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def step(s):\n",
    "    \"\"\" step \"\"\" # komentarz w potrójnym cudzysłowie\n",
    "    if s>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def neuron(x,w,f=step): # (in the neural library)\n",
    "    \"\"\"\n",
    "    MCP neuron\n",
    "\n",
    "    x: array of inputs  [x1, x2,...,xn]\n",
    "    w: array of weights [w0, w1, w2,...,wn]\n",
    "    f: activation function, with step as default\n",
    "    \n",
    "    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w\n",
    "    \"\"\"\n",
    "    return f(np.dot(np.insert(x,0,1),w)) # insert x0=1, signal s=x.w, output f(s)\n",
    "    \n",
    "def sig(s,T=1):\n",
    "    \"\"\" sigmoid \"\"\"\n",
    "    return 1/(1+np.exp(-s/T))\n",
    "    \n",
    "def dsig(s, T=1):\n",
    "    \"\"\"derivative of sigmoid\"\"\"\n",
    "    return sig(s)*(1-sig(s))/T\n",
    "    \n",
    "def lin(s):\n",
    "    \"\"\" linear function \"\"\"\n",
    "    return s\n",
    "    \n",
    "def dlin(s):\n",
    "    \"\"\" derivative of linear function \"\"\"\n",
    "    return 1\n",
    "    \n",
    "def relu(s):\n",
    "    \"\"\" ReLU function \"\"\"\n",
    "    if s>0:\n",
    "        return s\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def drelu(s):\n",
    "    \"\"\" derivative of ReLU function \"\"\"\n",
    "    if s>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def lrelu(s,a=0.1):\n",
    "    \"\"\" leaky ReLU function \"\"\"\n",
    "    if s>0:\n",
    "        return s\n",
    "    else:\n",
    "        return a*s\n",
    "\n",
    "def dlrelu(s,a=0.1):\n",
    "    \"\"\" derivative of leaky ReLU function \"\"\"\n",
    "    if s>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return a\n",
    "        \n",
    "def softplus(s):\n",
    "    \"\"\" softplus function \"\"\"\n",
    "    return np.log(1+np.exp(s))\n",
    "\n",
    "def dsoftplus(s):\n",
    "    \"\"\" derivative softplus function \"\"\"\n",
    "    return 1/(1+np.exp(-s))\n",
    "\n",
    "def eucl(p1,p2): # square of the Euclidean distance\n",
    "    \"\"\"\n",
    "    Squqre of Euclidean distance between to points in 2-dim. space\n",
    "    \n",
    "    input: p1, p1 - arrays in the format [x1,x2]\n",
    "    return: square of Euclidean distance\n",
    "    \"\"\"\n",
    "    return (p1[0]-p2[0])**2+(p1[1]-p2[1])**2\n",
    "\n",
    "def l2(w0,w1,w2):\n",
    "    \"\"\"for separating line\"\"\"\n",
    "    return [-.1,1.1],[-(w0-w1*0.1)/w2,-(w0+w1*1.1)/w2]\n",
    "    \n",
    "def rn():\n",
    "    \"\"\"\n",
    "    random number from [-0.5,0.5]\n",
    "    \"\"\"\n",
    "    return np.random.rand()-0.5\n",
    "    \n",
    "def point_c():\n",
    "    \"\"\"\n",
    "    random point from a cirle\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        x=np.random.random()\n",
    "        y=np.random.random()\n",
    "        if (x-0.5)**2+(y-0.5)**2 < 0.4**2:\n",
    "            break\n",
    "    return np.array([x,y])\n",
    "    \n",
    "def point():\n",
    "    \"\"\"\n",
    "    random point from [0,1]x[0,1]\n",
    "    \"\"\"\n",
    "    x=np.random.random()\n",
    "    y=np.random.random()\n",
    "    return np.array([x,y])\n",
    "    \n",
    "def set_ran_w(ar,s=1):\n",
    "    \"\"\"\n",
    "    Set weights randomly\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    s - scale factor: each weight is in the range [-0.s, 0.5s]\n",
    "    \n",
    "    return:\n",
    "    w - dictionary of weights for neuron layers 1, 2,...,l in the format\n",
    "    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}\n",
    "    \"\"\"\n",
    "    l=len(ar)\n",
    "    w={}\n",
    "    for k in range(l-1):\n",
    "        w.update({k+1: [[s*rn() for i in range(ar[k+1])] for j in range(ar[k]+1)]})\n",
    "    return w\n",
    "\n",
    "\n",
    "def set_val_w(ar,a=0):\n",
    "    \"\"\"\n",
    "    Set weights to a constant value\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    a - value for each weight\n",
    "    \n",
    "    return:\n",
    "    w - dictionary of weights for neuron layers 1, 2,...,l in the format\n",
    "    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}\n",
    "    \"\"\"\n",
    "    l=len(ar)\n",
    "    w={}\n",
    "    for k in range(l-1):\n",
    "        w.update({k+1: [[a for i in range(ar[k+1])] for j in range(ar[k]+1)]})\n",
    "    return w\n",
    "    \n",
    "\n",
    "def feed_forward(ar, we, x_in, ff=step):\n",
    "    \"\"\"\n",
    "    Feed-forward propagation\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    we - dictionary of weights for neuron layers 1, 2,...,l in the format\n",
    "    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}\n",
    "    \n",
    "    x_in - input vector of length n_0 (bias not included)\n",
    "    \n",
    "    ff - activation function (default: step)\n",
    "    \n",
    "    return:\n",
    "    x - dictionary of signals leaving subsequent layers in the format\n",
    "    {0: array[n_0+1],...,l-1: array[n_(l-1)+1], l: array[nl]}\n",
    "    (the output layer carries no bias)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    l=len(ar)-1                   # number of neuron layers\n",
    "    x_in=np.insert(x_in,0,1)      # input, with the bias node inserted\n",
    "    \n",
    "    x={}                          # empty dictionary\n",
    "    x.update({0: np.array(x_in)}) # add input signal\n",
    "    \n",
    "    for i in range(0,l-1):        # loop over layers till before last one\n",
    "        s=np.dot(x[i],we[i+1])    # signal, matrix multiplication\n",
    "        y=[ff(s[k]) for k in range(ar[i+1])] # output from activation\n",
    "        x.update({i+1: np.insert(y,0,1)}) # add bias node and update x\n",
    "\n",
    "    # the last layer - no adding of the bias node\n",
    "    s=np.dot(x[l-1],we[l])\n",
    "    y=[ff(s[q]) for q in range(ar[l])]\n",
    "    x.update({l: y})          # update x\n",
    "          \n",
    "    return x\n",
    "\n",
    "def back_prop(fe,la, p, ar, we, eps,f=sig, df=dsig):\n",
    "    \"\"\"\n",
    "    fe - array of features\n",
    "    la - array of labels\n",
    "    p  - index of the used data point\n",
    "    ar - array of numbers of nodes in subsequent layers\n",
    "    we - disctionary of weights\n",
    "    eps - learning speed\n",
    "    f   - activation function\n",
    "    df  - derivaive of f\n",
    "    \"\"\"\n",
    " \n",
    "    l=len(ar)-1 # number of neuron layers (= index of the output layer)\n",
    "    nl=ar[l]    # number of neurons in the otput layer\n",
    "   \n",
    "    x=feed_forward(ar,we,fe[p],ff=f) # feed-forward of point p\n",
    "   \n",
    "    # formulas from the derivation in a one-to-one notation:\n",
    "    \n",
    "    D={}\n",
    "    D.update({l: [2*(x[l][gam]-la[p][gam])*\n",
    "                    df(np.dot(x[l-1],we[l])[gam]) for gam in range(nl)]})\n",
    "    we[l]-=eps*np.outer(x[l-1],D[l])\n",
    "    \n",
    "    for j in reversed(range(1,l)):\n",
    "        u=np.delete(np.dot(we[j+1],D[j+1]),0)\n",
    "        v=np.dot(x[j-1],we[j])\n",
    "        D.update({j: [u[i]*df(v[i]) for i in range(len(u))]})\n",
    "        we[j]-=eps*np.outer(x[j-1],D[j])\n",
    "\n",
    "\n",
    "def feed_forward_o(ar, we, x_in, ff=sig, ffo=lin):\n",
    "    \"\"\"\n",
    "    Feed-forward propagation with different output activation\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    we - dictionary of weights for neuron layers 1, 2,...,l in the format\n",
    "    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}\n",
    "    \n",
    "    x_in - input vector of length n_0 (bias not included)\n",
    "    \n",
    "    f  - activation function (default: sigmoid)\n",
    "    fo - activation function in the output layer (default: linear)\n",
    "    \n",
    "    return:\n",
    "    x - dictionary of signals leaving subsequent layers in the format\n",
    "    {0: array[n_0+1],...,l-1: array[n_(l-1)+1], l: array[nl]}\n",
    "    (the output layer carries no bias)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    l=len(ar)-1                   # number of neuron layers\n",
    "    x_in=np.insert(x_in,0,1)      # input, with the bias node inserted\n",
    "    \n",
    "    x={}                          # empty dictionary\n",
    "    x.update({0: np.array(x_in)}) # add input signal\n",
    "    \n",
    "    for i in range(0,l-1):        # loop over layers till before last one\n",
    "        s=np.dot(x[i],we[i+1])    # signal, matrix multiplication\n",
    "        y=[ff(s[k]) for k in range(ar[i+1])] # output from activation\n",
    "        x.update({i+1: np.insert(y,0,1)}) # add bias node and update x\n",
    "\n",
    "    # the last layer - no adding of the bias node\n",
    "    s=np.dot(x[l-1],we[l])\n",
    "    y=[ffo(s[q]) for q in range(ar[l])] # output activation function\n",
    "    x.update({l: y})                    # update x\n",
    "          \n",
    "    return x\n",
    "\n",
    "\n",
    "def back_prop_o(fe,la, p, ar, we, eps, f=sig, df=dsig, fo=lin, dfo=dlin):\n",
    "    \"\"\"\n",
    "    fe - array of features\n",
    "    la - array of labels\n",
    "    p  - index of the used data point\n",
    "    ar - array of numbers of nodes in subsequent layers\n",
    "    we - disctionary of weights\n",
    "    eps - learning speed\n",
    "    f   - activation function\n",
    "    df  - derivaive of f\n",
    "    fo  - activation function in the output layer (default: linear)\n",
    "    dfo - derivative of fo\n",
    "    \"\"\"\n",
    " \n",
    "    l=len(ar)-1 # number of neuron layers (= index of the output layer)\n",
    "    nl=ar[l]    # number of neurons in the otput layer\n",
    "   \n",
    "    x=feed_forward_o(ar,we,fe[p],ff=f,ffo=fo) # feed-forward of point p\n",
    "   \n",
    "    # formulas from the derivation in a one-to-one notation:\n",
    "    \n",
    "    D={}\n",
    "    D.update({l: [2*(x[l][gam]-la[p][gam])*\n",
    "                   dfo(np.dot(x[l-1],we[l])[gam]) for gam in range(nl)]})\n",
    "    \n",
    "    we[l]-=eps*np.outer(x[l-1],D[l])\n",
    "    \n",
    "    for j in reversed(range(1,l)):\n",
    "        u=np.delete(np.dot(we[j+1],D[j+1]),0)\n",
    "        v=np.dot(x[j-1],we[j])\n",
    "        D.update({j: [u[i]*df(v[i]) for i in range(len(u))]})\n",
    "        we[j]-=eps*np.outer(x[j-1],D[j])\n",
    "\n",
    "    \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **draw.py** module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "\"\"\"\n",
    "Plotting functions used in the lecture.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot(*args, title='activation function', x_label='signal', y_label='response',\n",
    "         start=-2, stop=2, samples=100):\n",
    "    \"\"\"\n",
    "    Wrapper on matplotlib.pyplot library.\n",
    "    Plots functions passed as *args.\n",
    "    Functions need to accept a single number argument and return a single number.\n",
    "    Example usage: plot(lambda x: x * x)\n",
    "                   plot(func.step,func.sig)\n",
    "    \"\"\"\n",
    "\n",
    "    # defines range and detail level of the plot\n",
    "    s = np.linspace(start, stop, samples)\n",
    "\n",
    "    ff=plt.figure(figsize=(2.8,2.3),dpi=120)\n",
    "    plt.title(title, fontsize=11)\n",
    "    plt.xlabel(x_label, fontsize=11)\n",
    "    plt.ylabel(y_label, fontsize=11)\n",
    "\n",
    "    for fun in args:\n",
    "        data_to_plot = [fun(x) for x in s]\n",
    "        plt.plot(s, data_to_plot)\n",
    "\n",
    "    return ff;\n",
    "\n",
    "\n",
    "def plot_net_simp(n_layer):\n",
    "    \"\"\"\n",
    "    Draw the network architecture without bias nores\n",
    "    \n",
    "    input: array of numbers of nodes in subsequent layers [n0, n1, n2,...]\n",
    "    return: graphics object\n",
    "    \"\"\"\n",
    "    l_layer=len(n_layer)\n",
    "    ff=plt.figure(figsize=(4.3,2.3),dpi=120)\n",
    "\n",
    "# input nodes\n",
    "    for j in range(n_layer[0]):\n",
    "            plt.scatter(0, j-n_layer[0]/2, s=50,c='black',zorder=10)\n",
    "\n",
    "# neuron layer nodes\n",
    "    for i in range(1,l_layer):\n",
    "        for j in range(n_layer[i]):\n",
    "            plt.scatter(i, j-n_layer[i]/2, s=100,c='blue',zorder=10)\n",
    "            \n",
    "# bias nodes\n",
    "    for k in range(n_layer[l_layer-1]):\n",
    "        plt.plot([l_layer-1,l_layer],[n_layer[l_layer-1]/2-1,n_layer[l_layer-1]/2-1], s=50,c='gray',zorder=10)\n",
    "\n",
    "# edges\n",
    "    for i in range(l_layer-1):\n",
    "        for j in range(n_layer[i]):\n",
    "            for k in range(n_layer[i+1]):\n",
    "                plt.plot([i,i+1],[j-n_layer[i]/2,k-n_layer[i+1]/2], c='gray')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    return ff;\n",
    "\n",
    "\n",
    "def plot_net(ar):\n",
    "    \"\"\"\n",
    "    Draw network without bias nodes\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    return: graphics object\n",
    "    \"\"\"\n",
    "    l=len(ar)\n",
    "    ff=plt.figure(figsize=(4.3,2.3),dpi=120)\n",
    "\n",
    "# input nodes\n",
    "    for j in range(ar[0]):\n",
    "            plt.scatter(0, j-(ar[0]-1)/2, s=50,c='black',zorder=10)\n",
    "\n",
    "# neuron layer nodes\n",
    "    for i in range(1,l):\n",
    "        for j in range(ar[i]):\n",
    "            plt.scatter(i, j-(ar[i]-1)/2, s=100,c='blue',zorder=10)\n",
    "\n",
    "# bias nodes\n",
    "    for i in range(l-1):\n",
    "            plt.scatter(i, 0-(ar[i]+1)/2, s=50,c='gray',zorder=10)\n",
    "\n",
    "# edges\n",
    "    for i in range(l-1):\n",
    "        for j in range(ar[i]+1):\n",
    "            for k in range(ar[i+1]):\n",
    "                plt.plot([i,i+1],[j-(ar[i]+1)/2,k+1-(ar[i+1]+1)/2],c='gray')\n",
    "\n",
    "# the last edge on the right\n",
    "    for j in range(ar[l-1]):\n",
    "        plt.plot([l-1,l-1+0.7],[j-(ar[l-1]-1)/2,j-(ar[l-1]-1)/2],c='gray')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    return ff;\n",
    "\n",
    "\n",
    "def plot_net_w(ar,we,wid=1):\n",
    "    \"\"\"\n",
    "    Draw the network architecture with weights\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    we - dictionary of weights for neuron layers 1, 2,...,l in the format\n",
    "    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}\n",
    "    \n",
    "    wid - controls the width of the lines\n",
    "    \n",
    "    return: graphics object\n",
    "    \"\"\"\n",
    "    l=len(ar)\n",
    "    ff=plt.figure(figsize=(4.3,2.3),dpi=120)\n",
    "    \n",
    "# input nodes\n",
    "    for j in range(ar[0]):\n",
    "            plt.scatter(0, j-(ar[0]-1)/2, s=50,c='black',zorder=10)\n",
    "\n",
    "# neuron layer nodes\n",
    "    for i in range(1,l):\n",
    "        for j in range(ar[i]):\n",
    "            plt.scatter(i, j-(ar[i]-1)/2, s=100,c='blue',zorder=10)\n",
    "\n",
    "# bias nodes\n",
    "    for i in range(l-1):\n",
    "            plt.scatter(i, 0-(ar[i]+1)/2, s=50,c='gray',zorder=10)\n",
    "\n",
    "# edges\n",
    "    for i in range(l-1):\n",
    "        for j in range(ar[i]+1):\n",
    "            for k in range(ar[i+1]):\n",
    "                th=wid*we[i+1][j][k]\n",
    "                if th>0:\n",
    "                    col='red'\n",
    "                else:\n",
    "                    col='blue'\n",
    "                th=abs(th)\n",
    "                plt.plot([i,i+1],[j-(ar[i]+1)/2,k+1-(ar[i+1]+1)/2],c=col,linewidth=th)\n",
    " \n",
    "# the last edge on the right\n",
    "    for j in range(ar[l-1]):\n",
    "        plt.plot([l-1,l-1+0.7],[j-(ar[l-1]-1)/2,j-(ar[l-1]-1)/2],c='gray')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    return ff;\n",
    "\n",
    "\n",
    "def plot_net_w_x(ar,we,wid,x):\n",
    "    \"\"\"\n",
    "    Draw the network architecture with weights and signals\n",
    "    \n",
    "    input:\n",
    "    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]\n",
    "    (from input layer 0 to output layer l, bias nodes not counted)\n",
    "    \n",
    "    we - dictionary of weights for neuron layers 1, 2,...,l in the format\n",
    "    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}\n",
    "    \n",
    "    wid - controls the width of the lines\n",
    "    \n",
    "    x - dictionary the the signal in the format\n",
    "    {0: array[n_0+1],...,l-1: array[n_(l-1)+1], l: array[nl]}\n",
    "    \n",
    "    return: graphics object\n",
    "    \"\"\"\n",
    "    l=len(ar)\n",
    "    ff=plt.figure(figsize=(4.3,2.3),dpi=120)\n",
    "    \n",
    "# input layer\n",
    "    for j in range(ar[0]):\n",
    "            plt.scatter(0, j-(ar[0]-1)/2, s=50,c='black',zorder=10)\n",
    "            lab=np.round(x[0][j+1],3)\n",
    "            plt.text(-0.27, j-(ar[0]-1)/2+0.1, lab, fontsize=7)\n",
    "\n",
    "# intermediate layer\n",
    "    for i in range(1,l-1):\n",
    "        for j in range(ar[i]):\n",
    "            plt.scatter(i, j-(ar[i]-1)/2, s=100,c='blue',zorder=10)\n",
    "            lab=np.round(x[i][j+1],3)\n",
    "            plt.text(i+0.1, j-(ar[i]-1)/2+0.1, lab, fontsize=7)\n",
    "\n",
    "# output layer\n",
    "    for j in range(ar[l-1]):\n",
    "        plt.scatter(l-1, j-(ar[l-1]-1)/2, s=100,c='blue',zorder=10)\n",
    "        lab=np.round(x[l-1][j],3)\n",
    "        plt.text(l-1+0.1, j-(ar[l-1]-1)/2+0.1, lab, fontsize=7)\n",
    "\n",
    "# bias nodes\n",
    "    for i in range(l-1):\n",
    "            plt.scatter(i, 0-(ar[i]+1)/2, s=50,c='gray',zorder=10)\n",
    "\n",
    "# edges\n",
    "    for i in range(l-1):\n",
    "        for j in range(ar[i]+1):\n",
    "            for k in range(ar[i+1]):\n",
    "                th=wid*we[i+1][j][k]\n",
    "                if th>0:\n",
    "                    col='red'\n",
    "                else:\n",
    "                    col='blue'\n",
    "                th=abs(th)\n",
    "                plt.plot([i,i+1],[j-(ar[i]+1)/2,k+1-(ar[i+1]+1)/2],c=col,linewidth=th)\n",
    " \n",
    "# the last edge on the right\n",
    "    for j in range(ar[l-1]):\n",
    "        plt.plot([l-1,l-1+0.7],[j-(ar[l-1]-1)/2,j-(ar[l-1]-1)/2],c='gray')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    return ff;\n",
    "    \n",
    "    \n",
    "def l2(w0,w1,w2):\n",
    "    return [-.1,1.1],[-(w0-w1*0.1)/w2,-(w0+w1*1.1)/w2]\n",
    "\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
