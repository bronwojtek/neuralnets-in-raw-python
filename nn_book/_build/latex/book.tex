%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        

\title{Explaining neural networks in raw Python: lectures in Jupyter}
\date{Jul 06, 2021}
\release{}
\author{Wojciech Broniowski}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{docs/index::doc}}


\sphinxAtStartPar
\sphinxhref{https://www.ujk.edu.pl/~broniows}{\sphinxstylestrong{Wojciech Broniowski}}

\sphinxAtStartPar
\sphinxhref{https://www.ifj.edu.pl}{\sphinxstylestrong{Institute of Nuclear Physics PAN}}, Kraków, and

\sphinxAtStartPar
\sphinxhref{https://www.ujk.edu.pl}{\sphinxstylestrong{Jan Kochanowski University}}, Kielce, Poland

\sphinxAtStartPar
These lectures were originally given to students of computer engineering at the \sphinxhref{https://www.ujk.edu.pl}{Jan Kochanowski University} in Kielce, Poland, and for
the \sphinxhref{https://kisd.ifj.edu.pl/news/}{Kraków School of Interdisciplinary PhD Studies}. They explain the very basic concepts of neural networks at a level requiring only very rudimentary knowledge of Python, or actually any programming language. With simplicity in mind, the code for various algorithms of neural networks is written from absolute scratch, i.e. without any use of dedicated higher\sphinxhyphen{}level libraries. That way one can follow all the programming steps in an explicit manner.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Built with \sphinxhref{https://beta.jupyterbook.org/intro.html}{Jupyter Book
2.0} tool set, as part of the
\sphinxhref{https://ebp.jupyterbook.org/en/latest/}{ExecutableBookProject}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
The reader may download the complete code in the form of
\sphinxhref{https://jupyter.org}{jupyter} notebooks as well as some supplementary material from …
\end{sphinxadmonition}

\sphinxAtStartPar
ISBN: \sphinxstylestrong{978\sphinxhyphen{}83\sphinxhyphen{}962099\sphinxhyphen{}0\sphinxhyphen{}0}

\sphinxAtStartPar
\sphinxincludegraphics{{barcode}.png}


\chapter{Introduction}
\label{\detokenize{docs/intro:introduction}}\label{\detokenize{docs/intro::doc}}

\section{Purpose of these lectures}
\label{\detokenize{docs/intro:purpose-of-these-lectures}}
\sphinxAtStartPar
The purpose of this course is to teach some basics of the omnipresent neural networks with Python. Both the explanations of key concepts of neural networks and the illustrative programs are kept at a very elementary “high\sphinxhyphen{}school” level. The code, made very simple, is described in detail. Moreover, is is written without any use of higher\sphinxhyphen{}level libraries for neural networks, which brings in better understanding of the explaned algorithms and shows how to program them from scratch.

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
\sphinxstylestrong{The reader may thus be a complete novice, only slightly acquainted with Python (or actually any other programming language) and jupyter.}
\end{sphinxadmonition}

\sphinxAtStartPar
The material covers such classic topics as the perceptron, supervised learning with back\sphinxhyphen{}propagation and data classification, unsupervised learning and clusterization, the Kohonen self\sphinxhyphen{}organizing networks, and the Hopfield networks with feedback. This aims to prepare the necessary ground for the recent and timely advancements (not covered here) in neural networks, such as deep learning, convolutional networks, recurrent networks, generative adversarial networks, reinforcement learning, etc.

\sphinxAtStartPar
On the way of the course, some basic Python programing will be gently sneaked in for the newcomers.

\begin{sphinxadmonition}{note}{Exercises}

\sphinxAtStartPar
On\sphinxhyphen{}the\sphinxhyphen{}fly exercises are included, with the goal to familiarize the reader with the covered topics. Most of exercises involve simple modifications/extensions of appropriate pieces of the lecture code.
\end{sphinxadmonition}

\sphinxAtStartPar
There are countless textbooks and lecture notes on the material of the course. With simplicity as guidance, our choice of topics took inspiration from the
lectures by \sphinxhref{http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/IntroNeuralSyllabus.html}{Daniel Kersten} and from the on\sphinxhyphen{}line book by \sphinxhref{https://page.mi.fu-berlin.de/rojas/neural/}{Raul Rojas}. Further references include… test {[}\hyperlink{cite.docs/conclusion:id2}{Gut16}{]}.


\section{Biological inspiration}
\label{\detokenize{docs/intro:biological-inspiration}}
\sphinxAtStartPar
Inspiration for computational mathematical models discussed in this course originates from the biological structure of our neural system. The central nervous system (the brain) contains a huge number (\(\sim 10^{11}\)) of \sphinxhref{https://human-memory.net/brain-neurons-synapses/}{neurons}, which may viewed as tiny elementary processor units. They receive signal via the dendrites, and in case it is strong enough, the nucleus decides (a computation done here!) to “fire” an output signal along the axon, where it is subsequently passed via axon terminals to dendrites of other neurons. The axon\sphinxhyphen{}dendrite connections (the synaptic connections) may be weak or strong, modifying the stimulus. Moreover, the strength of the synaptic connections may change in time (\sphinxhref{https://en.wikipedia.org/wiki/Hebbian\_theory}{Hebbian rule} tels that the connections get stronger if they are being used repeatedly). In this sense, the neuron is “programmable”.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=450\sphinxpxdimen]{{neuron-structure}.jpg}
\caption{Biological neuron (from \sphinxhref{https://training.seer.cancer.gov/anatomy/nervous/tissue.html}{here}).}\label{\detokenize{docs/intro:neuron-fig}}\end{figure}

\sphinxAtStartPar
We may ask ourselves if the number of neurons in the brain should really be labeled so “huge” as usually claimed. Let us compare it to the computing devices which memory chips. The number of \(10^{11}\) neurons roughly correspond to the number of transistors in a 10GB memory chip, which does not impress us so much, as these days we may buy such chips for 2\$ or so.

\sphinxAtStartPar
Also, the speed of traveling of the nerve impulses, which is due to electrochemical processes, is not impresive, either. Fastest signals, such as those related to muscle positioning, travel at speeds up to 120m/s (the myelin sheaths are essential to achieve them). The touch signals reach about 80m/s, whereas pain is transmitted only at comparatively very slow speeds of 0.6m/s. This is the reason why when you drop a hammer on your toe, you sense it immediately, but the pain reaches your brain with a delay of \textasciitilde{}1s, as it has to pass the distance of \textasciitilde{}1.5m. On the other hand, in electronic devices the signal travels at the speed of light, \(\sim 300000{\rm km/s}=3\times 10^{8}{\rm m/s}\)!

\sphinxAtStartPar
For humans, the average \sphinxhref{https://backyardbrains.com/experiments/reactiontime}{reaction time} is 0.25s to a visual stimulus, 0.17s for an audio stimulus, and 0.15s for a touch. Thus setting the threshold time for a false start in sprints at 0.1s is safely below a possible reaction of a runner. These are very slow reactions compared to electronic responses.

\sphinxAtStartPar
Based on the energy consumption of the brain, one can estimate that on the average a cortical neuron \sphinxhref{https://aiimpacts.org/rate-of-neuron-firing/}{fires} about once per 6 seconds. Likewise, it is unlikely that an average cortical neuron fires more than once per second. Multplying the above firing rate by the number of all the cortical neurons, \(\sim 1.6 \times 10^{10}\), yields about \(3 \times 10^{9}\) firings/s in the cortex, or 3GHz. This is the rate of a typical processor chip! So if a firing is identified with an elementary calculation, the combined power of the brain is comparable to that of standart computer processor.

\sphinxAtStartPar
The above facts indicate that, from a point of view of naive comparisons with silicon\sphinxhyphen{}based chips, the human brain is nothing so special. So what is it that gives us our unique abilities: amazing visual and audio pattern recognition, thinking, consciousness, intuition, imagination? The answer is linked to an amazing architecture of the brain, where each neuron (processor unit) is connected via synapses to, on the average, 10000 (!) other neurons (cf. Fig. \hyperref[\detokenize{docs/intro:sample-fig}]{Fig.\@ \ref{\detokenize{docs/intro:sample-fig}}}). This feature makes it radically different and immensely more complicated than the architecture consisting of the control unit, processor, and memory in our computers (the von Neumann machine). There, the number of connections is of the order of the number of bits of memory. In contrast, there are about \(10^{15}\) synaptic connections in the human brain. As mentioned, the connections may be “programmed” to get stronger or weaker. If, for he sake of a simple estimate, we approximated the connection strength by just two states of a synapse, 0 or 1, the total number of combinatorial configurations of such a system would be \(2^{10^{15}}\) \sphinxhyphen{} a humongous number. Most of such confiburation, of course, never realize in practice, nevertheless the number of possible configuration states of the brain, or the “programs” it can run, is immense.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{feat}.jpg}
\caption{A small brain sample with axons clearly visible (from \sphinxhref{https://www.dailykos.com/stories/2021/6/12/2034998/-Top-Comments-Most-Intensive-Study-of-Brain-Neuron-Connections-Reveals-Never-Before-Seen-Structures}{here})}\label{\detokenize{docs/intro:sample-fig}}\end{figure}

\sphinxAtStartPar
In recent years, with powerful imaging techniques, it became possible to map the connections in the brain with unprecedented resolution, where single nerve boundles are visible. The efforts are part of the \sphinxhref{http://www.humanconnectomeproject.org}{Human Connectome Project}, with the ultimate goal to map one\sphinxhyphen{}to\sphinxhyphen{}one the human brain architecture. For the fruit fly the \sphinxhref{https://en.wikipedia.org/wiki/Drosophila\_connectome}{drosophila connectome project} is well advanced.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=280\sphinxpxdimen]{{brain}.jpg}
\caption{White matter fiber architecture of the brain (from \sphinxhref{http://www.humanconnectomeproject.org/gallery/}{Human Connectome Project})}\label{\detokenize{docs/intro:connectome-fig}}\end{figure}


\section{Feed\sphinxhyphen{}forward networks}
\label{\detokenize{docs/intro:feed-forward-networks}}
\sphinxAtStartPar
The neurophysiological research of the brain provides important guidelines for mathematical models used in artificial neural networks (ANNs). Conversely, the advances in algorithmics of ANNs frequently bring us closer to understanding of how our brain computer may actually work!

\sphinxAtStartPar
The simplest ANNs are the so called \sphinxstylestrong{feed forward} networks, depicted in Fig. \hyperref[\detokenize{docs/intro:ffnn-fig}]{Fig.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}. They consist of the \sphinxstylestrong{input} layer (black dots), which just represents digitized data, and layers of neurons (blobs). The number of neurons in each layer may be different. The complexity of the network and the tasks it may accomplish increases with the number of layers and the number of neurons in the layers. Networks with one layer of neurons are called \sphinxstylestrong{single\sphinxhyphen{}layer} networks. The last layer (light blue blobs) is called the \sphinxstylestrong{output layer}. In multi\sphinxhyphen{}layer networks the layers preceding the output layer (purple blobs) are called \sphinxstylestrong{intermediate layers}. If the number of layers is large (e.g. as many as 64, 128, …), we deal with \sphinxstylestrong{deep networks}.

\sphinxAtStartPar
The neurons in various layers do not have work the same way, in particular the output neurons may act differently from the others.

\sphinxAtStartPar
The signal from the input travels along the links (edges, synaptic connections) to the neurons in subsequent layers. In feed\sphinxhyphen{}forward networks it can only move forward. No going back to preceding layers or propagation among the neurons of the same layer are allowed (that would be the \sphinxstylestrong{recurrent} feature). As we will describe in detail in the next chapter, the signal is appropriately processeded by the neurons.

\sphinxAtStartPar
In the sample network of Fig. \hyperref[\detokenize{docs/intro:ffnn-fig}]{Fig.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} each neuron from a preceding layer is connected to each neuron in the following layer. Such ANNs are called \sphinxstylestrong{fully connected}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{feed_f}.png}
\caption{A sample feed\sphinxhyphen{}foward fully connected artificial neural network. The blobs represent the neurons, and the edges indicate the synaptic connections between them. The signal propagates starting from the input (black dots), via the neurons in subsequent intermediate (hidden) layers (purple blobs) and the output layer (light blue blobs), to finally end up as the output (black dots). The strength of the connections is controled by weights (hyperparameters) assigned to the edges.}\label{\detokenize{docs/intro:ffnn-fig}}\end{figure}

\sphinxAtStartPar
As we will learn in the following, each edge in the network has strength described with a number called \sphinxstylestrong{weight} (the weights are also termed \sphinxstylestrong{hyperparameters}). Even very small fully connected networks, such as the one of \hyperref[\detokenize{docs/intro:ffnn-fig}]{Fig.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}, have very many connections (here 30), hence carry a lot of parameters. Thus, while looking inoccuously, they are in fact complex multiparametric systems.

\sphinxAtStartPar
Also, a crucial feature here is an inherent nonlinearity of the neuron responses, as we discuss in chapter {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{MCP Neuron}}}}


\section{Why Python}
\label{\detokenize{docs/intro:why-python}}
\sphinxAtStartPar
The choice of  \sphinxhref{https://en.wikipedia.org/wiki/Python\_(programming\_language)}{Python} for the little codes of this course needs almost no explanation. Let us only quote \sphinxhref{https://en.wikipedia.org/wiki/Tim\_Peters\_(software\_engineer)}{Tim Peters}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Beautiful is better than ugly.

\item {} 
\sphinxAtStartPar
Explicit is better than implicit.

\item {} 
\sphinxAtStartPar
Simple is better than complex.

\item {} 
\sphinxAtStartPar
Complex is better than complicated.

\item {} 
\sphinxAtStartPar
Readability counts.

\end{itemize}

\sphinxAtStartPar
According to \sphinxhref{https://developer-tech.com/news/2021/apr/27/slashdata-javascript-python-boast-largest-developer-communities/}{SlashData}, there are now over 10 million developers in the world who code using Python, just second after JavaScript (\textasciitilde{}14 million).


\subsection{Imported packages}
\label{\detokenize{docs/intro:imported-packages}}
\sphinxAtStartPar
Throughout the course we use some standard Python libraries for the numerics and plotting (as stressed, we do not use any libraries dedicated to neural networks).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} plots}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} display imported graphics}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}\PYG{p}{,} \PYG{n}{Image}\PYG{p}{,} \PYG{n}{HTML}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
Functions created in the course which are of repeated use, are placed in library \sphinxstylestrong{neural}, described in the {\hyperref[\detokenize{docs/lib_app:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Appendix}}}}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
For brevity of the presenttion, some redundant or inessential pieces of code are present only in the source jupter notebooks, and are not included in the book. To repeat the calculations one\sphinxhyphen{}to\sphinxhyphen{}one, the reader should work with downloaded jupyter notebooks, which can be obtained from …
\end{sphinxadmonition}


\chapter{MCP Neuron}
\label{\detokenize{docs/mcp:mcp-neuron}}\label{\detokenize{docs/mcp:mcp-lab}}\label{\detokenize{docs/mcp::doc}}

\section{Definition}
\label{\detokenize{docs/mcp:definition}}
\sphinxAtStartPar
We need the basic buiding block of the ANN: the (artificial) neuron. The first mathematical model dates back to Warren McCulloch and Walter Pitts (MCP), who proposed it in 1942, hence at the very beginning of the electronic computer age during World War II. The MCP neuron depicted in \hyperref[\detokenize{docs/mcp:mcp1-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:mcp1-fig}}} in the basic ingredient of all ANNs and is built on very simple general rules, inspired neatly by the biological neuron:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The signal enters the nucleus via dendrites from the previous layer.

\item {} 
\sphinxAtStartPar
The synaptic connection for each dendrite may have a different (and adjustable) strength.

\item {} 
\sphinxAtStartPar
In the nucleus, the signal from all the dendrites is combined (summed up) into \(s\).

\item {} 
\sphinxAtStartPar
If the combined signal is stronger than a given threshold, then the neuron fires along the axon, in the opposice case it remains still.

\item {} 
\sphinxAtStartPar
In the siplest relization, the strenth of the fired signal has two possible levels: on or off, i.e. 1 or 0. No intermediate values are needed.

\item {} 
\sphinxAtStartPar
Axon terminal connect to dendrites of the neurons in the next layer.

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=320\sphinxpxdimen]{{mcp-1a}.png}
\caption{MCP neuron: \(x_i\) are the inputs (different in each instance of the data), \(w_i\) are the weights, \(s\) is the signal, \(b\) is the bias, and \(f(s;b)\) represents the acitvation function, yielding the output \(y=f(s;b)\). The blue oval encircles the whole neuron, as used in \hyperref[\detokenize{docs/intro:ffnn-fig}]{Fig.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}.}\label{\detokenize{docs/mcp:mcp1-fig}}\end{figure}

\sphinxAtStartPar
Translating this into a mathematical prescription, one assigns to the input cells the numbers \(x_1, x_2 \dots, x_n\) (input data). The strength of the synaptic connections is controled with the \sphinxstylestrong{weights} \(w_i\). Then the combined signal is defined as the weighted sum
\begin{equation*}
\begin{split}s=\sum_{i=1}^n x_i w_i.\end{split}
\end{equation*}
\sphinxAtStartPar
Thesignal becomes an argument of the \sphinxstylestrong{activation function}, which to begin takes the simple form of the step function
\begin{equation*}
\begin{split}
f(s;b) = \left \{ \begin{array}{l} 1 {\rm ~for~} s \ge b \\ 0 {\rm ~for~} s < b \end{array} \right .
\end{split}
\end{equation*}
\sphinxAtStartPar
When the combined signal \(s\) is larger than the bias (threshold) \(b\), the nucleus fires. i.e. the signal passed along the axon is 1. in the opposite case, the generated signal value is 0 (no firing). This is precisely what we need to mimick the biological prototype.

\sphinxAtStartPar
There is a convenient notational covnention which is frequently used. Instead of splitting the bias from the input data, we may treat it uniformly. The condition for firing may be triviallly transformed as
\begin{equation*}
\begin{split}
s \ge b  \to s-b \ge 0 \to \sum_{i=1}^n x_i w_i - b \ge 0 \to \sum_{i=1}^n x_i w_i +x_0 w_0 \ge 0 
\to \sum_{i=0}^n x_i w_i \ge 0,
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(x_0=1\) and \(w_0=-b\). In other words, we may treat the bias as a weight on the edge connected to an additional cell with input set to 1. This notation is shown in \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}. Now, the activation function is simply
\begin{equation}\label{equation:docs/mcp:eq-f}
\begin{split}f(s) = \left \{ \begin{array}{l} 1 {\rm ~for~} s \ge 0 \\ 0 {\rm ~for~} s < 0 \end{array} \right .,\end{split}
\end{equation}
\sphinxAtStartPar
with the summation index in \(s\) starting from \(0\):
\begin{equation}\label{equation:docs/mcp:eq-f0}
\begin{split}s=\sum_{i=0}^n x_i w_i = x_0 w_0+x_1 w_1 + \dots + x_n w_n.\end{split}
\end{equation}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=320\sphinxpxdimen]{{mcp-2a}.png}
\caption{Alternative, more uniform representation of the MCP neuron, with \(x_0=1\) and \(w_0=-b\).}\label{\detokenize{docs/mcp:mcp2-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Hyperparameters}

\sphinxAtStartPar
The weights \(w_0=-b,w_1,\dots,w_n\) are referred to as hyperparameters. They determine the functionality of the MCP neuron and may be changed during the learning (training) process (see the following). However, they are kept fixed when using the trained neuron on a particular input data set.
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
An essential property of neurons in ANNs is the \sphinxstylestrong{nonlinearity} of the activation function. Without this feature, the MCP neuron would simply represent a scalar product, and the feed\sphinxhyphen{}forward networks would involve trivial matrix multilications.
\end{sphinxadmonition}


\section{MCP neuron in Python}
\label{\detokenize{docs/mcp:mcp-neuron-in-python}}\label{\detokenize{docs/mcp:mcp-p-lab}}
\sphinxAtStartPar
We will now implement the mathematical model of the neuron of Sec. {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{MCP Neuron}}}}. First, we obviously need arrays (vectors), which in Python are represented as

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{w} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{2.5}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
and are indexed starting from 0, e.g.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\sphinxAtStartPar
The numpy library functions carry the prefix \sphinxstylestrong{np}, which is the alias given at import. Note that these fumctions act \sphinxstyleemphasis{distributively} over arrays, e.g.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.84147098, 0.14112001, 0.6569866 ])
\end{sphinxVerbatim}

\sphinxAtStartPar
which is a convenient feature. We also have the scalar product \(x \cdot w = \sum_i x_i w_i\) handy, which we can use to build the combined signal \(s\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
21.5
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we need to construct the neuron activation function, which presently is just the step function \eqref{equation:docs/mcp:eq-f}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} step function (in neural library)}
     \PYG{k}{if} \PYG{n}{s} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
     \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}

\sphinxAtStartPar
where in the comment we indicate, that the function is also defined in the \sphinxstylestrong{neural} library, cf. {\hyperref[\detokenize{docs/lib_app:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Appendix}}}}. For the visualizers, the plot of the step function is following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} set the size of the figure}

\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} array 100+1 equally spaced points between \PYGZhy{}2 and 2}
\PYG{n}{fs} \PYG{o}{=} \PYG{p}{[}\PYG{n}{step}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)} \PYG{k}{for} \PYG{n}{z} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} corresponding array of function values}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{signal s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} axes labels}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response f(s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{13}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} plot title}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{mcp_25_0}.png}

\sphinxAtStartPar
Since \(x_0=1\) always, we do not want to explicitly carry this in the argument of the functions that will follow. We will be inserting \(x_0=1\) into the input, for instance:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert 1 in x at position 0}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1, 5, 7])
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we are ready to construct the {\hyperref[\detokenize{docs/mcp:mcp1-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{MCP neuron}}}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neuron}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} (in the neural library)}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    MCP neuron}

\PYG{l+s+sd}{    x: array of inputs  [x1, x2,...,xn]}
\PYG{l+s+sd}{    w: array of weights [w0, w1, w2,...,wn]}
\PYG{l+s+sd}{    f: activation function, with step as default}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}} 
    \PYG{k}{return} \PYG{n}{f}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert x0=1, signal s=x.w, output f(s)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We diligently put the comments in triple quotes to be able to get the help, when needed:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{help}\PYG{p}{(}\PYG{n}{neuron}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Help on function neuron in module \PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}:

neuron(x, w, f=\PYGZlt{}function step at 0x7fb008a4e320\PYGZgt{})
    MCP neuron
    
    x: array of inputs  [x1, x2,...,xn]
    w: array of weights [w0, w1, w2,...,wn]
    f: activation function, with step as default
    
    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w
\end{sphinxVerbatim}

\sphinxAtStartPar
Note that the function f is an argument of neuron, but it has the default set to step and thus does not have to be present. The sample usage with \(x_1=3\), \(w_0=-b=-2\), \(w_1=1\) is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\sphinxAtStartPar
As we can see, the neuron fired in this case, as \(s=1*(-2)+3*1>0\). Next, we show how the neuron operates on a varying input \(x_1\) taken in the range \([-2,2]\). We also change the bias parameter, to illustrate its role. It is clear that the bias works as the threshold: if the signal \(x_1 w_1\) is above \(b=-x_0\), the neuron fires.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)} 

\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{fs1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} more function on one plot}
\PYG{n}{fs0} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}
\PYG{n}{fsm12} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Change of bias}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{13}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs0}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fsm12}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=1/2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}                               \PYG{c+c1}{\PYGZsh{} legend}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{mcp_35_0}.png}

\sphinxAtStartPar
When the sign of the weight \(w_1\) is negative, we get in some sense a \sphinxstylestrong{reverse} behavior, where the neuron fires when \(x_1 |w_1| < w_0\):

\noindent\sphinxincludegraphics{{mcp_37_0}.png}

\sphinxAtStartPar
Note that here (and similarly in other places) the trivial code for the above output is hidden and can be found in the corresponding jupyter notebook.

\sphinxAtStartPar
Admittedly, in the last example one departs from the biological pattern, as negative weights are not possible to realize in a biological neuron. However, this enriches the mathematical model, which one is free to use without constraints.


\section{Boolean functions}
\label{\detokenize{docs/mcp:boolean-functions}}\label{\detokenize{docs/mcp:bool-sec}}
\sphinxAtStartPar
Having constructed the MCP neuron in Python, the question is: \sphinxstyleemphasis{What is the simplest (but still non\sphinxhyphen{}trivial) application we can use it for?} We will show here that one can easily construct \sphinxhref{https://en.wikipedia.org/wiki/Boolean\_function}{boolean functions}, or logical networks, with the help of networks of MCP neurons. Boolean functions, by definition, have arguments and values in the set \(\{ 0,1 \}\), or \{True, False\}.

\sphinxAtStartPar
To warm up, let us start with some guesswork, where we take the neuron with the weights \(w=[w_0,w_1,w_2]=[-1,0.6,0.6]\) (why not). We shall here denote \(x_1=p\), \(x_2=q\), in accordance with the traditional notation for logical variables, where \(p,q \in \{0,1\}\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q n(p,q)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} loop over p}
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over q}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print all cases}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q n(p,q)

0 0  0
0 1  0
1 0  0
1 1  1
\end{sphinxVerbatim}

\sphinxAtStartPar
We immediately recognize in the above output the logical table for the conjunction, \(n(p,q)=p \land q\), or the logical \sphinxstylestrong{AND} operation. It is clear how the neuron works. The condition for the firing \(n(p,q)=1\) is \(-1+p*0.6+q*0.6 \ge 0\), and it is satisfied if and only if \(p=q=1\), which is the definition of the logical conjunction. Of course, we could use here 0.7 instead of 0.6, or in general \(w_1\) and \(w_2\) such that \(w_1<1, w_2<1, w_1+w_2 \ge 1\). In the electronics terminology, we can call the present system the \sphinxstylestrong{AND gate}.

\sphinxAtStartPar
We can thus define the short\sphinxhyphen{}hand

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Quite similarly, we may define other boolean functions (or logical gates) of two variables. In particular, the NAND gate (the negation of conjunction) and the OR gate (alternative) are realized with the following MCP neurons:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:}   \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{l+m+mf}{1.2}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
They correspond to the logical tables

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q  NAND OR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q  NAND OR

0 0   1   0
0 1   1   1
1 0   1   1
1 1   0   1
\end{sphinxVerbatim}


\subsection{Problem with XOR}
\label{\detokenize{docs/mcp:problem-with-xor}}
\sphinxAtStartPar
The XOR gate, or the \sphinxstylestrong{exclusive alternative}, is defined with the following logical table:
\begin{equation*}
\begin{split}
\begin{array}{ccc}
p & q & p \oplus q \\
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0
\end{array}
\end{split}
\end{equation*}
\sphinxAtStartPar
This is one of possible boolean functions of two arguments (in total, we have 16 different functions of this kind, why?). We could now try very hard to adjust the weights in our neuron to behave as the XOR gate, but we are doomed to fail. Here is the reson:

\sphinxAtStartPar
From the first row of the above table it follows that for the input 0 0 the neuron should not fire. Hence

\sphinxAtStartPar
\(w_0  + 0* w_1 + 0*w_2  < 0\), or \(-w_0>0\).

\sphinxAtStartPar
For the cases of rows 2 and 3 the neuron must fire, therefore

\sphinxAtStartPar
\(w_0+w_2 \ge 0\) and \(w_0+w_1 \ge 0\).

\sphinxAtStartPar
Adding side\sphinxhyphen{}by\sphinxhyphen{}side the three obtained inequalities we get \(w_0+w_1+w_2 > 0\). However, the fourth row yields
\(w_0+w_1+w_2<0\) (no firing), so we encounter a contradiction. Therefore no choice of \(w_0, w_1, w_2\) exists to do the job!

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
A single MCP neuron cannot represent the \sphinxstylestrong{XOR} gate.
\end{sphinxadmonition}


\subsection{XOR from composition of AND, NAND and OR}
\label{\detokenize{docs/mcp:xor-from-composition-of-and-nand-and-or}}
\sphinxAtStartPar
One can solve the XOR problem by composing three MCP neurons, for instance

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurXOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neurAND}\PYG{p}{(}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{,}\PYG{n}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q XOR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurXOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
\end{sphinxVerbatim}

\sphinxAtStartPar
The above construction corresponds to the simple network of \hyperref[\detokenize{docs/mcp:xor-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:xor-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=260\sphinxpxdimen]{{xor}.png}
\caption{The XOR gate compsed of the NAND, OR, and AND MCP neurons.}\label{\detokenize{docs/mcp:xor-fig}}\end{figure}

\sphinxAtStartPar
Note that we are dealing here, for the first time, with a network having an intermediate layer, consisting of the NAND and OR neurons. This layer is indispensable to construct the XOR gate.


\subsection{XOR composed from NAND}
\label{\detokenize{docs/mcp:xor-composed-from-nand}}
\sphinxAtStartPar
Within the theory of logical networks, one proves that any network (or boolean function) can be composed of only NAND gates, or only the NOR gates. One says that the NAND (or NOR) gates are \sphinxstylestrong{complete}. In particular, the XOR gate can be constructed as

\sphinxAtStartPar
{[} p NAND ( p NAND q ) {]} NAND {[} q NAND ( p NAND q ) {]},

\sphinxAtStartPar
which we can write in Python as

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{nXOR}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{j}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q XOR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{nXOR}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)} 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Exercises}

\sphinxAtStartPar
Construct (all in Python)
\begin{itemize}
\item {} 
\sphinxAtStartPar
gates NOT, NOR

\item {} 
\sphinxAtStartPar
gates OR, AND, NOT by composing gates NAND \sphinxurl{https://en.wikipedia.org/wiki/NAND\_logic}

\item {} 
\sphinxAtStartPar
the half adder and full adder \sphinxurl{https://en.wikipedia.org/wiki/Adder\_(electronics)}

\end{itemize}

\sphinxAtStartPar
as networks of MCP neurons.
\end{sphinxadmonition}


\chapter{Models of memory}
\label{\detokenize{docs/memory:models-of-memory}}\label{\detokenize{docs/memory::doc}}

\section{Heteroassociative memory}
\label{\detokenize{docs/memory:heteroassociative-memory}}

\subsection{Pair associations}
\label{\detokenize{docs/memory:pair-associations}}
\sphinxAtStartPar
We now pass to further illustrations of elementary capabilities of ANNs, desribing two very simple models of memory based on linear algebra, supplemented with (nonlinear) filtering (an implementation of these models in Mathematica is provided in \sphinxurl{http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/IntroNeuralSyllabus.html}). Speaking of memory here, we have very simple tools in mind, which is far from the actual complex and hitherto not completely understood memory mechanism operating in our brain.

\sphinxAtStartPar
The first model concerns the so called \sphinxstylestrong{heterassociative} memory, where some objects (here graphic bitmap symbols) are joined in pairs. In particular, we take the set of five graphical symbols, \{A, a, I, i, Y\}, and define two pair associations A \(\leftrightarrow\) a and I \(\leftrightarrow\) i between different (hetero) symbols. Y remain unassociated.

\sphinxAtStartPar
The symbols are defined as 2\sphinxhyphen{}dimensional \(12 \times 12\) pixel arrays, for instance

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}     
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}  
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\sphinxAtStartPar
The remaining symbols are defined smilarly.

\sphinxAtStartPar
The whole set looks like this, with yellow=1 and violet=0:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sym}\PYG{o}{=}\PYG{p}{[}\PYG{n}{A}\PYG{p}{,}\PYG{n}{a}\PYG{p}{,}\PYG{n}{ii}\PYG{p}{,}\PYG{n}{I}\PYG{p}{,}\PYG{n}{Y}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} array of symbols, numbered from 0 to 4}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} figure with horizontal and vertical size}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} loop over 5 figure panels, i is from 1 to 5}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} panels, numbered from 1 to 5}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} no axes}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sym}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} plot symbol, numbered from 0 to 4}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{memory_14_0}.png}

\sphinxAtStartPar
It is more convenient to work not with the above two\sphinxhyphen{}dimensional arrays, but with one\sphinxhyphen{}dimensional vectors obtained with the so\sphinxhyphen{}called \sphinxstylestrong{flattening} procedure, where a matrix is cut along its rows into a vector. For example

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} a matrix}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}                            
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} matrix flattened into a vector   }
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 2 3]
 [0 4 0]
 [3 2 7]]
[1 2 3 0 4 0 3 2 7]
\end{sphinxVerbatim}

\sphinxAtStartPar
We thus perform the flattenning:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fA}\PYG{o}{=}\PYG{n}{A}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fa}\PYG{o}{=}\PYG{n}{a}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fi}\PYG{o}{=}\PYG{n}{ii}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fI}\PYG{o}{=}\PYG{n}{I}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fY}\PYG{o}{=}\PYG{n}{Y}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
to obtain, for instance

\noindent\sphinxincludegraphics{{memory_20_0}.png}

\noindent\sphinxincludegraphics{{memory_21_0}.png}

\sphinxAtStartPar
The advantage of working with vectors is that we can use the scalar product. Here, the scalar product between two symbols is just equal to the number of common yellow pixels. For instance, for the flattened symbols plotted above we have only two common yellow pixels:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2
\end{sphinxVerbatim}

\sphinxAtStartPar
It is clear that one can use the scalar product as a measure of similarity between the symbols. For the following method to work, the symbols should not be too similar.


\subsection{Memory matrix}
\label{\detokenize{docs/memory:memory-matrix}}
\sphinxAtStartPar
The next algebraic concept we need is the \sphinxstylestrong{outer product}. For two vectors \(v\) and \(w\), it is defined as \(v w^T = v \otimes w\) (as opposed to the scalar product, where \(w^T v = w \cdot v\)). \(T\) denotes transposition. The result is a matrix with the number of rows equal to the length of \(v\), and the number of column equal to the length of \(w\).

\sphinxAtStartPar
For example, with
\begin{equation*}
\begin{split} v = \left ( \begin{array}{c} v_1 \\ v_2 \\v_3 \end{array}  \right ), \;\;\;\; w = \left ( \begin{array}{c} w_1 \\ w_2 \end{array}  \right ), \end{split}
\end{equation*}
\sphinxAtStartPar
we have
\begin{equation*}
\begin{split} 
v \otimes w = v w^T=
\left ( \begin{array}{c} v_1 \\ v_2 \\v_3 \end{array}  \right ) (w_1,w_2)
= \left ( \begin{array}{cc} v_1 w_1 & v_1 w_2 \\ v_2 w_1 & v_2 w_2 \\v_3 v_1 & v_3 w_2 \end{array}  \right ).
\end{split}
\end{equation*}
\sphinxAtStartPar
In numpy

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} outer product of two vectors}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 2  7]
 [ 4 14]
 [ 6 21]]
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we construct a \sphinxstylestrong{memory matrix} needed for modeling our heteroassociative memory. Suppose first for simplicity of notation that we only have two associations: \(a \to A\) and \(b \to B\).
Let
\begin{equation*}
\begin{split}M = A a^T/a\cdot a + B b^T/b\cdot b.\end{split}
\end{equation*}
\sphinxAtStartPar
Then
\begin{equation*}
\begin{split}M a=  A + B \, a\cdot b /b \cdot a, \end{split}
\end{equation*}
\sphinxAtStartPar
and if \(a\) and \(b\) were \sphinxstylestrong{orthogonal}, i.e. \(a \cdot b =0\), then

\sphinxAtStartPar
\( M a =  A\)

\sphinxAtStartPar
yielding an exact association. Similarly, we would have \(M b = B\). However, since in a general case the vectors are not exactly orthogonal, an error \(B \, b \cdot a/a \cdot a\) (for the association of \(a\)) is generated. It is usually small if the number of pixels in our symbols is large and the symbols are, loosely speaking, not too similar. As we will see, the emerging error can be efficiently “filtered out” with an appropriate neuron activation function.

\sphinxAtStartPar
Coming back to our particular case, we thus need four terms in \(M\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{M}\PYG{o}{=}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}
   \PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{\PYGZsh{} associated pairs}
\end{sphinxVerbatim}

\sphinxAtStartPar
Now, for each flattened symbol \(s\) we will evaluate \(Ms\). The result is a vector, which we want to bring back to the form of the \(12\times 12\) pixel array. The operation inverse to flattening in Python is \sphinxstylestrong{reshape}. For instance

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test vector}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tt}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} cutting into 2 rows of length 2}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 2]
 [3 5]]
\end{sphinxVerbatim}

\sphinxAtStartPar
For our vectors we have

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Yp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fY}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} we also try unassociated symbol Y}

\PYG{n}{symp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{Ap}\PYG{p}{,}\PYG{n}{ap}\PYG{p}{,}\PYG{n}{Ip}\PYG{p}{,}\PYG{n}{ip}\PYG{p}{,}\PYG{n}{Yp}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} array of associated symbols}
\end{sphinxVerbatim}

\sphinxAtStartPar
For the case of association to A (which shou
ld be a), it yields (we use rounding to 2 decimal digits)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{Ap}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} pixel map for the association of the symbol A}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.25 0.85 0.25 0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.85 0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   1.   1.6  1.85 1.89 0.   0.   0.   0.   0.  ]
 [0.   0.   1.   0.   0.6  0.25 1.6  0.   0.   0.   0.   0.  ]
 [0.   0.   1.   0.6  0.   0.54 1.29 0.6  0.   0.   0.   0.  ]
 [0.   0.   1.   0.6  0.   0.25 1.29 0.6  0.   0.   0.   0.  ]
 [0.   0.   0.6  1.6  1.6  1.85 1.89 1.6  0.6  0.   0.   0.  ]
 [0.   0.   0.6  0.   0.   0.25 0.29 0.   0.6  0.   0.   0.  ]
 [0.   0.6  0.   0.   0.   0.25 0.   0.29 0.29 0.6  0.   0.  ]
 [0.   0.6  0.   0.   0.25 0.25 0.25 0.   0.   0.6  0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]
\end{sphinxVerbatim}

\sphinxAtStartPar
We note that the strength of pixels is now not necessarily equal to 0 or 1, as it was in the original symbols. The graphic representation looks as follows:

\noindent\sphinxincludegraphics{{memory_37_0}.png}

\sphinxAtStartPar
We should be able to see in the above picture the sequence a, A, i, I, and nothing particular in the association of Y. We almost do, but the situation is not perfect due to the nonorthogonality error discussed above.


\subsection{Applying a filter}
\label{\detokenize{docs/memory:applying-a-filter}}
\sphinxAtStartPar
The result improves greatly when a filter is applied to the pixel maps. Looking at the above print out or the plot of Ap (the symbol associated to A which shoul be a), we note that we should get rid of the “faint shadows”, and leave only the pixels of suffient strength, which should then acquire the value 1. In other words, pixels below a bias (threshold) \(b\) should be reset to 0, and those above or equal to \(b\) should be reset to 1. This can be neatly accomplished with our \sphinxstylestrong{neuron} function from Sec. {\hyperref[\detokenize{docs/mcp:mcp-p-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{MCP neuron in Python}}}}. This function has been placed in the library \sphinxstylestrong{neural} (see {\hyperref[\detokenize{docs/lib_app:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Appendix}}}}), which we now read in:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{sys} \PYG{c+c1}{\PYGZsh{} system library}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./lib\PYGZus{}nn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} my path (linux, Mac OS)}

\PYG{k+kn}{from} \PYG{n+nn}{neural} \PYG{k+kn}{import} \PYG{o}{*} \PYG{c+c1}{\PYGZsh{} import my library packages}
\end{sphinxVerbatim}

\sphinxAtStartPar
We thus define the filter as a neuron with weight \(w_0=-b\) and \(w_1=1\):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{filter}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} a \PYGZhy{} symbol (2\PYGZhy{}dim pixel array), b \PYGZhy{} bias}
    \PYG{n}{n}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} number of rows (and columns)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{b}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
       \PYG{c+c1}{\PYGZsh{} 2\PYGZhy{}dim array with the filter applied}
\end{sphinxVerbatim}

\sphinxAtStartPar
When operating on Ap with appropriately chosen \(b=0.9\) (the level of the bias is very much relevant), the result is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{n}{Ap}\PYG{p}{,}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 1 1 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 0 1 1 1 1 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]
\end{sphinxVerbatim}

\sphinxAtStartPar
where we can notice a “clean” symbol a. We check that it actually works perfectly well for all our accociations (such perfection is not always the case):

\noindent\sphinxincludegraphics{{memory_47_0}.png}

\sphinxAtStartPar
A representation of the presented model of the heteroassociative memory in terms of ANN can be readily given. In the plot below we indicate all the operations, going from left to right. The input symbol is flattened. The input and output layers are fully connected with edges (not shown) connecting the input cells to the neurons in the output layer. The weights of the edges are equal to the matrix elements \(M_{ij}\), indicated with symbol M. The activation function is the same for all neurons and it has the form of a step function.

\sphinxAtStartPar
At the bottom we indicate the elements of the input vector, \(x_i\), of the signal reaching the neuron \(j\), \(s_j=\sum_i x_i M_{ij}\), and the final output \(y_j=f(s_j)\).

\noindent\sphinxincludegraphics{{memory_50_0}.png}

\begin{sphinxadmonition}{note}{Summary of the model of the heteroassociative memory}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Define pairs of associated symbols and construct the memory matrix \(M\).

\item {} 
\sphinxAtStartPar
The input is a symbol in the form of a 2\sphinxhyphen{}dim array of pixels with values 0 or 1.

\item {} 
\sphinxAtStartPar
Flaten the symbol into a vector, which forms the layer of inputs \(x_i\).

\item {} 
\sphinxAtStartPar
The weight matrix of the fully connected ANN is \(M\).

\item {} 
\sphinxAtStartPar
The signal entering neuron \(j\) in the output layer is \(s_j=\sum_i x_i M_{ij}\).

\item {} 
\sphinxAtStartPar
The activation (step) function with a properly chosen bias yields \(y_j=f(s_j)\).

\item {} 
\sphinxAtStartPar
Cut the output vector into a matrix of pixels, which constitutes the final output.
It should be the symbol associated to the input.

\end{enumerate}
\end{sphinxadmonition}


\section{Autoassociative memory}
\label{\detokenize{docs/memory:autoassociative-memory}}

\subsection{Self\sphinxhyphen{}associations}
\label{\detokenize{docs/memory:self-associations}}
\sphinxAtStartPar
The autoassociative memory model is in close analogy to the case of the heteroassociatine memory, but now the symbol is associated \sphinxstylestrong{to itself}. Why we do such a thing will become clear shortly, when we consider distorted input. We thus define the association matrix as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ma}\PYG{o}{=}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}
    \PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
After multiplying the flattened symbol with matrix Ma, reshaping, and filtering (all steps as in the heteroassociative case) we properly get back th original symbols (except for Y, which was not associated).

\noindent\sphinxincludegraphics{{memory_58_0}.png}


\subsection{Distorting the image}
\label{\detokenize{docs/memory:distorting-the-image}}
\sphinxAtStartPar
Now imagine that the original input gets partially destroyed, with some pixels randomly altered from 1 to 0 and vice versa.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ne}\PYG{o}{=}\PYG{l+m+mi}{12} \PYG{c+c1}{\PYGZsh{} number of alterations}

\PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{sym}\PYG{p}{:}                     \PYG{c+c1}{\PYGZsh{} loop over symbols}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ne}\PYG{p}{)}\PYG{p}{:}           \PYG{c+c1}{\PYGZsh{} loop over alteratons}
        \PYG{n}{i}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random position in row}
        \PYG{n}{j}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random position in column}
        \PYG{n}{s}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}           \PYG{c+c1}{\PYGZsh{} switching 1 and 0}
\end{sphinxVerbatim}

\sphinxAtStartPar
After this the input symbols look like this:

\noindent\sphinxincludegraphics{{memory_63_0}.png}


\subsection{Restoring the symbols}
\label{\detokenize{docs/memory:restoring-the-symbols}}
\sphinxAtStartPar
We now apply our model of the autoassociative memory to all the “distroyed” symbols:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Yp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fY}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}

\PYG{n}{symp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{Ap}\PYG{p}{,}\PYG{n}{ap}\PYG{p}{,}\PYG{n}{Ip}\PYG{p}{,}\PYG{n}{ip}\PYG{p}{,}\PYG{n}{Yp}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} array of self\PYGZhy{}associated symbols}
\end{sphinxVerbatim}

\sphinxAtStartPar
which yields

\noindent\sphinxincludegraphics{{memory_68_0}.png}

\sphinxAtStartPar
After filtering, with \(b=0.9\), we obtain back the original symbols:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} loop over panels}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}  
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}       
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{n}{symp}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mf}{0.9}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} plot filtered symbol}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{memory_70_0}.png}

\begin{sphinxadmonition}{note}{Summary of the model of the autooassociative memory}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Construct the memory matrix \(Ma\).

\item {} 
\sphinxAtStartPar
The input is a symbol in the form of a 2\sphinxhyphen{}dim array of pixels with values 0 or 1, with a
certain number of pixels randomly distorted.

\item {} 
\sphinxAtStartPar
Flaten the symbol into a vector, which forms the layer of inputs \(x_i\).

\item {} 
\sphinxAtStartPar
The weight matrix of the fully connected ANN is \(Ma\).

\item {} 
\sphinxAtStartPar
The signal entering neuron \(j\) in the output layer is \(s_j=\sum_i x_i M_{ij}\).

\item {} 
\sphinxAtStartPar
The activation (step) function with a properly chosen bias yields \(y_j=f(s_j)\).

\item {} 
\sphinxAtStartPar
Cut the output vector into a matrix of pixels, which constitutes the final output. It should bring back the ariginal symbol.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
The application can thus decifer a “destroyed” text, or, more generally,
provide an error correcion mechanism.

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
Message: ANN can serve as very simple models of memory!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Exercises}

\sphinxAtStartPar
Play with the lecture code and
\begin{itemize}
\item {} 
\sphinxAtStartPar
add more and more symbols,

\item {} 
\sphinxAtStartPar
change the filter level,

\item {} 
\sphinxAtStartPar
increase the number of alterations.

\end{itemize}

\sphinxAtStartPar
Discuss your findings.
\end{sphinxadmonition}


\chapter{Perceptron}
\label{\detokenize{docs/perceptron:perceptron}}\label{\detokenize{docs/perceptron:perc-lab}}\label{\detokenize{docs/perceptron::doc}}

\section{Supervised learning}
\label{\detokenize{docs/perceptron:supervised-learning}}
\sphinxAtStartPar
We have shown in the previous chapters that even the simplest ANNs can carry out useful tasks (emulate logical networks or provide simple memory models). Generally, each ANN has
\begin{itemize}
\item {} 
\sphinxAtStartPar
some \sphinxstylestrong{architecture}, i.e. the number of layers, number of neurons in each layer, scheme of connections between the neurons (fully connected or not, feed forward, recurrent, …),

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{weights (hyperparameters)}, with specific values, defining the network’s functionality.

\end{itemize}

\sphinxAtStartPar
The prime practical question is how to set (for a given architecture) the weights such that a requested goal is realized, i.e., a given input yields a desired output.
In the tasks discussed earlier, the weights could be constructed \sphinxstyleemphasis{a priori}, be it for the logical gates or for the memory models. However, for more involved applications we want to have an “easier” way of determining the weights. Actually, for complicated problems a “theoretical” determination of weights is not possible at all. This is the basic reason for inventing \sphinxstylestrong{learning algorithms}, which automatically adjust the weights with the help of a data sample.

\sphinxAtStartPar
In this chapter we begin to explore such algorithms with the \sphinxstylestrong{supervised learning}, used for data classification.

\begin{sphinxadmonition}{note}{Supervised learning}

\sphinxAtStartPar
In this strategy, the data must possess \sphinxstylestrong{labels} which a priori determine the correct category for each point. Think for example of pictures of animals (data) and their descriptions (cat,dog,…), which are the labels.
The labeled data are split into a \sphinxstylestrong{training} sample and a \sphinxstylestrong{test} sample.

\sphinxAtStartPar
The basic steps of supervised learning for a given ANN are following:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Initialize somehow the weights, for instance randomly or to zero.

\item {} 
\sphinxAtStartPar
Read subsequently the data points from the training sample and pass them throught your ANN. The obtained answer may differ from the correct one, determimed by the label, in which case the weights are adjusted according to a specific prescription (to be discussed later on).

\item {} 
\sphinxAtStartPar
Repeat, if needed, the previous step. Typically, the weights are changed less and less as the algorithm proceeds.

\item {} 
\sphinxAtStartPar
Finish the training when a stopping criterion is reached (weights do not change much any more or the maximum number of iterations has been completed).

\item {} 
\sphinxAtStartPar
Test the trained ANN on the test sample.

\end{itemize}

\sphinxAtStartPar
If satisfied, you have a desired trained ANN performing a specific task, which can be used on new, unlabeled data. If not, you can split the sample in the training and the test parts in a different way and repeat the procedure from the beginning. Also, you may try to acquire more data, or change your network’s architecture.

\sphinxAtStartPar
This he term “supervised” comes form the interpretation of the procedure where the labels are held by a “teacher”, who thus knows which answers are correct and which are wrong, and who \sphinxstylestrong{supervises} the training process.
\end{sphinxadmonition}


\section{Binary classifier}
\label{\detokenize{docs/perceptron:binary-classifier}}
\sphinxAtStartPar
The simplest supervised learning algorithm
is the \sphinxhref{https://en.wikipedia.org/wiki/Perceptron}{perceptron}, invented in 1958 by Frank Rosenblatt. It can be used to
construct \sphinxstylestrong{binary classifiers} for the data. \sphinxstyleemphasis{Binary} means that the network
is used to assess if a data point has a particular feature, or not.

\begin{sphinxadmonition}{note}{Remark}

\sphinxAtStartPar
The term \sphinxstyleemphasis{perceptron} is also used for ANNs (without or with intermediate layers) consisting of the MCP neurons (cf. Fig. \hyperref[\detokenize{docs/intro:ffnn-fig}]{Fig.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} and \hyperref[\detokenize{docs/mcp:mcp1-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:mcp1-fig}}}), on which the perceptron algorithm is executed.
\end{sphinxadmonition}


\subsection{Sample with a known classification rule}
\label{\detokenize{docs/perceptron:sample-with-a-known-classification-rule}}
\sphinxAtStartPar
To begin, we need some training data, which we will generate as random points in a square. Thus the coordinates of the point, \(x_1\) and \(x_2\), are taken in the range \([0,1]\). We define two categories: one for the points lying above the line \(x_1=x_2\) (call them pink), and the other for the points lying below (blue). During the generation, we check whether \(x_2 > x_1\) or not, and assign a \sphinxstylestrong{label} to each data point equal to, correspongigly, 1 or 0.

\sphinxAtStartPar
The function generating the described data point with a label is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} returns random coordinates x1, x2 and 1 if x2\PYGZgt{}x1, 0 otherwise}
\PYG{k}{def} \PYG{n+nf}{point}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random number from the range [0,1]}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZgt{}}\PYG{n}{x1}\PYG{p}{)}\PYG{p}{:}                     \PYG{c+c1}{\PYGZsh{} condition met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 1}
    \PYG{k}{else}\PYG{p}{:}                          \PYG{c+c1}{\PYGZsh{} not met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 0}
\end{sphinxVerbatim}

\sphinxAtStartPar
We generate a \sphinxstylestrong{training sample} of \sphinxstylestrong{npo}=300 labeled data points:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{npo}\PYG{o}{=}\PYG{l+m+mi}{300} \PYG{c+c1}{\PYGZsh{} number of data points in the training sample}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{  x1         x2         label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} header}
\PYG{n}{samp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npo}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} training sample, \PYGZus{} is dummy iterator}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}                           \PYG{c+c1}{\PYGZsh{} first 5 data points}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  x1         x2         label
[[0.19968208 0.34399239 1.        ]
 [0.11365587 0.21869436 1.        ]
 [0.10854327 0.43462234 1.        ]
 [0.80135246 0.69970322 0.        ]
 [0.41130912 0.80434883 1.        ]]
\end{sphinxVerbatim}

\sphinxAtStartPar
Not to print unnecessarily the very long table, we have used above for the first time the \sphinxstylestrong{ranges for array indices}. For example, 2:5 means from 2 to 4 (the last one is excluded!), :5  \sphinxhyphen{} from 0 to 4, 5: \sphinxhyphen{} from 5 to the end, and : \sphinxhyphen{} all the indices.

\sphinxAtStartPar
Graphically, our data are show in the figure below. We also plot the line \(x_2=x_1\), which separates the blue and purple points. In this case the division is a priori possible (we know the rule) in an exact manner.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}                 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}                                  \PYG{c+c1}{\PYGZsh{} axes limits}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} label determines the color}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} point size and color}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{)}                 \PYG{c+c1}{\PYGZsh{} separating line}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}                    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{perceptron_15_0}.png}

\begin{sphinxadmonition}{note}{Linearly separable sets}

\sphinxAtStartPar
Two sets of points (e.g. blue and pink) on a plane which are possible to separate with a straigh line are called \sphinxstylestrong{linearly separable}. In three dimensions, the sets must be separable with a plane, in general in \(n\) dimensions the sets must must be separable with a \(n-1\) dimensional hyperplane.
\end{sphinxadmonition}

\sphinxAtStartPar
Analitically, if the points in the \(n\) dimensional space have coordinates \((x_1,x_2,\dots,x_n)\), one may chose the parameters \((w_0,w_1,\dots,w_n)\) in such a way that one set of points must satisfy the condition
\begin{equation}\label{equation:docs/perceptron:eq-linsep}
\begin{split}w_0+x_1 w_1+x_2 w_2 + \dots x_n w_n > 0\end{split}
\end{equation}
\sphinxAtStartPar
and the other the opposite condition, with \(>\) replaced with \(\le\).

\sphinxAtStartPar
Now a crucial, albeit simple observation: the above inequality is precisely the condition implemented in the {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{MCP neuron}}}} (with the step activation function) in the convention of \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}! We may thus enforce condition \eqref{equation:docs/perceptron:eq-linsep} with the \sphinxstylestrong{neuron} function from our \sphinxstylestrong{neural} library.

\sphinxAtStartPar
In our example we have for the pink points, by construction,
\begin{equation*}
\begin{split}
x_2>x_1 \to s=-x_1+x_2 >0
\end{split}
\end{equation*}
\sphinxAtStartPar
from where, using  Eq. \eqref{equation:docs/perceptron:eq-linsep}, we can immediately read out
\begin{equation*}
\begin{split}
w_0=0, \;\; w_1=-1, w_2=1.
\end{split}
\end{equation*}
\sphinxAtStartPar
Thus the \sphinxstylestrong{neuron} function is used on a sample point p like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} sample point with x\PYGZus{}2 \PYGZgt{} x\PYGZus{}1}
\PYG{n}{w}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} weights as given above}

\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)} 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\sphinxAtStartPar
The neuron fired, so point p is pink.

\begin{sphinxadmonition}{note}{Observation}

\sphinxAtStartPar
A single MCP neuron with properly chosen weights can be used as a binary classifier.
\end{sphinxadmonition}


\subsection{Sample with an unknown classification rule}
\label{\detokenize{docs/perceptron:sample-with-an-unknown-classification-rule}}
\sphinxAtStartPar
At this point the reader may be a bit misled by the apparent triviality of the result. The confusion may stem from the fact that in our example we knew from the outset the rule defining the two classes of points (\(x_2>x_1\), or opposite). However, in a general “real life” situation this is frequently not the case! Imagine that we encounter the (labeled) data \sphinxstylestrong{samp2} looking like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.52395167 0.07006229 0.        ]
 [0.46580068 0.17984878 0.        ]
 [0.88313537 0.80437289 1.        ]
 [0.22527853 0.93083163 1.        ]
 [0.67041055 0.97779144 1.        ]]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{perceptron_26_0}.png}

\sphinxAtStartPar
The situation is in some sense inverted now. We have abtained from somewhere the (linearly separable) data, and want to find the rule that defines the two classes. In other words, we need to draw a dividing line, which is equivalent to finding the weights of the MCP neuron of \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}} that would carry out the classification.


\section{Perceptron algorithm}
\label{\detokenize{docs/perceptron:perceptron-algorithm}}
\sphinxAtStartPar
We could still try to figure out somehow the proper weights for the present example and find the dividing line, for instance with a ruler and pencil, but this is not the point. We wish to have a systematic algorithmic procedure that will effortlessly work for this one and any similar situation. The answer is the already mentioned \sphinxhref{https://en.wikipedia.org/wiki/Perceptron}{perceptron algorithm}.

\sphinxAtStartPar
Before presenting the algorithm, let us remark that the MCP neuron with some set of weigths \(w_0, w_1, w_2\) will always yield some answer for a labeled data point, correct or wrong. For example

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}           \PYG{c+c1}{\PYGZsh{} arbitrary choice of weights}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label  answer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} header}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} look at first 5 points}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{    }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} 
    \PYG{c+c1}{\PYGZsh{} samp2[i,2] is the label, samp2[i,:2] is [x\PYGZus{}1,x\PYGZus{}2]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
label  answer
0      1
0      0
1      1
1      0
1      1
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that some answers are equal to the corresponding labels (correct), and some are different (wrong). The general idea now is to \sphinxstylestrong{use the wrong answers} to adjust cleverly, in small steps, the weights, such that after many iterations we get all the answers for the training sample correct!

\begin{sphinxadmonition}{note}{Perceptron algorithm}

\sphinxAtStartPar
We iterate over the points of the training data sample.
If for a given point  the obtained result \(y_o\) is equal to the true value \(y_t\) (the label), i.e. the answer is  correct, we do nothing. However, if it is wrong, we change the weights a bit, such that the chance of getting the wrong answer decreses. The explicit recipe is as follows:

\sphinxAtStartPar
\(w_i \to w_i  +  \varepsilon  (y_t - y_o)  x_i\),

\sphinxAtStartPar
where \( \varepsilon \) is a small number (called the \sphinxstylestrong{learning speed}) and \(x_i\) are the coordinates of the input point, with \(i=0,\dots,n\).

\sphinxAtStartPar
Let us follow how it works. Suppose first that \( x_i> 0\). Then if the label \( y_t = 1 \) is greater than the obtained answer \( y_o = 0\), the weight \(w_i\) is increased. Then \( w \cdot x \) also increases and \( y_o = f (w \cdot x) \) is more likely to acquire the correct value of 1 (we remember how the step function \(f\) looks like). If, on the other hand, the label \( y_t = 0 \) is less than the obtained answer \( y_o = 1 \), then the weight \(w_i\) is decreased, \( w \cdot x \) decreases, and \( y_o = f (w \cdot x) \) has a better chance of achieving the correct value of 0.

\sphinxAtStartPar
If \( x_i < 0 \) it is easy to use the same method to check that the recipe also works properly.

\sphinxAtStartPar
When the anwer is correct, \(y_t=y_0\), then \( w_i \to w_i\), so nothing changes. We do not “spoil” the perceptron!

\sphinxAtStartPar
The above formula can be used many times for the same point from the traing sample. Next,  we loop over all the points of the sample, and the whole procedure can still be repeated in many rounds to obtain stable weights (not changing any more as we continue the procedure, or changing very slightly).

\sphinxAtStartPar
Typically, in such algorithms the learning speed \( \varepsilon \) is being decreased in successive rounds. This is technically very important, because too large steps can spoil the obtained solution.
\end{sphinxadmonition}

\sphinxAtStartPar
The Python implementation of the perceptron algorithm for the 2\sphinxhyphen{}dimesional data is as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w0}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}   \PYG{c+c1}{\PYGZsh{} initialize weights randomly in the range [\PYGZhy{}0.5,0.5]}
\PYG{n}{w1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}
\PYG{n}{w2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.3}  \PYG{c+c1}{\PYGZsh{} initialize the learning speed }
   
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over 20 rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} in each round decrease the learning speed }
        
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npo}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over the points from the data sample}
        
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} repeat 5 times for each points}
            
            \PYG{n}{yo} \PYG{o}{=} \PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} obtained answer}
            
            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} weight update (perceptron formula)}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{  w0     w1     w2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} header }
\PYG{n}{w\PYGZus{}o}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} obtained weights}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} result, rounded to 3 decimal places }
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  w0     w1     w2
[\PYGZhy{}0.441 \PYGZhy{}0.88   1.74 ]
\end{sphinxVerbatim}

\sphinxAtStartPar
It yields the result

\noindent\sphinxincludegraphics{{perceptron_37_0}.png}

\sphinxAtStartPar
We can see that the algorithm works! All the pink points are above the line, and all the blue ones below. Let us emphasize that the dividing line, given by the equation
\begin{equation*}
\begin{split} w_0+x_1 w_1 + x_2 w_2=0,\end{split}
\end{equation*}
\sphinxAtStartPar
does not result from our a priori knowledge, but from the training of the MCP neuron which
sets its weights.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
One can prove that the perceptrom algorith converges if and only if the data are linearly seperable.
\end{sphinxadmonition}

\sphinxAtStartPar
We may now reveal our secret! The data of our sample were labeled at the time of creation with the rule
\begin{equation*}
\begin{split} x_2>0.25+0.52 x_1 \end{split}
\end{equation*}
\sphinxAtStartPar
which corresponds to weights \(w_0^c=0.25\), \(w_1^c=-0.52\), \(w_2^c=1\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} weights used for labeling the training sample}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w\PYGZus{}c}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZhy{}0.25 \PYGZhy{}0.52  1.  ]
\end{sphinxVerbatim}

\sphinxAtStartPar
Note that this is not at all the same as the weights obtained from the training:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZhy{}0.441 \PYGZhy{}0.88   1.74 ]
\end{sphinxVerbatim}

\sphinxAtStartPar
The reason is twofold. First, note that the inequality condition \eqref{equation:docs/perceptron:eq-linsep} is unchanged if we multiply both sides by a \sphinxstylestrong{positive} constant \(c\). We may therefore scale all the weight by \(c\), and the situation (the answers of the MCP neuron, the dividing line) remains exactly the same (we encounter here an \sphinxstylestrong{equivalece class} of weight scaled with a positive factor).

\sphinxAtStartPar
For that reason, when we divide correspondingly the obtained weights by the weights used to label the sample, we get (almost) constant values:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{o}{/}\PYG{n}{w\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1.764 1.693 1.74 ]
\end{sphinxVerbatim}

\sphinxAtStartPar
The reason why the ratio values for \(i=0,1,2\) are not exactly the same is that the sample has a finite number of points (here 300). Thus, there is gap between the two classes of points and there is some room for “jiggling” the separating line a bit. With more data points this mismatch effect would decrease (see the homework problem below).


\subsection{Testing the classifier}
\label{\detokenize{docs/perceptron:testing-the-classifier}}
\sphinxAtStartPar
Due to the limited size of the training sample and the “jiggling” effect desribed above, the classification result on a test sample is sometimes wrong. This always applies to the points near the dividing line, which is determined with accuracy depending on the multiplicity of the training sample. The code below carries out the check on a test sample. The test sample consists of labeled data generated randomly “on the flight” with the same function \sphinxstylestrong{point2} as for the training data used before:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{point2}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random number from the range [0,1]}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZgt{}}\PYG{n}{x1}\PYG{o}{*}\PYG{l+m+mf}{0.52}\PYG{o}{+}\PYG{l+m+mf}{0.25}\PYG{p}{)}\PYG{p}{:}           \PYG{c+c1}{\PYGZsh{} condition met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 1}
    \PYG{k}{else}\PYG{p}{:}                          \PYG{c+c1}{\PYGZsh{} not met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 0}
\end{sphinxVerbatim}

\sphinxAtStartPar
The code for testing is as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{er}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} initialize an empty 1 x 3 array to store misclassified points}

\PYG{n}{ner}\PYG{o}{=}\PYG{l+m+mi}{0}                 \PYG{c+c1}{\PYGZsh{} initial number of misclassified points}
\PYG{n}{nt}\PYG{o}{=}\PYG{l+m+mi}{3000}               \PYG{c+c1}{\PYGZsh{} number of test points}

\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nt}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} run for nt points}
    \PYG{n}{ps}\PYG{o}{=}\PYG{n}{point2}\PYG{p}{(}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} a test point }
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{ps}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{!=}\PYG{n}{ps}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} if wrong answers                                      }
        \PYG{n}{er}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{er}\PYG{p}{,}\PYG{p}{[}\PYG{n}{ps}\PYG{p}{]}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} add the point to er}
        \PYG{n}{ner}\PYG{o}{+}\PYG{o}{=}\PYG{l+m+mi}{1}
        
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{number of misclassified points = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{ner}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ per }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{nt}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ (}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{ner}\PYG{o}{/}\PYG{n}{nt}\PYG{o}{*}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{ )}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}        
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
number of misclassified points =  12  per  3000  ( 0.4 \PYGZpc{} )
\end{sphinxVerbatim}

\sphinxAtStartPar
As we can see, a small number of test points are misclassified. All these points lie near the separating line.

\noindent\sphinxincludegraphics{{perceptron_51_0}.png}

\begin{sphinxadmonition}{note}{Misclassification}

\sphinxAtStartPar
As it became clear, the reason for misclassification comes from the fact that the training sample does not determine the separating line precisely, but with some uncertainty, as there is a gap between the points of the training smaple. For a better result, the training points would have to be “denser” in the vicinity of the separation line, or the training sample would have to be larger.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Exercises}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Play with the lecture code and see how the percentage of misclassified points decreases with the increasing size of the training sample.

\item {} 
\sphinxAtStartPar
As the perceptron algorithm converges, at some point the weights stop to change. Improve the lecture code by implementing stopping when the weights do not change when passing to the next round.

\item {} 
\sphinxAtStartPar
Generalize the above classifier to points in 3\sphinxhyphen{}dimensional space.

\end{itemize}
\end{sphinxadmonition}


\chapter{More layers}
\label{\detokenize{docs/more_layers:more-layers}}\label{\detokenize{docs/more_layers:more-lab}}\label{\detokenize{docs/more_layers::doc}}

\section{Two layers of neurons}
\label{\detokenize{docs/more_layers:two-layers-of-neurons}}
\sphinxAtStartPar
In the previous chapter we have seen that the MCP neuron with the step activation fuction realizes the inequality \(x \cdot w=w_0+x_1 w_1 + \dots x_n w_n > 0\), where \(n\) in the dimensionality of the input space. It is instructive to come up here with a geometric interpretation. Taking for definiteness \(n=2\) (the plane), the above inequality corresponds to a division into two half\sphinxhyphen{}planes. The line given by the equation
\begin{equation*}
\begin{split}x \cdot w=w_0+x_1 w_1 + \dots x_n w_n = 0\end{split}
\end{equation*}
\sphinxAtStartPar
is the \sphinxstylestrong{dividing line}.

\sphinxAtStartPar
Imagine now that we have more such conditions: two, three, etc., in general \(k\) independent conditions. Taking a conjunction of these conditions, we can build regions as shown, e.g., in \hyperref[\detokenize{docs/more_layers:regions-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:regions-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=620\sphinxpxdimen]{{regions}.png}
\caption{Sample convex regions in the plane obtained, from left to right, with one inequality condition, and a conjunction of 2, 3, or 4 inequality conditions, yielding \sphinxstylestrong{polygons}.}\label{\detokenize{docs/more_layers:regions-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Convex region}

\sphinxAtStartPar
By definition, region \(A\) is convex if and only if a straight line between any two points in \(A\) is contained in \(A\). A region which is not convex is called \sphinxstylestrong{concave}.
\end{sphinxadmonition}

\sphinxAtStartPar
Clearly, \(k\) inequality conditions can be imposed with \(k\) MCP neurons.
Recall from section {\hyperref[\detokenize{docs/mcp:bool-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Boolean functions}}}} that we can straightforwardly build boolean functions with the help of the neural networks. In particular, we can make a conjunction of \(k\) conditions by taking a neuron with the weights \(w_0=-1\) and \(1/k < w_i < 1/(k-1)\), where \(i=1,\dots,k\). One possibility is, e.g.,
\begin{equation*}
\begin{split}w_i=\frac{1}{k-\frac{1}{2}}.\end{split}
\end{equation*}
\sphinxAtStartPar
Indeed, let \(p_0=0\), and the conditions imposed by the inequalities be denoted as \(p_i\), \(i=1,\dots,k\), which may take values 1 or 0 (true or false). Then
\begin{equation*}
\begin{split}p \cdot w =-1 + p_1 w_1 + \dots + p_k w_k = -1+\frac{p_1+\dots p_k}{k-\frac{1}{2}} > 0\end{split}
\end{equation*}
\sphinxAtStartPar
if and only if all \(p_i=1\), i.e. all the conditions are true. This builds th AND gate for the incoming \(k\) boolean signals.

\sphinxAtStartPar
The architecture of networks for \(k=1\), 2, 3, or 4 conditions is shown in \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Starting from the second panel, we have networks with two layers of neurons, with \(k\) neurons in the intermediate layer, providing the inequality conditions, and one neuron in the output layer, acting as the AND gate. Of course, for one condition it is sufficient to have a single neuron, as in the left panel of \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=820\sphinxpxdimen]{{nf1-4}.png}
\caption{Networks capable of classifying data in the corresponding regions of \hyperref[\detokenize{docs/more_layers:regions-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:regions-fig}}}.}\label{\detokenize{docs/more_layers:nfn-fig}}\end{figure}

\sphinxAtStartPar
With the geometric interpretation, the first neuron layer represents the \(k\) half\sphinxhyphen{}planes, and the neuron in the second layer correspond to a convex region with \(k\) sides.

\sphinxAtStartPar
The situation generalizes in an obvious way to data in more dimensions. In that case we have more black dots in the inputs in  \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Geometrically, non \(n=3\) we deal with dividing planes and convex \sphinxhref{https://en.wikipedia.org/wiki/Polyhedron}{polyhedrons}, and for \(n>3\) with dividing \sphinxhref{https://en.wikipedia.org/wiki/Hyperplane}{hyperplanes} and convex \sphinxhref{https://en.wikipedia.org/wiki/Polytope}{polytopes}.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If there are numerous neurons \(k\) in the intermediate layer, the resulting polygon has many sides which may approximate a smooth boundary, such as an arc. The approximation is better and better as \(k\) increases.
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
A percepton with two\sphinxhyphen{}layers of neurons can classify points belonging to a convex region in \(n\)\sphinxhyphen{}dimensional space.
\end{sphinxadmonition}


\section{Three or more layers of neurons}
\label{\detokenize{docs/more_layers:three-or-more-layers-of-neurons}}
\sphinxAtStartPar
We have just shown that a two\sphinxhyphen{}layer network may classify a convex polygon. Imagine now that we produce two such figures in the second layer of neurons, for instane as in the following network:

\noindent\sphinxincludegraphics{{more_layers_13_0}.png}

\sphinxAtStartPar
Note that the first and second neuron layers are not fully connected here, as we “stack on top of each other” two networks producing triangles, as in the third panel of \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Next, in the third neuron layer (here having a single neuron) we implement a \(p \,\wedge \!\sim\!q\) gate, i.e. the conjunction of the conditions that the points belong to one triangle and do not belong to the other one. As we will show shortly, with appropriate weights, the above network may produce a concave region, for example a triangle with a triangular hollow:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{tritri}.png}
\caption{Triangle with a tringular hollow.}\label{\detokenize{docs/more_layers:tri-fig}}\end{figure}

\sphinxAtStartPar
Generalizing this argument to other shapes, one can show an important theorem that

\begin{sphinxadmonition}{important}{Important:}
\sphinxAtStartPar
A perceptron with three or more layers with sufficiently many neurons can classify points belonging to \sphinxstylestrong{any} region in \(n\)\sphinxhyphen{}dimensional space with \(n-1\)\sphinxhyphen{}dimensional hyperplane boundaries.
\end{sphinxadmonition}

\sphinxAtStartPar
It is worth stressing here that three layers provide full functionality! Adding more layers to a classifier does not increase its capabilities.


\section{Feeding forward in Python}
\label{\detokenize{docs/more_layers:feeding-forward-in-python}}
\sphinxAtStartPar
Before proceeding with an explicit example, we need a Python code for propagation of the signal in a general fully\sphinxhyphen{}connected feed\sphinxhyphen{}forward network. First, we represent the architecture of a network with \(l\) neuron layers as an array of the form
\begin{equation*}
\begin{split}[n_0,n_1,n_2,...,n_l],\end{split}
\end{equation*}
\sphinxAtStartPar
where \(n_0\) in the number of the input nodes, and \(n_i\) are the numbers of neurons in layers \(i=1,\dots,l\). For instance, the architecture of the network from the fourth panel of \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}} is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} 
\PYG{n}{arch}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2, 4, 1]
\end{sphinxVerbatim}

\sphinxAtStartPar
In the codes of this course we use the convention of \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Fig.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}, namely, the bias is treated uniformly with the remaining signal. However, the bias notes are not included in the numbers \(n_i\) defined above. In particular, a more detailed view of the fourth panel of \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}} is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{more_layers_22_0}.png}

\sphinxAtStartPar
Here, black dots mean input, gray dots indicate the bias nodes carrying input =1, and the blue blobs are the neurons.

\sphinxAtStartPar
Next, we need the weights of the connections. There are \(l\) sets of weights, each one corresponding to the set of edges entering a given neuron layer from the left.
In the above example, the first neuron layer (blue blobs to the left) has weights which form a \(3 \times 4\) matrix. Here 3 is the number of nodes in the preceding layer (including the bias node) and 4 is the number of neurons in the first neuron layer. Similarly, the weights associated with the second (output) layer form a \(4 \times 1\) matrix. Hence, in our convention, the weight matrices corresponding to subsequent neuron layers \(1, 2, \dots, l\) have dimensions
\begin{equation*}
\begin{split}
(n_0+1)\times n_1, \; (n_1+1)\times n_2, \; \dots \; (n_{l-1}+1)\times n_l.
\end{split}
\end{equation*}
\sphinxAtStartPar
To store all the weights of a network we actually need \sphinxstylestrong{three} indices: one for the layer, one for the number of nodes in the preceding layer, and one for the number of nodes in the given layer. We could have used a three\sphinxhyphen{}dimensional array here, but since we number the neuron layers staring from 1, and arrays start numbering from 0, it is more convenient to use the Python \sphinxstylestrong{dictionary} structure. We will then store the weights as
\begin{equation*}
\begin{split}w=\{1: arr^1, 2: arr^2, ..., l: arr^l\},\end{split}
\end{equation*}
\sphinxAtStartPar
where \(arr^i\) is a \sphinxstylestrong{two\sphinxhyphen{}dimensional} array (matrix) of weights for the neuron layer \(i\). For the case of the above figure we could thus take, for instance

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 1.   2.   1.   1. ]
 [ 2.  \PYGZhy{}3.   0.2  2. ]
 [\PYGZhy{}3.  \PYGZhy{}3.   5.   7. ]]

[[ 1. ]
 [ 0.2]
 [ 2. ]
 [ 2. ]
 [\PYGZhy{}0.5]]
\end{sphinxVerbatim}

\sphinxAtStartPar
For the signal propagating along the network we also use a dictionary of the form
\begin{equation*}
\begin{split}x=\{0: x^0, 1: x^1, 2: x^2, ..., l: x^l\},\end{split}
\end{equation*}
\sphinxAtStartPar
where \(x^0\) is the input, and \(x^i\) is the output leaving the neuron layer \(i\), with \(i=1, \dots, l\). All symbols \(x^j\), \(j=0, \dots, l\), are one\sphinxhyphen{}dimensional arrays. The bias nodes are included, hence the dimensions of \(x^j\) are \(n_j+1\), except for the ouput layer which has no bias node, hence \(x^l\) has dimension \(n_l\). In other words, the dimensions of the signal arrays are equal to the total number of nodes in each layer.

\sphinxAtStartPar
Next, we present the corresponding formulas in rather painful detail, as this is key to avoid any possible confusion related to the notation.
We already know from \eqref{equation:docs/mcp:eq-f0} that for a single neuron with \(n\) inputs its incoming signal is calculated as
\begin{equation*}
\begin{split}s = x_0 w_0 + x_1 w_1 + x_2 w_2 + ... + x_n w_n = \sum_{\beta=0}^n x_\beta w_\beta .\end{split}
\end{equation*}
\sphinxAtStartPar
With more layers (index \(i\)) and neurons (\(n_i\) in layer \(i\)),
the notation generalizes into
\begin{equation*}
\begin{split}
s^i_\alpha=\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l.
\end{split}
\end{equation*}
\sphinxAtStartPar
Note that the summation starts from \(\beta=0\) to account for the bias node in the preceding layer \((i-1)\), but \(\alpha\) starts from 1, as only neurons (and not the the bias node) in layer \(i\) receive the signal (see the figure below).

\sphinxAtStartPar
In the algebraic matrix notation, we can also write more compactly
\(s^{iT} = x^{(i-1)T} W^i\), with \(T\) denoting transposition. Explicitly,
\begin{equation*}
\begin{split}
\begin{pmatrix} s^i_1 & s^i_2 & ...& s^i_{n_i} \end{pmatrix} = 
\begin{pmatrix} x^{i-1}_0 & x^{i-1}_1 & ...& x^{i-1}_{n_{i-1}} \end{pmatrix}
\begin{pmatrix} w^i_{01} & w^i_{02} & ...& w^i_{0,n_i} \\ w^i_{11} & w^i_{12} & ...& w^i_{1,n_i} \\ 
 ... & ... & ...& ... \\ w^i_{n_{i-1}1} & w^i_{n_{i-1}2} & ...& w^i_{n_{i-1}n_i} \end{pmatrix}.
\end{split}
\end{equation*}
\sphinxAtStartPar
As we already know very well, the output from a neuron is obtained by acting on its incoming input with an activation function. Thus we finally have
\begin{equation*}
\begin{split} 
x^i_\alpha  = f(s^i_\alpha) = f \left (\sum_{\beta=0}^{n_{i-1}} x^{(i-1)}_\beta w^i_{\beta \alpha} \right), \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l , \\
x^i_0 =1, \;\; i=1,\dots,l-1,  
\end{split}
\end{equation*}
\sphinxAtStartPar
with the bias nodes set to one.
The figure below illustrates the scheme:

\noindent\sphinxincludegraphics{{more_layers_28_0}.png}

\sphinxAtStartPar
The implementation of the feed\sphinxhyphen{}forward propagation explained above in Python is following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: }
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    f \PYGZhy{} activation function (default: step)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: }
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over layers except the last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication }
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{f}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

                                  \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal   }
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{f}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we test how \sphinxstylestrong{feed\_forward} works on a sample input. For brevity, we do not pass the input bias node of the input in the argument. It is added inside the function.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}0: array([ 1,  2, \PYGZhy{}1]), 1: array([1, 1, 0, 0, 0]), 2: [1]\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
The final output of this network is obtained as

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}


\section{Visualization}
\label{\detokenize{docs/more_layers:visualization}}
\sphinxAtStartPar
For visualization of simple networks, in the \sphinxstylestrong{neural} library we provide some drawing functions which show the weights, as well as the signals. Function \sphinxstylestrong{plot\_net\_w} draws positive weights in red and negative in blue, with the widths reflecting their magnitude. The last parameter, here 0.5, rescales  the widths such that the graphics looks nice. Function \sphinxstylestrong{plot\_net\_w\_x}  prints in addition the values of the signal leaving the neurons in each layer.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{more_layers_37_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{more_layers_38_0}.png}


\section{Classifier with three neuron layers}
\label{\detokenize{docs/more_layers:classifier-with-three-neuron-layers}}
\sphinxAtStartPar
We are now ready to explicitly construct an example of a binary classifier of points in a concave region: a triagle with a triangular hollow of \hyperref[\detokenize{docs/more_layers:tri-fig}]{Fig.\@ \ref{\detokenize{docs/more_layers:tri-fig}}}.
The network architecture is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{more_layers_41_0}.png}

\sphinxAtStartPar
The geometric conditions and the corresponding weights for the first neuron layer are


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
inequality condition
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^1\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\(x_1>0.1\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\(x_2>0.1\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.1
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
\(x_1+x_2<1\)
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
\(x_1>0.25\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.25
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
\(x_2>0.25\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.25
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
\(x_1+x_2<0.8\)
&
\sphinxAtStartPar
0.8
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Conditions 1\sphinxhyphen{}3 provide boundaries for the bigger traingle, and 4\sphinxhyphen{}6 for the smaller one contained in the bigger one.
In the second neuron layer we need to realize two AND gates for conditions 1\sphinxhyphen{}3 and 4\sphinxhyphen{}6, espoectively, hence we take


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{3\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{4\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{5\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{6\alpha}^2\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Finally, in the output layer we take the \(p \wedge \! \sim \! q\)  gate, hence


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^3\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
1.2
&
\sphinxAtStartPar
\sphinxhyphen{}0.6
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Putting all togethr, the weight dictionary is

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{l+m+mi}{3}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{1.2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Feeding forward a sample input yields

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mf}{0.3}\PYG{p}{]}
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{)}
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{more_layers_45_0}.png}

\sphinxAtStartPar
We have just found that point {[}0.2,0.3{]} is within our region (1 from the output layer). Actually, we we have more information from the intermediae layers. From the second neuron layer we know that the point belongs to the bigger triangle (1 from the lower neuron) and does not belong to the smaller triangle (0 from the upper neuron). From the first neuron layer we may read the condition from the six inequalities.

\sphinxAtStartPar
Next, we will test how our network works for other points. First, we define a function generating a  random point in the square \([0,1]\times [0,1]\) and pass it through the network. We assign to it label 1 if it belongs to the requested triangle with the hollow, and 0 otherwise. Subsequently, we create a large sample of such points and generate the graphics, using pink for label 1 and blue for label 0.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{po}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} random point from the [0,1]x[0,1] square}
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} run feed forward}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{xi}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} the point\PYGZsq{}s coordinates and label}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{po}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.83806748 0.62718519 0.        ]
 [0.46726263 0.34999938 1.        ]
 [0.69545367 0.95397076 0.        ]
 [0.19930639 0.52982584 1.        ]
 [0.41437336 0.69846145 0.        ]]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{more_layers_50_0}.png}

\sphinxAtStartPar
We can see that our little machine works perfectly well!

\sphinxAtStartPar
At this point the reader might rightly say that the preceding results are trivial: in essence, we have just been implementing some geometric conditions and their conjunctions.

\sphinxAtStartPar
However, there is an important case against this apparent triviality. Imagine we have the data sample with labels, and only this, as in the example of the single MCP neuron of chapter {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{MCP Neuron}}}}. Then we do not have the dividing conditions to begin with and need some efficient way to find them. This is exacly what teaching of classifiers does: its sets the weights in such a way that the proper conditions are implicitly built in. After the material of this section, the reader should be convinced that this is perfectly possible.


\chapter{Back propagation}
\label{\detokenize{docs/backprop:back-propagation}}\label{\detokenize{docs/backprop::doc}}

\section{Minimizing the error}
\label{\detokenize{docs/backprop:minimizing-the-error}}
\sphinxAtStartPar
We now go back to the perceptron algorithm of chapter {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}} to look in some more detail at its performance as a function of weights. Note that in our example with points on the plane the condition for the pink points is given by the inequality

\sphinxAtStartPar
\(w_0+w_1 x_1 + w_2 x_2 > 0\).

\sphinxAtStartPar
We have already mentioned the equivalence class related to dividing this inequality with a
positive constant. In general, at least one of the weights must be nonzero to have a nontrivial condition. Suppose or definiteness that \(w_0 \neq 0\) (other cases may be treated analogously). Then we can divide both sides with \(|w_0|\)
\begin{equation*}
\begin{split}\frac{w_0}{|w_0|}+\frac{w_1}{|w_0|} \, x_1 + \frac{w_2}{|w_0|} \, x_2 > 0. \end{split}
\end{equation*}
\sphinxAtStartPar
Introducing \(v_1=\frac{w_1}{w_0}\) and \(v_2=\frac{w_2}{w_0}\), this can be rewritten in the form
\begin{equation*}
\begin{split}{\rm sgn}(w_0)( 1+v_1 \, x_1 +v_2 \, x_2) > 0,\end{split}
\end{equation*}
\sphinxAtStartPar
where \({\rm sgn}(w_0) = \frac{w_0}{|w_0|}\), hence we effectively have a two\sphinxhyphen{}parameter system (for each sign of \(w_0\)).

\sphinxAtStartPar
Obviously, with some values of \( v_1 \) and \( v_2 \) and for a given point from the sample, the perceptron will provide a correct or incorrect answer. It is thus natural to define the \sphinxstylestrong{error function} \(E\) such that each point of \(p\) from the sample contributes 1 if the answer is incorrect, and 0 if it is correct:
\begin{equation*}
\begin{split} E(v_1,v_2)=\sum_p \left\{ \begin{array}{ll} 1 -{\rm incorrect,~}\\ 0 -{\rm correct} \end{array}\right .\end{split}
\end{equation*}
\sphinxAtStartPar
\(E\) is thus the number of misclassified points. We can easily construct this function for a labeled data sample in the format {[}x1, x2, label{]}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{error}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,} \PYG{n}{w1} \PYG{p}{,}\PYG{n}{w2}\PYG{p}{,} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    error function for the perceptron (for 2\PYGZhy{}dim data with labels)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    inputs:}
\PYG{l+s+sd}{    w0, w1, w2 \PYGZhy{} weights}
\PYG{l+s+sd}{    sample \PYGZhy{} labeled data sample in format [x1, x1, label]}
\PYG{l+s+sd}{    f \PYGZhy{} activation function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    returns:}
\PYG{l+s+sd}{    error}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{er}\PYG{o}{=}\PYG{l+m+mi}{0}                           \PYG{c+c1}{\PYGZsh{} initial value of error}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over data points       }
        \PYG{n}{yo}\PYG{o}{=}\PYG{n}{f}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} obtained answer}
        \PYG{n}{er}\PYG{o}{+}\PYG{o}{=}\PYG{p}{(}\PYG{n}{yo}\PYG{o}{\PYGZhy{}}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
                      \PYG{c+c1}{\PYGZsh{} sample[i,2] is the label}
                      \PYG{c+c1}{\PYGZsh{} adds the square of the difference of yo and the label}
                      \PYG{c+c1}{\PYGZsh{} this adds 1 if the answer is incorrect, and 0 if correct}
    \PYG{k}{return} \PYG{n}{er}  \PYG{c+c1}{\PYGZsh{} the error}
\end{sphinxVerbatim}

\sphinxAtStartPar
Atually, we have used a little trick here, in b=view if the future developments. Denoting the obtained result for a given data point as \(y_o^{(p)}\) and the true result (label) as \(y_t^{(p)}\) (both have values 0 or 1), we may write equivalently
\begin{equation*}
\begin{split} E(v_1,v_2)=\sum_p \left ( y_o^{(p)}-y_t^{(p)}\right )^2,\end{split}
\end{equation*}
\sphinxAtStartPar
which is the programmed formula. Indeed, when  \(y_o^{(p)}=y_t^{(p)}\) (correct answer) the contribution of the point is 0, and when \(y_o^{(p)}\neq y_t^{(p)}\) (wrong answer) the contribution is \((\pm 1)^2=1\).

\sphinxAtStartPar
We repeat the simulations of chapter {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}} for the sample \sphinxstylestrong{samp2} of 200 points (the sample was built with \(w_0=-0.25\), \(w_1=-0.52\), and \(w_2=1\), which corresponds to \(v_1=2.08\) and \(v_2=-4\), with \({\rm sgn}(w_0)=-1\)). Then we evaluate the error function \(E(v_1,v_2)\).

\sphinxAtStartPar
Next, we run the perceptron alogorithm:

\sphinxAtStartPar
We note above that the final error is very small or 0 (depending on the particular simulation). It is illuminating to look at a contour map of the error function \(E(v_1, v_2)\) in the vicinity of the optimal parameters:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.7}\PYG{p}{,}\PYG{l+m+mf}{3.7}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{0.01} \PYG{c+c1}{\PYGZsh{} grid step in v1 and v2 for the contour map}
\PYG{n}{ran}\PYG{o}{=}\PYG{l+m+mf}{0.5}       \PYG{c+c1}{\PYGZsh{} plot range around (v1\PYGZus{}o, v2\PYGZus{}o)}

\PYG{n}{v1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,}\PYG{n}{v1\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} grid for v1}
\PYG{n}{v2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,}\PYG{n}{v2\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} grid for v2}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{v1}\PYG{p}{,} \PYG{n}{v2}\PYG{p}{)} 

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{error}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{n}{v1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{n}{v2}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)} 
             \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{v1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{v2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} values of E(v1,v2) }

\PYG{n}{CS} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{35}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{clabel}\PYG{p}{(}\PYG{n}{CS}\PYG{p}{,} \PYG{n}{inline}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}1.0f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} contour labels}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}aspect}\PYG{p}{(}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}v\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}v\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{found minimum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} our found optimal point}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;} 
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_12_0}.png}

\sphinxAtStartPar
We can see in the plot that that the found minimum is in (or close to, depending on the simulation) the elongated region of \( v_1 \) and \( v_2\) where the error vanishes.


\section{Continuous activation function}
\label{\detokenize{docs/backprop:continuous-activation-function}}
\sphinxAtStartPar
Coming back to the contour chart above, we can see that the lines are “serrated”. This is because the error function, for an obvious reason, assumes integer values. It is therefore discontinuous and non\sphinxhyphen{}differentiable. The discontinuities obviously originate from the discontinuous activation function, i.e. the step function. Having in mind the techniques we will get to know soon, it is advantageous to use the continuous activation functions. Historically, the so\sphinxhyphen{}called \sphinxstylestrong{sigmoid}
\begin{equation*}
\begin{split} \sigma(s)=\frac{1}{1+e^{-s}}\end{split}
\end{equation*}
\sphinxAtStartPar
has been used in many practical applications of ANNs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sigmoid, a.k.a. the logistic function, or simply (1+arctanh(\PYGZhy{}s/2))/2 }
\PYG{k}{def} \PYG{n+nf}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{start}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{stop}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_18_0}.png}

\sphinxAtStartPar
This function is of course differentiable. Moreover,

\sphinxAtStartPar
\( \sigma '(s) = \sigma (s) [1- \sigma (s)] \),

\sphinxAtStartPar
which is its special feature.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} derivative of sigmoid}
\PYG{k}{def} \PYG{n+nf}{dsig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
     \PYG{k}{return} \PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{start}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{stop}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Derivative of sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;} 
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_21_0}.png}

\sphinxAtStartPar
A sigmoid with “temperature” \( T \) is also introduced (this nomenclature is associated with similar expressions for thermodynamic functions in physics):
\begin{equation*}
\begin{split}\sigma(s;T)=\frac{1}{1+e^{-s/T}}.\end{split}
\end{equation*}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sigmoid with temperature T}
\PYG{k}{def} \PYG{n+nf}{sig\PYGZus{}T}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{T}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{o}{/}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_24_0}.png}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
For smaller and smaller \( T \), it approaches the previously used step function. Note that the argument of the sigmoid is the quotient

\sphinxAtStartPar
\$\( s / T = (w_0 + w_1 x_1 + w_2 x_2) / T = w_0 / T + w_1 / T \, x_1 + w_2 / T \, x_2 = \xi_0 + xi_1 x_1 + xi_2 x_2 \)\$,

\sphinxAtStartPar
which means that we can always assume \( T = 1 \) without losing generality (\( T \) is the “scale”). However, we now have three independent arguments \( \xi_0 \), \( \xi_1 \), and \( \xi_2\). Thus, it is impossible to reduce the situation to two independent parameters, as was the case above.
\end{sphinxadmonition}

\sphinxAtStartPar
We will now repeat our example with the classifier, but with the activation function given by the sigmoid. The error function, with
\begin{equation*}
\begin{split}y_o^{(p)}=\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)}). \end{split}
\end{equation*}
\sphinxAtStartPar
becomes
\begin{equation*}
\begin{split}E(w_0,w_1,w_2)=\sum_p \left [\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)} \right]^2.\end{split}
\end{equation*}
\sphinxAtStartPar
We run the perceptron alorithm with the sigmoid activation function 500 times:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} random weights from [\PYGZhy{}0.5,0.5]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0   w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                       \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9995}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update weights}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99:
        \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} updated weights }
        \PYG{n}{w1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{w2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{w1\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{w2\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}                             
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   w0   w1/w0  w2/w0 error
\PYGZhy{}19.439 2.253 \PYGZhy{}4.118 0.62695
\PYGZhy{}24.146 2.288 \PYGZhy{}4.16 0.43044
\PYGZhy{}27.015 2.32 \PYGZhy{}4.208 0.27756
\PYGZhy{}29.122 2.344 \PYGZhy{}4.246 0.19078
\PYGZhy{}30.81 2.362 \PYGZhy{}4.274 0.14195
\PYGZhy{}32.227 2.376 \PYGZhy{}4.295 0.11308
\PYGZhy{}33.452 2.386 \PYGZhy{}4.31 0.09482
\PYGZhy{}34.529 2.393 \PYGZhy{}4.32 0.08244
\PYGZhy{}35.487 2.398 \PYGZhy{}4.328 0.07349
\PYGZhy{}36.347 2.402 \PYGZhy{}4.334 0.06667
\end{sphinxVerbatim}

\sphinxAtStartPar
The error function now has 3 arguments, so it cannot be drawn in two dimensions. We can, however, look at its projections, e.g. with a fixed value of \( w_0 \).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.7}\PYG{p}{,}\PYG{l+m+mf}{3.7}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{0.5}
\PYG{n}{ran}\PYG{o}{=}\PYG{l+m+mi}{20} 
\PYG{n}{r1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{w1\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{w1\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} 
\PYG{n}{r2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{w2\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{w2\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} 
\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{r1}\PYG{p}{,} \PYG{n}{r2}\PYG{p}{)} 

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{n}{r1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{r2}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)} 
             \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{r1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{r2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}  

\PYG{n}{CS} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{35}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{clabel}\PYG{p}{(}\PYG{n}{CS}\PYG{p}{,} \PYG{n}{inline}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}1.0f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error function for \PYGZdl{}w\PYGZus{}0\PYGZdl{}=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}aspect}\PYG{p}{(}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}w\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}w\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{w1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w2\PYGZus{}o}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{optimum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} our found optimal point}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_29_0}.png}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the present example, when we carry out more and more iterations, we notice that the magnitude of weights becomes lager and lager, while the error naturally gets smaller. The reason is following: our data sample is separable, so in the case when the step function is used for activation, it is possible to separate the sample with the dividing line and get down with the error to zero. In the case of the sigmoid, there is always some (tiny) contribution to the error, as the values are between 0 and 1. As we have discussed above, in the sigmoid, whose argument is \( (w_0 + w_1 x_1 + w_2 x_2) / T\), increasing the weights is equivalent to scaling down the temperature. Then, however, the sigmoid approaches the step function, and the error tends to zero. Precisely this behavior is seen in the simulations.
\end{sphinxadmonition}


\section{Steepest descent}
\label{\detokenize{docs/backprop:steepest-descent}}
\sphinxAtStartPar
Generic comment on minimization …

\sphinxAtStartPar
For a differentiable function of multiple variables, \( F (z_1, z_2, ..., z_n) \), locally the steepest slope is defined by the minus gradient of the function \( F \), i.e. the slope is in the direction of the vector
\begin{equation*}
\begin{split}-\left (\frac{\partial F}{\partial z_1}, \frac{\partial F}{\partial z_2}, ..., 
\frac{\partial F}{\partial z_n} \right ), \end{split}
\end{equation*}
\sphinxAtStartPar
where the partial derivatives are defined as the limit
\begin{equation*}
\begin{split}\frac{\partial F}{\partial z_1} =  \lim _ {\Delta \to 0} \frac {F (z_1 + \Delta, z_2, ..., z_n) -F (z_1, z_2, ..., z_n)} { \Delta}, \end{split}
\end{equation*}
\sphinxAtStartPar
and similarly for the other \( z_i \).

\sphinxAtStartPar
The method of finding the minimum of a function by the steepest descent method is given by the iterative algorithm, where we update the position at each iteration step \(m\) with
\begin{equation*}
\begin{split}z_{i}^{(m+1)} = z_i^{(m)} - \epsilon  \, \frac{\partial F}{\partial z_i}. \end{split}
\end{equation*}
\sphinxAtStartPar
In our case, we minimize the error function
\begin{equation*}
\begin{split}E(w_0,w_1,w_2)= \sum_p [y_o^{(p)}-y_t^{(p)}]^2=\sum_p [\sigma(s^{(p)})-y_t^{(p)}]^2=\sum_p [\sigma(w_0  x_0^{(p)}+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)}]^2, \end{split}
\end{equation*}
\sphinxAtStartPar
hence
\begin{equation*}
\begin{split} \frac{\partial E}{\partial w_i} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma'(s^{(p)}) \,x_i^{(p)} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma(s^{(p)})\, [1-\sigma(s^{(p)})] \,x_i^{(p)}\end{split}
\end{equation*}
\sphinxAtStartPar
(derivative of square function \( \times \) derivative of the sigmoid \( \times \) derivative of \( s ^ {(p)} \)), where we have used the special property of the sigmoid derivative in the last equality. The steepest descent method updates the weights as follows:
\begin{equation*}
\begin{split}w_i \to w_i - \varepsilon (y_o^{(p)} -y_t^{(p)}) y_o^{(p)} (1-y_o^{(p)}) x_i.\end{split}
\end{equation*}
\sphinxAtStartPar
Note that updating always occurs, because the response \( y_o^ {(p)} \) is never strictly 0 or 1 for the sigmoid, whereas
the true value (label) \( y_t ^ {(p)} \) is 0 or 1.

\sphinxAtStartPar
Because \( y_o ^ {(p)} (1-y_o ^ {(p)}) = \sigma (s ^ {(p)}) [1- \sigma (s ^ {(p)})] \) is nonzero only around \( s ^ {(p)} = \) 0 (see the sigmoid derivative plot earlier), thus updating only occurs near the “threshold”. This is fine, as the “problems” are near the dividing line.

\sphinxAtStartPar
For comparison, the earlier perceptron algorithm is structurally very similar,
\begin{equation*}
\begin{split}w_i \to w_i - \varepsilon \,(y_o^{(p)} - y_t^{(p)}) \, x_i,\end{split}
\end{equation*}
\sphinxAtStartPar
but here the updating occurs for all points of the sample, not just near the threshold.

\sphinxAtStartPar
The code for the learning algorith with the steepest descent update of weights is following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Steepest descent for a single perceptron}

\PYG{k}{def} \PYG{n+nf}{teach\PYGZus{}sd}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{w\PYGZus{}in}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{:}
    \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}\PYG{o}{=}\PYG{n}{w\PYGZus{}in} 
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} 
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}      
            
            \PYG{n}{yo}\PYG{o}{=}\PYG{n}{f}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
            
            \PYG{c+c1}{\PYGZsh{} update of weights}
            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{1}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{return} \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
Its performance is similar to perceptron algorithm studied above.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} random weights from [\PYGZhy{}0.5,0.5]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0   w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                       \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9995}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}sd}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update weights}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99:
        \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} updated weights }
        \PYG{n}{w1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{w2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{w1\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{w2\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}                             

               
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   w0   w1/w0  w2/w0 error
\PYGZhy{}9.485 2.271 \PYGZhy{}4.147 1.63446
\PYGZhy{}11.975 2.297 \PYGZhy{}4.173 1.16447
\PYGZhy{}13.537 2.321 \PYGZhy{}4.21 0.92417
\PYGZhy{}14.687 2.339 \PYGZhy{}4.24 0.779
\PYGZhy{}15.607 2.353 \PYGZhy{}4.262 0.68315
\PYGZhy{}16.375 2.364 \PYGZhy{}4.279 0.6152
\PYGZhy{}17.036 2.372 \PYGZhy{}4.291 0.56424
\PYGZhy{}17.615 2.378 \PYGZhy{}4.3 0.52435
\PYGZhy{}18.129 2.382 \PYGZhy{}4.307 0.49208
\PYGZhy{}18.589 2.386 \PYGZhy{}4.313 0.46532
\end{sphinxVerbatim}

\sphinxAtStartPar
The problems with finding the minimum of multivariable functions are well known:
\begin{itemize}
\item {} 
\sphinxAtStartPar
There may be local minima, and therefore it may be difficult to find the global minimum (see example above with a function with two minima).

\item {} 
\sphinxAtStartPar
The minimum can be at infinity (that is, it does not exist mathematically).

\item {} 
\sphinxAtStartPar
The function around the minimum can be very flat, so the gradient is very small, and the update is extremely slow.

\item {} 
\sphinxAtStartPar
Numerical accuracy can be a problem.

\end{itemize}

\sphinxAtStartPar
Overall, numerical minimization of functions is an art!


\section{Backprop algorithm}
\label{\detokenize{docs/backprop:backprop-algorithm}}
\sphinxAtStartPar
The material of this section is absolutely \sphinxstylestrong{crucial} to understanding this very important idea of training neural networks. At the same time, it can be quite difficult for people less familiar with mathematical analysis, as there will be derivations and formulas with rich notation. However, this cannot be presented more simply than below, with the necessary accuracy.

\sphinxAtStartPar
In the example above, we knew in advance with what recipe we were generating points, so in this fortunate and rare situation, we were able to determine the weights exactly by simple reasoning. In general, this is not the case. As for the single neuron in the previous lecture, we want to train our network on the training sample. The difference is that now there are two layers (apart from the input) and two sets of weights: between the input and the middle layer and between the middle layer and the output.

\sphinxAtStartPar
The method we derive step by step here, which is the famous \sphinxstylestrong{back propagation algorithm (backprop)} {[}Arthur E. Bryson, Yu\sphinxhyphen{}Chi Ho, 1969{]} for updating the weights of a multi\sphinxhyphen{}layer network, uses two elements:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the \sphinxstylestrong{chain rule} for computing the derivative of a composite function, known to you from the mathematical analysis, and

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{steepest descent method}, explained in the previous lecture.

\end{itemize}

\begin{sphinxadmonition}{note}{Chain rule}

\sphinxAtStartPar
For a composite function

\sphinxAtStartPar
\([f(g(x))]' = f'(g(x)) g'(x)\).

\sphinxAtStartPar
For a composition of more functions \([f(g(h(x)))]' = f'(g(h(x))) \,g'(h(x)) \,h'(x)\), etc.
\end{sphinxadmonition}

\sphinxAtStartPar
We now move on to formulating the back propagation algorithm for a perceptron with any number of neuron layers, \(l\), and any number of \(n_l\) neurons in the output layer. This generalization is conceptually very simple and is based on the same reasoning as for the case of single neuron presented earlier, involving the chain rule and the steepest descent. On the other hand, the notation becomes quite cumbersome, which can make the material seem difficult.

\sphinxAtStartPar
The neurons in intermediate layers \(j=1,\dots,l-1\) are numbered with corresponding indices \(\alpha_j=0,\dots,n_j\), with 0 indicating the bias node. In the output layer, having no bias node, the numbering is \(\alpha_l=1,\dots,n_l\).
The error function introduced earlier is a sum over the points of the training sample and over the nodes in the otput layer:
\begin{equation*}
\begin{split}
E(\{w\})=\sum_p \sum_{\alpha_l=1}^{n_l} \left[ y_{o,{\alpha_l}}^{(p)}(\{w\})-y_{t,{\alpha_l}}^{(p)}\right]^2,
\end{split}
\end{equation*}
\sphinxAtStartPar
where \( \{w \} \) represent all the network weights.
We will deal with a single point contribution to \(E\), denoted as \( e \).
It is a sum over all neurons in the output layer:
\begin{equation*}
\begin{split}
e(\{w\})= \sum_{{\alpha_l}=1}^{n_l}\left[ y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right]^2, 
\end{split}
\end{equation*}
\sphinxAtStartPar
where we have dropped the superscript \((p)\) for brevity.
For neuron \(\alpha_j\) in layer \(j\) the entering signal is
\begin{equation*}
\begin{split}
s_{\alpha_j}^{j}=\sum_{\alpha_{j-1}=0}^{n_{j-1}} x_{\alpha_{j-1}}^{j-1} w_{\alpha_{j-1} \alpha_j}^{j}.
\end{split}
\end{equation*}
\sphinxAtStartPar
The outputs from the output layer are
\begin{equation*}
\begin{split}
y_{o,{\alpha_l}}=f\left( s_{\alpha_l}^{l} \right)
\end{split}
\end{equation*}
\sphinxAtStartPar
whereas the output signals in the intermediate layers \(j=1,\dots,l-1\) are
\begin{equation*}
\begin{split}
x_{\alpha_j}^{j}=f \left ( s_{\alpha_j}^{j}\right ),\;\;\;\alpha_{j}=1,\dots,n_j, \;\;\; {\rm and} \;\;\; x_0^{j}=1,
\end{split}
\end{equation*}
\sphinxAtStartPar
with the bias node having the value 1.

\sphinxAtStartPar
Subsequent explicit substitutions of the above formulas into \(e\) are as follows:

\sphinxAtStartPar
\(e = \sum_{{\alpha_l}=1}^{n_l}\left( y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( f \left (\sum_{\alpha_{l-1}=0}^{n_{l-1}} x_{\alpha_{l-1}}^{l-1} w_{\alpha_{l-1} {\alpha_l}}^{l} \right )-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f \left( \sum_{\alpha_{l-2}=0}^{n_{l-2}} x_{\alpha_{l-2}}^{l-2} w_{\alpha_{l-2} \alpha_{l-1}}^{l-1}\right) w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 \gamma}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\sum_{\alpha_{l-2}=1}^{n_{l-2}} f\left( \sum_{\alpha_{l-3}=0}^{n_{l-3}} x_{\alpha_{l-3}}^{l-3} w_{\alpha_{l-3} \alpha_{l-2}}^{l-2}\right) w_{\alpha_{l-2} \alpha_{l-1}}^{l-1} + 
x_{0}^{l-2} w_{0 \alpha_{l-1}}^{l-1}
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\dots f\left( \sum_{\alpha_{0}=0}^{n_{0}} x_{\alpha_{0}}^{0} w_{\alpha_{0} \alpha_{1}}^{1}\right) w_{\alpha_{1} \alpha_{2}}^{2} + 
x_{0}^{1} w_{0 \alpha_{2}}^{2} \dots
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
Calculating successive derivatives with respect to the weights, and going backwards, i.e. from \(j=l\) down to 1, we get (the evaluation requires dilligence and noticing the emerging regularity)
\begin{equation*}
\begin{split}
\frac{\partial e}{\partial w^j_{\alpha_{j-1} \alpha_j}} = x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j} , \;\;\; \alpha_{j-1}=0,\dots,n_{j-1}, \;\; \alpha_{j}=1,\dots,n_{j},
\end{split}
\end{equation*}
\sphinxAtStartPar
where

\sphinxAtStartPar
\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f'(s_{\alpha_l}^{l})\),

\sphinxAtStartPar
\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\).

\sphinxAtStartPar
The last expression is a recurrence going backward. We note that to obtain \(D^j\), we need \(D^{j+1}\), which we have already obtained in the previos step, as well as the signal \(s^j\), which we know from the outset, as we first carry out the feed forward propagation. This provides a simplification in the evaluation of derivatives and updating the weights.

\sphinxAtStartPar
With the steepest descent prescription, the weights are updated as
\begin{equation*}
\begin{split} w^j_{\alpha_{j-1} \alpha_j} \to  w^j_{\alpha_{j-1} \alpha_j} -\varepsilon x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j}, \end{split}
\end{equation*}
\sphinxAtStartPar
For the case of sigmoid we can use
\begin{equation*}
\begin{split}
\sigma'(s_A^{(i)})=\sigma'(s_A^{(i)}) (1-\sigma'(s_A^{(i)})) =x_A^{(i)}(1-x_A^{(i)}).
\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The above formulas explain the name \sphinxstylestrong{back propagation}, because in updating the weights we start from the last layer and then we go back recursively to the beginning of the network. At each step, we need only the signal in the given layer and the properties of the next layer! These features follow from
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
the feed\sphinxhyphen{}forward nature of the network, and

\item {} 
\sphinxAtStartPar
the chain rule in evaluation of derivatives.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
If activation functions are diferent in various layers (denote them with \(f_j\) for layer \(j\)), then there is an obvious modification:

\sphinxAtStartPar
\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f_l'(s_{\alpha_l}^{l})\),

\sphinxAtStartPar
\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f_j'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\).


\subsection{Code for backprop}
\label{\detokenize{docs/backprop:code-for-backprop}}
\sphinxAtStartPar
Now we present a code that implements our algorithm for networks with any number of layers and any number of neurons in the output layer. The previous one was for a single intermediate layer and one output neuron. It is simpy a programmatic implementation of the formulas derived above. In the code, we keep the notation from the above derivation.

\sphinxAtStartPar
The code has 12 lines only, not counting the comments!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} disctionary of weights}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed }
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivaive of f}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer  }
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                 
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                    \PYG{n}{df}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}   
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} 
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}           
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)} 
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}          
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)} 
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}      
\end{sphinxVerbatim}


\section{Example with the circle}
\label{\detokenize{docs/backprop:example-with-the-circle}}
\sphinxAtStartPar
We illustrate the code on the example of a binary classifier of points inside a circle.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cir}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} coordinate 1}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} coordinate 2}
    \PYG{k}{if}\PYG{p}{(}\PYG{p}{(}\PYG{n}{x1}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{l+m+mf}{0.4}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} inside the circle of radius 0.4}
                                            \PYG{c+c1}{\PYGZsh{} centered at (0.5,0.5)}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}                                              \PYG{c+c1}{\PYGZsh{} outside}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
For future generality \sphinxstylestrong{(new convention)}, we split the sample into an array of features and labels:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{cir}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} sample}
\PYG{n}{features\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{labels\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{,}\PYG{n}{norm}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{colors}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_53_0}.png}

\sphinxAtStartPar
We take the following architecture and initial parameters:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch\PYGZus{}c}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                  \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} scaled random initial weights}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.7}                            \PYG{c+c1}{\PYGZsh{} initial learning speed }
\end{sphinxVerbatim}

\sphinxAtStartPar
The simulation takes a few minutes,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.995}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                       \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} backprop}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  
\end{sphinxVerbatim}

\sphinxAtStartPar
whereas testing is very fast:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{test}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]} 

\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{po}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} 
    \PYG{n}{xt}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{po}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}   
    \PYG{n}{test}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{[}\PYG{n}{po}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{po}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{xt}\PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{tt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{test}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} drawing the circle}
\PYG{n}{ax}\PYG{o}{=}\PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{circ}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{radius}\PYG{o}{=}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{circ}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{,}\PYG{n}{norm}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{colors}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_59_0}.png}

\sphinxAtStartPar
The trained network looks like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fnet}\PYG{o}{=}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{l+m+mf}{.1}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{backprop_61_0}.png}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
It is fascinating that we have trained the network to recognize if a point is in a circle, and it has no concept whatsoever of geometry, Euclidean distance, equation of the circle, etc. The network just learned “empirically” how to proceed on a training sample!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The result in the plot is very good, perhaps except, as always, near the boundary. In view of our discussion of chapter {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{More layers}}}}, where we have set the weights of a network with three neuron layers from geometric considerations, the quality of the present result is stunning. We do not see any straight sides of a polygon, but a nicely rounded boundary.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Local minima}

\sphinxAtStartPar
We have mentioned before the emergence of local minima in multivariable optimization as a potential problem. In the figure below we show three different results of the backprop code for our classifier of points in a circle. We note that each of them has a radically different set of weights, whereas the results on the test sample are equally good for each case. This shows that the backprop optimization end up, as anticipated, in a local minimum. However, each of these local minima works well and equally good. This is actually the reason why backprop can be used in practical problems: there are zillions of local mnima, but it does not matter!
\end{sphinxadmonition}

\noindent\sphinxincludegraphics{{backprop_64_0}.png}


\section{General remarks}
\label{\detokenize{docs/backprop:general-remarks}}
\sphinxAtStartPar
There are some more important nad general observations:

\begin{sphinxadmonition}{note}{Note:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Supervised training of an ANN takes a very long time, but using a trained ANN takes a blink of an eye. The asymmetry originates from the simple fact that the multi\sphinxhyphen{}parameter optimization takes very many function calls (here \sphinxstylestrong{feed\sphinxhyphen{}forward}), but the usage on a point involves just one function call.

\item {} 
\sphinxAtStartPar
The classifier trained with backprop may work inaccurately for the points near the boundary lines. A remedy is to trained more for improvement, and/or increase the
size of the training sample.

\item {} 
\sphinxAtStartPar
However, a too long learning on the same training sample does not actually make sense, because the accuracy stops improving at some point.

\item {} 
\sphinxAtStartPar
Local minima occur in bckprop, but this is by no means an obstacle to the use of the backprop algorithm. This is an important practical feature.

\item {} 
\sphinxAtStartPar
Various improvements of the steepest descent method, or altogether different minimization methods may be used (see homework). They can largely increase the efficiency of the algorithm.

\item {} 
\sphinxAtStartPar
When going backwards with updating the weights in subsequent layers, one may introduce an increamnet factor (see homework below). This helps with performance.

\item {} 
\sphinxAtStartPar
Finally, different activation functions may be used to improve performance (see the following).

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Exercises}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Prove (analytically) by taking the derivative that \( \sigma '(s) = \sigma (s) [1- \sigma (s)]\). Show that the sigmoid is the only function with this property.

\item {} 
\sphinxAtStartPar
Modify the lecture example of the classifier of points in a circle by replacing the figure into
\begin{itemize}
\item {} 
\sphinxAtStartPar
semicircle,

\item {} 
\sphinxAtStartPar
two circles,

\item {} 
\sphinxAtStartPar
ring, or

\item {} 
\sphinxAtStartPar
any of your favorite shapes.

\end{itemize}

\item {} 
\sphinxAtStartPar
Repeat 2., experimenting with the number of layers and neurons, but remember that a large number of them increases the computation time and does not necessarily improve the result. Rank each case by the fractin of misclassified points in a test sample. Find an optimum architecture for each of the considered figures.

\item {} 
\sphinxAtStartPar
If the network has a lot of neurons and connections, little signal flows through each synapse, hence the network is resistant to a small random damage. This is what happens in the brain, which is constantly “damaged” (cosmic rays, alcohol, …). Besides, such a network after destruction can be (already with a smaller number of connections) retrained. Take your trained network from problem 2. and remove one of its \sphinxstylestrong{weak} connections, setting the coresponding weight to 0. Test this demaged network on a test sample and draw conclusions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scaling weights in back propagation.}
A disadvantage of using the sigmoid in the back propagation algorithm is a very slow updating of weights in layers distant from the output layer (the closer to the beginning of the network, the slower it is). A remedy here is a re\sphinxhyphen{}scaling the weights, where the learning speed in the layers, counting from the back, is successively increased by a certain factor. We remember that successive derivatives contribute factors of the form \( \sigma '(s) = \sigma (s) [1- \sigma (s)] = y (1-y) \) to the update rate, where \( y \) is in the range \( (0, 1) \). Thus the value of \( y (1-y \) cannot exceed 1/4, so in the following layers (counting from the back) the product \( [y (1-y] ^ n \le 1/4 ^ n\).
To prevent this “shrinking”, the learning rate can be multiplied by \( 4 ^ n \): \( 4, 16, 64, 256, ... \). Another heuristic argument {[}Rigler, Irvine, \& Vogl, 1989{]} suggests even faster growing factors of the form \( 6 ^ n \): \( 6, 36, 216, 1296, ... \)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Enter the above recipes into the code for backpropagation.

\item {} 
\sphinxAtStartPar
Check if they improve the algorithm performance for the deeper networks used, e.g. circle point classifier, etc.

\item {} 
\sphinxAtStartPar
For assessment of performance, carry out the execution time measurement (e.g., using the Python \sphinxstylestrong{time} library packet).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Steepest descent improvement.}
The method of the steepest descent of finding the minimum of a function of many variables used in the lecture depends on the local gradient. There are much better approaches that give a better convergence to the (local) minimum. One of them is the recipe of \sphinxhref{https://en.wikipedia.org/wiki/Gradient\_descent}{Barzilai\sphinxhyphen{}Borwein} explained below. Implement this method in the back propagation algorithm. Vectors \(x\) in \(n\)\sphinxhyphen{}dimensional space are updated in subsequent iterations as \( x^{(m + 1)} = x^{(m)} - \gamma_m \nabla F (x^{(m)})\),
where \(m\) numbers the iteration, and the speed of learning depends on the behavior at the two (current and previous) points:

\end{enumerate}
\begin{equation*}
\begin{split} \gamma _ {m} = \frac {\left | \left (x^{(m)}-x^{(m-1)} \right) \cdot
\left [\nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right] \right |}
{\left \| \nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right \| ^ {2}}.
\end{split}
\end{equation*}\end{sphinxadmonition}


\chapter{Interpolation}
\label{\detokenize{docs/interpol:interpolation}}\label{\detokenize{docs/interpol::doc}}

\section{Simulated data}
\label{\detokenize{docs/interpol:simulated-data}}
\sphinxAtStartPar
So far we have been concerned with \sphinxstylestrong{classification}, i.e. with networks recognizing whether a given object (in our case a point on a plane) has certain features. Now we pass to another practical application, namely \sphinxstylestrong{interpolating functions}. This use of ANNs has become widely used in scientific data analysis. We illustrate the method on a simple example, which explains the basic idea and shows how the method works.

\sphinxAtStartPar
Imagine you have some experimental data. Here we simulate them in an artificial way, e.g.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{fi}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mf}{0.2}\PYG{o}{+}\PYG{l+m+mf}{0.8}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3} \PYG{c+c1}{\PYGZsh{} some function}

\PYG{k}{def} \PYG{n+nf}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:} 
    \PYG{n}{x} \PYG{o}{=} \PYG{l+m+mf}{7.}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random x coordinate}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{fi}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} y coordinate = the function + noise from [\PYGZhy{}0.2,0.2]}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
We should now think in terms of supervised learning: \(x\) is the “feature”, and \(y\) the “label”.

\sphinxAtStartPar
We table our noisy data points and plot them together with the function \sphinxstylestrong{fi(x)} around which they concentrate. It is an imitation of an experimental measurement, which is always burdened with some error, here mimicked wih random noise.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} data sample}
\PYG{n}{features}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}                   \PYG{c+c1}{\PYGZsh{} x coordinate}
\PYG{n}{labels}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}                     \PYG{c+c1}{\PYGZsh{} y coordinate}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{interpol_7_0}.png}

\sphinxAtStartPar
In our current language of ANNs, we therefore have a training sample consisting of points with the input (feature) \(x\) and the true output (label) \(y\). As before, we minimize the error function from an appropriate neural network,
\begin{equation*}
\begin{split}E(\{w \}) = \sum_p (y_o^{(p)} - y^{(p)})^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Since the obained \(y_o\) is a certain (weight\sphinxhyphen{}dependent) function of \(x\), this method is a variant of the \sphinxstylestrong{least squares fit}, commonly used in data analysis. The difference is that in the standard least squares method the model function that we fit to the data has some simple analytic form (e.g. \( f(x) = A + B x\)), while now it is some “disguised” weight\sphinxhyphen{}dependent function provided by the neural network.


\section{ANNs for interpolation}
\label{\detokenize{docs/interpol:anns-for-interpolation}}
\sphinxAtStartPar
To understand the fundamental idea, consider a network with just two neurons in the middle layer, with the sigmoid activation function:

\noindent\sphinxincludegraphics{{interpol_11_0}.png}

\sphinxAtStartPar
The signals entering the two neurons in the middle layer are, in the notation of chapter {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{More layers}}}},
\begin{equation*}
\begin{split}s_1^{1}=w_{01}^{1}+w_{11}^{1} x, \end{split}
\end{equation*}\begin{equation*}
\begin{split}s_2^{1}=w_{02}^{1}+w_{12}^{1} x, \end{split}
\end{equation*}
\sphinxAtStartPar
and the outgoing signals are, correspondingly,
\begin{equation*}
\begin{split}\sigma \left( w_{01}^{1}+w_{11}^{1} x \right), \end{split}
\end{equation*}\begin{equation*}
\begin{split}\sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \end{split}
\end{equation*}
\sphinxAtStartPar
Therefore the combined signal entering the output neuron is
\begin{equation*}
\begin{split}s_1^{1}=w_{01}^{2}+ w_{11}^{2}\sigma \left( w_{01}^{1}+w_{11}^{1} x \right)
+  w_{21}^{2}\sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \end{split}
\end{equation*}
\sphinxAtStartPar
Taking, for an illustation, the weight values
\begin{equation*}
\begin{split}w_{01}^{2}=0, \, w_{11}^{2}=1, \, w_{21}^{2}=-1, \,
w_{11}^{1}=w_{12}^{1}=1, \, w_{01}^{1}=-x_1,  \, w_{02}^{1}=-x_2, \end{split}
\end{equation*}
\sphinxAtStartPar
where \(x_1\) and \(x_2\) are parameters with a natural interpretation explained below, we get

\sphinxAtStartPar
\(s_1^{1}=\sigma(x-x_1)-\sigma(x-x_2)\).

\sphinxAtStartPar
This function is shown in the plot below, with \(x_1=-1\) and \(x_2=4\).
It tends to 0 at \(- \infty\), then grows with \(x\) to achieve a maximum at
\((x_1+x_2)/2\), and then decreases, tending to 0 at \(+\infty\). At \(x=x_1\) and \(x=x_2\), the values are around 0.5.

\noindent\sphinxincludegraphics{{interpol_13_0}.png}

\sphinxAtStartPar
This is an important observation:
We are able to form, with a pair of neurons, a “hump” signal located around a given value, here \( (x_1 + x_2) / 2 = 2\), and with a spread of the order of \(|x_2-x_1|\). Changing the weights, we are able to modify its shape, width, and height.

\sphinxAtStartPar
One may think as follows: Imagine we have many neurons to our disposal in the intemediate layer. We may join them in pairs, forming humps “specializing” in particular regions of coordinates. Then, adjusting the heights of the humps, we may readily approximate a given function.

\sphinxAtStartPar
In an actual fitting procedure, we do not need to “join the neurons in pairs”, but make a combined fit of all parameters simutaneously, as we did in the case of classifiers.
The example below shows a composition of 8 sigmoids,
\begin{equation*}
\begin{split}
f = \sigma(z+3)-\sigma(z+1)+2 \sigma(z)-2\sigma(z-4)+
      \sigma(z-2)-\sigma(z-8)-1.3 \sigma(z-8)-1.3\sigma(z-10). 
\end{split}
\end{equation*}
\sphinxAtStartPar
In the figure, the component functions (the thin lines representing single humps) add up to a function of a rather complicated shape, marked with a thick line.

\noindent\sphinxincludegraphics{{interpol_15_0}.png}

\sphinxAtStartPar
There is an important difference in ANNs used for function approximation compared to the binary classifiers discussed earlier. There, the answers were 0 or 1, so we were using a step function in the output layer, or rather its smooth sigmoid variant. For function approximation, the answers form a continuum from the range of the function values. For that reason, in the output layer we just use the \sphinxstylestrong{linear} function, i.e., we just pass the incoming signal through. Of course, sigmoids remain in the intermediate layers.

\begin{sphinxadmonition}{note}{Output layer for function approximation}

\sphinxAtStartPar
In ANNs used for function approximation, the activation function in the output layer is \sphinxstylestrong{linear}.
\end{sphinxadmonition}


\subsection{Backprop for one\sphinxhyphen{}dimensional functions}
\label{\detokenize{docs/interpol:backprop-for-one-dimensional-functions}}
\sphinxAtStartPar
Minimization of the error function leads to the backprop algorithm (with a modified output layer), which we employ to fit our experimental data. Let us take the architecture:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
and the weights

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
As just discussed, the output is no longer between 0 and 1:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{,}\PYG{n}{features}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{ffo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{)}
\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{interpol_24_0}.png}

\sphinxAtStartPar
In the library module \sphinxstylestrong{func} we have the function for the backprop algorithm which allows for one activation function in the intermediate layers and a different one in the output layer. We carry out the training in two stages:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.02}                           \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the data sample points}
        \PYG{n}{pp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{pp}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{400}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.999}\PYG{o}{*}\PYG{n}{eps}                  \PYG{c+c1}{\PYGZsh{} dicrease of the learning speed}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over points taken in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
which nicely does the job:

\noindent\sphinxincludegraphics{{interpol_30_0}.png}

\sphinxAtStartPar
We note that the obtained red curve is very close to the function used to generate the data sample (black line). This shows that the approximation works. A construction of a quantitative measure (least square sum) is a topic of a homework problem.


\section{Remarks}
\label{\detokenize{docs/interpol:remarks}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The activation function in the output layer may be any function with values containing the values of the interpolated function, not necessarily linear.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{More dimensions}

\sphinxAtStartPar
To interpolate general functions of two or more arguments, one needs use ANNs with at least 3 neuron layers.
\end{sphinxadmonition}

\begin{sphinxadmonition}{tip}{Tip:}
\sphinxAtStartPar
The number of neurons reflects the behavior of the interpolated function. If the function varies a lot, one needs more neurons, typically at least twice as many as the number of extrema.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Overfitting}

\sphinxAtStartPar
There must be much more data for fitting than the network parameters, to avoid the so\sphinxhyphen{}called overfitting problem.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Exercises}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Fit the data points generated by your favorite function (of one variable) with noise. Play with the network architecture.

\item {} 
\sphinxAtStartPar
Compute the sum of squared distances of the values of the data points and the corresponding approximating function, and use it as a measure of the goodness of the fit. Use it to test how the number of neurons in the network affects the result.

\item {} 
\sphinxAtStartPar
Use a network with more layers (at least 3 neuron layers) to fit the data points generated with your favorite two\sphinxhyphen{}variable function. Make two\sphinxhyphen{}dimensional contour plots for this function and for the function obtained from the neural network and compare the results (of course, they should be very similar if everything works).

\end{enumerate}
\end{sphinxadmonition}


\chapter{Rectification}
\label{\detokenize{docs/rectification:rectification}}\label{\detokenize{docs/rectification::doc}}
\sphinxAtStartPar
In the previous chapter we have made a hump function from two sigmoids, we may now ask a question: can we make a sigmoid  as a linear combination (or simply difference) of some other functions. Then we could use these functions for activation of neurons in place of the sigmoid. The answer is yes. For instance, the \sphinxstylestrong{Rectified Linear Unit (ReLU)} function
\begin{equation*}
\begin{split}
{\rm ReLU}(x) = \left \{ \begin{array}{l} x {\rm ~~~ for~} x \ge 0 \\
                                          0 {\rm ~~~ for~} x < 0 \end{array}    \right . = {\rm max}(x,0)
\end{split}
\end{equation*}
\sphinxAtStartPar
does (approximately) the job. The a bit ackward name comes from electonics, where such a unit is used to cut out the negative values of an electric signal. Rectification means “straightening up”. The plot of ReLU looks as follows:

\noindent\sphinxincludegraphics{{rectification_5_0}.png}

\sphinxAtStartPar
Taking a difference of two ReLU functions with shifted arguments yield, for example,

\noindent\sphinxincludegraphics{{rectification_7_0}.png}

\sphinxAtStartPar
which looks pretty much as a sigmoid, apart for the sharp corners. One can make things smooth by taking a different function, the \sphinxstylestrong{softplus},
\begin{equation*}
\begin{split}
{\rm softplus}(x)=\log \left( 1+e^x \right ),
\end{split}
\end{equation*}
\sphinxAtStartPar
which looks as

\noindent\sphinxincludegraphics{{rectification_9_0}.png}

\sphinxAtStartPar
A difference of two softplus functions yields a result very similar to the sigmoid.

\noindent\sphinxincludegraphics{{rectification_11_0}.png}

\sphinxAtStartPar
One may then use the ReLU of softplus, or a plethora of other similar functions, for the activation. Why one should actually do this will be dicussed later.


\section{Interpolation with ReLU}
\label{\detokenize{docs/rectification:interpolation-with-relu}}
\sphinxAtStartPar
We will now approximate our simulated data with an ANN with ReLU acivation in the intermediate layers (and a linear function is the output layer, as above). The fuctions are taken from the module \sphinxstylestrong{func}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{relu}
\PYG{n}{dfff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{drelu}
\end{sphinxVerbatim}

\sphinxAtStartPar
The network must now have more neurons:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                   \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} initialize weights randomly in [\PYGZhy{}2.5,2.5]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.0003}          \PYG{c+c1}{\PYGZsh{} small learning speed}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the data sample points}
        \PYG{n}{pp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{pp}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.999}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} points in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{rectification_20_0}.png}

\sphinxAtStartPar
We note again a quite safisfactory result, noticing that the plot of the fitting function is a sequence of straight lines, simply reflecting the feature of the ReLU activation function.


\section{Classifiers with rectification}
\label{\detokenize{docs/rectification:classifiers-with-rectification}}
\sphinxAtStartPar
There are technical reasons in favor of using \sphinxhref{https://en.wikipedia.org/wiki/Rectifier\_(neural\_networks)}{rectified functions} rather than sigmoid\sphinxhyphen{}like ones in backprop. The derivatives of sigmoid are very close to zero apart for the narrow region near the threshold. This makes updating the weights unlikely, especially when going many layers back, as the small numbers multiply (this is known as the \sphinxstylestrong{vanishing gradient problem}). With rectified functions, the range where the derivative is large is big (for ReLU it holds for all positive coordinates), hence the problem is cured. For that reason, rectified functions are used in deep ANNs, where there are many layers, impossible to train when the activation function is of a sigmoid type.
Application of rectified activation functions was one of the key tricks that allowed a breakthrough in deep ANNs in 2011.

\sphinxAtStartPar
On the other hand, with ReLU it may happen that weights are set into such values that many neurons become inactive, i.e. never fire for any input, and so are effectively eliminated. This is known as the “dead neuron” probem, which arises especially when the learning speed parameter is too high. A way to reduce the problem is to use an activation function which does not have a range with zero derivative, such as the \sphinxhref{https://en.wikipedia.org/wiki/Activation\_function}{leaky ReLU}. Here we take it in the form
\begin{equation*}
\begin{split}
{\rm Leaky~ReLU}(x) = \left \{ \begin{array}{ll} x &{\rm ~~~ for~} x \ge 0 \\
                                          0.1 \, x &{\rm ~~~ for~} x < 0 \end{array}    \right . .
\end{split}
\end{equation*}
\sphinxAtStartPar
Next, we repeat our example with the classification of points in the circle.

\sphinxAtStartPar
We take the following architecture and initial parameters,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch\PYGZus{}c}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                   \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} scaled random initial weights}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.01}                           \PYG{c+c1}{\PYGZsh{} initial learning speed }
\end{sphinxVerbatim}

\sphinxAtStartPar
and run the algorithm in two stages: with leaky ReLu, and then with ReLU.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.9999}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
            \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lrelu}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlrelu}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} backprop}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{700}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.9999}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
            \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{relu}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{drelu}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} backprop}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  
\end{sphinxVerbatim}

\sphinxAtStartPar
The result is quite satistactory, showing that the method works. With the present architecture, not surprisingly, we can notice below a polygon approxiating the cirle.

\noindent\sphinxincludegraphics{{rectification_34_0}.png}


\section{General remarks on backprop}
\label{\detokenize{docs/rectification:general-remarks-on-backprop}}
\sphinxAtStartPar
Conclude here our discussion of the supervised learning and the back proparation, we provide a number of remarks nd hints. First, in programmers life, bulding a well\sphinxhyphen{}functioning ANN, even for simple problems as used for illustrations up to now, can be a frustrating eperience! …

\sphinxAtStartPar
Initial conditions for minimization, basen of convergence, learning stategy, improvements of steepest descent …

\sphinxAtStartPar
Professional libraries, experience verified with success …

\begin{sphinxadmonition}{note}{Exercises}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Use various rectified activation functions for the binary classifiers and test then on various figures (in analogy to the example with the circe above).

\end{enumerate}
\end{sphinxadmonition}


\chapter{Unsupervised learning}
\label{\detokenize{docs/unsupervised:unsupervised-learning}}\label{\detokenize{docs/unsupervised:un-lab}}\label{\detokenize{docs/unsupervised::doc}}
\begin{sphinxadmonition}{note}{Motto}

\sphinxAtStartPar
\sphinxstyleemphasis{teachers! leave those kids alone!}


\end{sphinxadmonition}

\sphinxAtStartPar
Supervised learning discussed in previous lectures needs a teacher or a training sample with labels, where we know a priori the characteristics of the data (e.g., in our example, whether the point is inside or outside the circle).

\sphinxAtStartPar
However, this is quite a special situation, because most often the data that we encounter do not have pre\sphinxhyphen{}assigned labels and “are what they are”. Also, from the neurobiological or methodological point of view, we learn many facts and activities “on an ongoing basis”, classifying and then recognizing them, whilst the process goes on without any external supervision.

\sphinxAtStartPar
Imagine an alien botanist who enters a meadow and encounters various species of flowers. He has no idea what they are and what to expect at all, as he has no prior knowledge on earthly matters. After finding the first flower, he records its features: color, size, number of petals, etc. He goes on, and finds a different flower, and records its features, and so on with the next flowers. At some point, however, he finds a flower that he already had met. More precisely, its features will be close, though not identical (the size may easily differ somewhat, so the color, etc.), to the previous instance. Hence he concludes it belongs to the same category. The exploration goes on, and new flowers either start a new category, of join one already present. At the end he has a catalog of flowers and now he can assign names (labels) to each species: corn poppy, bluebottle, mullein,…  They are useful in sharing the knowledge with ohers, as they summarize, so to speak, the features of the flower. Note, however, that these labels have actually never been used in the meadow exploration (learning) process.

\sphinxAtStartPar
Formally, this problem of \sphinxstylestrong{unsupervised learning} is related to data classification (division into categories, or \sphinxstylestrong{clusters}, i.e. subsets of tah sample where the suitably defined distances between individual data are small, smaller than the assumed distances between clusters). Colloquially speaking, we are looking for similarities between individual data points and try to divide the sample into groups of similar objects.


\section{Clusters of points}
\label{\detokenize{docs/unsupervised:clusters-of-points}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{pA}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.75}\PYG{p}{,} \PYG{l+m+mf}{.95}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.7}\PYG{p}{,} \PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pB}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{l+m+mf}{.6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.6}\PYG{p}{,} \PYG{l+m+mf}{.75}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pC}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{l+m+mf}{.3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{l+m+mf}{.5}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pD}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.7}\PYG{p}{,} \PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{.2}\PYG{p}{)}\PYG{p}{]} 
\end{sphinxVerbatim}

\sphinxAtStartPar
Let us create data samples with a few points from each category:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samA}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pA}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samB}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pB}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samC}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pC}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samD}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pD}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Our data looks like this:

\noindent\sphinxincludegraphics{{unsupervised_10_0}.png}

\sphinxAtStartPar
If we show the above picture to someone, he will undoubtedly state that the are four clusters. But what algorithm is he using to determine this? We will construct such an algorithm shortly and will be able to carry out clusterization. For the moment let us jump ahead and assume we know the clusters. Clearly, in our example the clusters are well defined, i.e. visibly separated from each other.

\sphinxAtStartPar
One can represent clusters with \sphinxstylestrong{representative points} that lie somewher within the cluster. For example, one could take an item belonging to a given cluster as its representative, or in each cluster one can evaluate the mean position of its points and use it as representative points:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rA}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rB}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rC}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rD}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\sphinxAtStartPar
We add thus defined characteristic points to our graphics. For visual convenience, we assign a color for each category (after having the clusters, we may assign labels, and the color here serves this purpose).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{col}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magenta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Clusters with representative points}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{unsupervised_15_0}.png}


\section{Voronoi areas}
\label{\detokenize{docs/unsupervised:voronoi-areas}}\label{\detokenize{docs/unsupervised:vor-lab}}
\sphinxAtStartPar
Having the situation as in the figure above, i.e. some representative points present, we can divide the entire plane into areas according to the following Voronoi criterion, which is a simple geometric notion:

\begin{sphinxadmonition}{note}{Voronoi areas}

\sphinxAtStartPar
Consider point \(P\) in a metric space, where we have a number representative points (also called the Voronoi points). The representative point \(R\) closest to \(P\) determines its category. All such points \(P\) form a Voronoi area of \(R\).
\end{sphinxadmonition}

\sphinxAtStartPar
Let us define the color of the point P in our plane as the color of the nearest representative point. For this we first need (the square of) a distance function (here: Euclidean) between two points in 2\sphinxhyphen{}dim. space:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{eucl}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} square of the Euclidean distance}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\sphinxAtStartPar
Then we find the nearest representative point and determine its color:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{col\PYGZus{}char}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the nearest point}
    \PYG{k}{return} \PYG{n}{col}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} e.g.}
\PYG{n}{col\PYGZus{}char}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{.5}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The result is following, with straight\sphinxhyphen{}line boundaries between naighboring areas:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.2}\PYG{p}{,}\PYG{l+m+mf}{3.2}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Voronoi areas}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{70}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} 70 points in x}
    \PYG{k}{for} \PYG{n}{x2} \PYG{o+ow}{in} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{70}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} 70 points in y}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col\PYGZus{}char}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{edgecolors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{18}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
A practical message here is that once we have the characteristic points, we can use Voronoi’s criterion for classification of data
\end{sphinxadmonition}


\section{Naive clusterization}
\label{\detokenize{docs/unsupervised:naive-clusterization}}
\sphinxAtStartPar
Now the “real” botanist’s problem:  imagine we have our sample, but we know nothing about how its points were generated (we do not have any labels A, B, C, D, nor colors of the points). Moreover, the data is mixed, i.e., the data points appear in a random order. So we merge our points,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{alls}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{,} \PYG{n}{samB}\PYG{p}{,} \PYG{n}{samC}\PYG{p}{,} \PYG{n}{samD}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
and shuffle them,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The data visualization looks as in the first plot of thsi chapter.

\sphinxAtStartPar
We now want to somehow create representative points, but a priori we don’t know where they should be, or even how many of them there are. Very different strategies are possible here. Their common feature is that the position of the representative points is updated as the sample data is processed.

\sphinxAtStartPar
Let us start with just one representative point, \(\vec{R}\). Not very ambitious, but in the end we will at least know some mean characteristics of the sample. The initial position  position is \( R=(R_1, R_2) \), a two dimensional vector in \([0,1]\times [0,1]\). After reading a data point \(P\) with coordinates \( (x_1 ^ P, x_2 ^ P) \), \(R\) changes as follows:
\begin{equation*}
\begin{split} (R_1, R_2) \to (R_1, R_2) + \varepsilon (x_1 ^P-R_1, x_2 ^P-R_2), \end{split}
\end{equation*}
\sphinxAtStartPar
or in the vector notation
\begin{equation*}
\begin{split} \vec {R} \to \vec {R} + \varepsilon (\vec {x}^P - \vec {R}). \end{split}
\end{equation*}
\sphinxAtStartPar
The step is repeated for all points of the sample, and then many such round may be carried out. As in the previous chapters, \( \varepsilon \) is the learning rate that (preferably) decreases
as the algorithm proceeds. The above formula realizes the “snapping” of the point \(\vec{R}\) by the data point \(\vec{P}\).

\sphinxAtStartPar
The following code implements the above prescription:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{initial location:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rounds  location}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}    \PYG{c+c1}{\PYGZsh{} decrease the update speed }
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} reshuffle the sample}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over points of the whole sample}
        \PYG{n}{R}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update/learning}
    \PYG{k}{if} \PYG{n}{j}\PYG{o}{\PYGZpc{}}\PYG{k}{5}==4: print(j+1, \PYGZdq{}    \PYGZdq{},np.round(R,3))   
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
initial location:
[0.005 0.512]
rounds  location
5      [0.553 0.517]
10      [0.637 0.503]
15      [0.612 0.481]
20      [0.602 0.48 ]
25      [0.604 0.481]
30      [0.603 0.482]
35      [0.603 0.482]
40      [0.603 0.482]
45      [0.603 0.482]
50      [0.603 0.482]
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that the position of the characteristic point converges. Actually, it becomes very close to the average of the locatio of all points,

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R\PYGZus{}mean}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R\PYGZus{}mean}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[0.602 0.481]
\end{sphinxVerbatim}

\sphinxAtStartPar
We have decided a priori to have one category, and here is our plot of the result fot the characeristic point, indicated with a gray blob:

\noindent\sphinxincludegraphics{{unsupervised_37_0}.png}

\sphinxAtStartPar
Let us try to generalize the above algorithm for the case of several (\( n_R> \) 1) representative points.
\begin{itemize}
\item {} 
\sphinxAtStartPar
We initialize randomly representative vectors \( \vec{R}^i \), \(i = 1, \dots, n_R \).

\item {} 
\sphinxAtStartPar
Round: We take the sample points P one by one and update only the \sphinxstylestrong{closest} point \(R^m\) to the point P in a given step:

\end{itemize}
\begin{equation*}
\begin{split} \vec{R}^m \to \vec{R}^m + \varepsilon (\vec{x} - \vec{R}^m). \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
The position of the other representative points remains the same. This strategy is called the \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}all}.

\item {} 
\sphinxAtStartPar
We repeat the rounds, reducing the learning speed \( \varepsilon \) each time.

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}all} strategy is an important concept in ANN modeling. The competing neurons in a layer fight for the signal, and the one that wins, takes it all (its weighs get updated), while the loosers get nothing.
\end{sphinxadmonition}

\sphinxAtStartPar
So let’s consider two representative points that we initialize randomly:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{R2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Then we carry out the above algorithm. For each data point we find the nearest representative point out of the two, and update only this one:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{initial locations:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rounds  locations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)} 
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} data point}
\PYG{c+c1}{\PYGZsh{}        print(p)}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R2}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} squares of distances}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} minimum}
        \PYG{k}{if} \PYG{n}{ind\PYGZus{}min}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} if R1 closer to the new data point}
            \PYG{n}{R1}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R1}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} if R2 closer ...}
            \PYG{n}{R2}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R2}\PYG{p}{)}            

    \PYG{k}{if} \PYG{n}{j}\PYG{o}{\PYGZpc{}}\PYG{k}{5}==4: print(j+1,\PYGZdq{}    \PYGZdq{}, np.round(R1,3), np.round(R2,3))  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
initial locations:
[0.905 0.549] [0.419 0.255]
rounds  locations
5      [0.836 0.104] [0.476 0.613]
10      [0.837 0.107] [0.559 0.678]
15      [0.838 0.101] [0.509 0.645]
20      [0.837 0.102] [0.504 0.643]
25      [0.837 0.102] [0.507 0.644]
30      [0.837 0.102] [0.505 0.643]
35      [0.837 0.102] [0.505 0.642]
40      [0.837 0.102] [0.504 0.642]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{unsupervised_44_0}.png}

\sphinxAtStartPar
One of the characteristic poits “specializes” in the lower right cluster, and the other in the remaining three.

\sphinxAtStartPar
We continue, anologously, with four representative points.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{cl4_2}.jpg}
\caption{Left: proper characteristic points. Right: one “dead body”.}\label{\detokenize{docs/unsupervised:p-fig}}\end{figure}

\sphinxAtStartPar
Running the case for four categories, we notice that it does not always give the correct answer. Quite often one of the representative points is not updated at all and becomes the so\sphinxhyphen{}called \sphinxstylestrong{dead body}. This is because the other representative points always “win”, i.e. one of them is always closer to each point of the sample than the “corpse”.

\sphinxAtStartPar
When we set up five characteristic points, several situation may occur, as shown in the figure below. Sometimes, depending on the initializtion, a cluster is split into two smaller ones, sometimes dead bodies occur.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=870\sphinxpxdimen]{{cl5}.jpg}
\caption{From left to right: 5 characteristic points with one cluster split into two, with another cluster split into two, one dead body, and two dead bodies.}\label{\detokenize{docs/unsupervised:id1}}\end{figure}

\sphinxAtStartPar
Enforcing more representative points leads to the formation of dead bodies even more often. Of course, we may disregard them, but the example shows that the current startegy is problematic.


\section{Clustering scale}
\label{\detokenize{docs/unsupervised:clustering-scale}}
\sphinxAtStartPar
In the previous section we were trying to guess from the outset how many clusters there are in the data. This lead to problems, also many times we do not really know how many clusters there are. Actually, up to now we have not even defined what precisely a cluster is, using some intuition only. This intuition told us that the points in the same cluster must be close to one another, or close to a characteristi point, but how close? Actually, the definition must involve a scale telling us “how close is close”. Fo instance, in our example we may take a scale of about 0.2, where there are 4 clusters, but we may take a smaller one and resolve the bigger clusters inti smaller ones, as in the  left panels of \hyperref[\detokenize{docs/unsupervised:id1}]{Fig.\@ \ref{\detokenize{docs/unsupervised:id1}}}.

\begin{sphinxadmonition}{note}{Definition of cluster}

\sphinxAtStartPar
A cluster of scale \(d\) associated with a characteristic point \(R\) is the set of data points \(P\), whose distance from \(R\) is less than \(d\), whereas the distance from other characteristic points is \(\ge d\). The characteristic points must be selected in such a way that each data point belongs to a cluster, and no characteristic point is a dead body (i.e., its cluster must contain at least one data point).
\end{sphinxadmonition}

\sphinxAtStartPar
Various strategies can be used to implement this prescription. We use here the \sphinxstylestrong{dynamical clusterization}, where a new cluster/representative point is created whenever an encoutered data point is farther than \(d\) from any present characteristic point up to now.

\begin{sphinxadmonition}{note}{Dynamical clusterization}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Set the clustering scale \(d\) and the learning speed \(\varepsilon\). Shuffle the sample.

\item {} 
\sphinxAtStartPar
Read the first data point \(P_1\) and set the first characteristic point \(R^1=P_1\). Add it to an array \(R\). Mark \(P_1\) as belonging to cluster \(1\).

\item {} 
\sphinxAtStartPar
Read the next data points \(P\). If the distance of \(P\) to the \sphinxstylestrong{closest} characteristic point, \(R^m\), is \(\le d\), then
\begin{itemize}
\item {} 
\sphinxAtStartPar
mark \(P\) as belonging to cluster \(m\).

\item {} 
\sphinxAtStartPar
move \(R^m\) towards \(P\) with the learning speed \(\varepsilon\).Otherwise, add to \(R\) a new characteristic point a location of the point \(P\).

\end{itemize}

\item {} 
\sphinxAtStartPar
Repeat from \(2.\) until all the data points are read.

\item {} 
\sphinxAtStartPar
Repeat from \(2.\) a number of rounds, decreasing each time \(\varepsilon\). The result is a division of the sample into a number of clusters, and ghe location of corresponding charactristic points. The result may depend on the reshuffling, hence does not have to the same when the procedure is repeated.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
A Python implementation finding dynamically the representative points is following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mf}{0.2}  \PYG{c+c1}{\PYGZsh{} clustering scale}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} initial learning speed}

\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}                  \PYG{c+c1}{\PYGZsh{} decrease the learning speed }
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} shuffle the sample}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}                      \PYG{c+c1}{\PYGZsh{} in the first round}
        \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{alls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} R \PYGZhy{} array of representative points}
                                  \PYG{c+c1}{\PYGZsh{} initialized to the first data point}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} loop over the sample points}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}                 \PYG{c+c1}{\PYGZsh{} new data point}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} 
         \PYG{c+c1}{\PYGZsh{} array of squares of distances of p from the current repr. points in R}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the closest repr. point}
        \PYG{k}{if} \PYG{n}{dist}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{d}\PYG{o}{*}\PYG{n}{d}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} if its distance square \PYGZgt{} d*d}
                                  \PYG{c+c1}{\PYGZsh{} dynamical creation of a new category}
            \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} add new repr. point to R}
        \PYG{k}{else}\PYG{p}{:}   
            \PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} otherwise, apdate the \PYGZdq{}old\PYGZdq{} repr. point}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of representative points: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of representative points:  4
\end{sphinxVerbatim}

\sphinxAtStartPar
The outcome for various values of the clustering scale \(d\) is shown in \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Fig.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}. At very low values of \(d\), smaller than the minimum separation betwenn the points, there are as many clusters as the data points. Then, as we increase \(d\), the number of clusters decreases. At very large \(d\), order of the of the span of the sample, there is only one cluster.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=770\sphinxpxdimen]{{cd}.jpg}
\caption{Dynamical clustering for various values of the scale \(d\).}\label{\detokenize{docs/unsupervised:dyn-fig}}\end{figure}

\sphinxAtStartPar
Certainly, an algorithm will not tell us which clustering scale to use. The proper value depends on the nature of the problem. Recall our botanist. If he used a very small \(d\), he would get as many categories as there are flowers in the meadow, as all flowers, even of the same species, are slightly different from one another. That would be useless. On the other extreme, if his \(d\) is too large, then the classification is too crude. Something in between is just right!

\begin{sphinxadmonition}{note}{Labels}

\sphinxAtStartPar
After forming the clusters, we may assign them \sphinxstylestrong{labels} for convenience. They are not used in the learning (cluster formation) process.
\end{sphinxadmonition}

\sphinxAtStartPar
Having determined the clusters, we have a \sphinxstylestrong{classifier}. We may use it in a two\sphinxhyphen{}fold way:
\begin{itemize}
\item {} 
\sphinxAtStartPar
continue the dynamical update as new data are encountered, or

\item {} 
\sphinxAtStartPar
“close” it, and see where the new data falls in.

\end{itemize}

\sphinxAtStartPar
In the first case, we assign the corresponding cluster label to the data point (our botanist knows what new flower he found), or initiate a new category if the point does not belong to any of the existing clusters. This is just a continuation of the dynamical algorithm descibed above on the new incoming data

\sphinxAtStartPar
In the latter case (we bought the ready botanist’s catalogue), a data point may
\begin{itemize}
\item {} 
\sphinxAtStartPar
belong to a cluster (we know its label),

\item {} 
\sphinxAtStartPar
fall outside any cluster, then we just do not know what it is, or

\item {} 
\sphinxAtStartPar
fall into an overlapping region of two or more clusters (cf. \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Fig.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}, where we only get “partial” classification.

\end{itemize}


\subsection{Interpretation via steepest descent}
\label{\detokenize{docs/unsupervised:interpretation-via-steepest-descent}}
\sphinxAtStartPar
Let us denote a given cluster with \(C_i\), \(i = 1, ..., n\), where \( n \) is the total number of clusters. The sum of the squared distances of data points in \( C_i \) to its representative point \( R ^ i \) is
\begin{equation*}
\begin{split}
\sum_{P \in C_i} | \vec{R}^i- \vec{x}^P|^2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Summing up over all clusters, we obtain a function analogous to the previously discussed error function:
\begin{equation*}
\begin{split}E (\{R \}) = \sum_{i = 1}^ n \sum_ {P \in C_i} |\vec{R}^i- \vec{x}^P |^2 .\end{split}
\end{equation*}
\sphinxAtStartPar
Its derivative with respect to \( \vec{R}_i \) is
\begin{equation*}
\begin{split} \frac{\partial E (\{R \})}{\partial \vec{R}^i}
= 2 \sum_{P \in C_i} (\vec{R}^i- \vec{x}^P). \end{split}
\end{equation*}
\sphinxAtStartPar
The steepest descent method results \sphinxstylestrong{exactly} in the recipe used in the
dynamic clasterization algorithm presented above, i.e.
\begin{equation*}
\begin{split} \vec{R} \to \vec{R} - \varepsilon (\vec{R} - \vec {x}^P). \end{split}
\end{equation*}
\sphinxAtStartPar
To summarize, the algorithm used here actually involves the steepest descent method for the function \( E (\{R \})\), as discussed in previous lectures.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Note, however, that the minimization used in the present algorithms also takes into account different division of points into clusters. In particular, a given data point may change its cluster assignment during the execution of the algorithm. This happens when the closest representative point is changed.
\end{sphinxadmonition}


\section{Interpretation via neural networks}
\label{\detokenize{docs/unsupervised:interpretation-via-neural-networks}}\label{\detokenize{docs/unsupervised:inn-sec}}
\sphinxAtStartPar
We will now interpret the above unsupervised learning algorithm with the winner\sphinxhyphen{}take\sphinxhyphen{}all strategy in the neural network language.

\noindent\sphinxincludegraphics{{unsupervised_70_0}.png}

\sphinxAtStartPar
Our example network has four neurons in the intermediate neuron layer, each corresponding to one characteristic point \(\vec{R}^i\). The weights are the coordinates of \(\vec{R}^i\). There is one node in the output layer. We note significant differences from the perceptron discussed earlier.
\begin{itemize}
\item {} 
\sphinxAtStartPar
There are no threshold nodes.

\item {} 
\sphinxAtStartPar
In the intermediate layer, the signal equals the distance squared of the input from the correspoding characteristic point. It is not a weighted sum.

\item {} 
\sphinxAtStartPar
The node in the last layer (MIN) indicates in which neuron of the intermediate layer the signal is the smallest, i.e., where we have the shortest distance. Hence it works as a control unit selecting the minimum.

\end{itemize}

\sphinxAtStartPar
During (unsupervised) learning, an input point “attracts” the closest characteristic point, whose weights are updated.

\sphinxAtStartPar
The application of the above network classifies the point with the coordinates \((x_1, x_2)\), assigning it the index of the representative point of a given category (here it is the number 1, 2, 3, or 4).


\subsection{Representation with spherical coordinates}
\label{\detokenize{docs/unsupervised:representation-with-spherical-coordinates}}
\sphinxAtStartPar
Even with our vast “mathematical freedom”, calling the above system a neural network is quite abusive, as it seems very far away from any neurobiological pattern. In particular, the use of a (non\sphinxhyphen{}linear) signal of the form \(\left(\vec{R}^i-\vec{x}\right)^2\) contrasts with the perceptron, where the signal entering the neurons is a (linear) weighted sum of inputs, i.e.
\begin{equation*}
\begin{split} s ^ i = x_1 w_1 ^ i + x_2 w_2 ^ i + ... + w_1 ^ m x_m = \vec {x} \cdot \vec {w} ^ i. \end{split}
\end{equation*}
\sphinxAtStartPar
We can alter our problem with a simple geometric construction to make it similar to the perceptron principle. For this purpose we enter a (spurious) third coordinate defined as
\begin{equation*}
\begin{split} x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2}, \end{split}
\end{equation*}
\sphinxAtStartPar
where \( r \) is chosen such that for all data points \( r ^ 2 \ge x_1 ^ 2 + x_2 ^ 2 \).
From the construction \( \vec {x} \cdot \vec {x} = x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 = r ^ 2 \), so the data points lie on the hemisphere (\( x_3 \ge 0 \)) of radius \( r \). Similarly, for the representative points we introduce:
\begin{equation*}
\begin{split} w_1 ^ i = R_1 ^ i,  \; w_2 ^ i = R_2 ^ i,  \; 
w_3 ^ i = \sqrt {r ^ 2-(R_1 ^i)^2 -(R_2 ^i)^2}. \end{split}
\end{equation*}
\sphinxAtStartPar
It is geometrically obvious that two points in a plane in coordinates 1 and 2 are close to each other if and only if their extensions to the hemisphere are close. We support this statement with a simple calculation:

\sphinxAtStartPar
The dot product of two points \( \vec {x} \) and \( \vec {y} \) on a hemisphere can be written as
\begin{equation*}
\begin{split} \vec {x} \cdot \vec {y} = x_1 y_1 + x_2 y_2 + \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \sqrt {r ^ 2-y_1 ^ 2-y_2 ^ 2}. \end{split}
\end{equation*}
\sphinxAtStartPar
For simplicity, let us consider a situation when \( x_1 ^ 2 + x_2 ^ 2 \ll r ^ 2 \) and \( y_1 ^ 2 + y_2 ^ 2 \ll r ^ 2 \), i.e. both points lie near the pole of the hemisphere. Using your knowledge of mathematical analysis
\begin{equation*}
\begin{split} \sqrt{r^2-a^2} \simeq r - \frac{a^2}{2r},  \;\;\;a \ll r, \end{split}
\end{equation*}
\sphinxAtStartPar
hence

\sphinxAtStartPar
\(\vec{x} \cdot \vec{y} \simeq x_1 y_1 + x_2 y_2 + \left( r -\frac{x_1^2+x_2^2}{2r} \right) \left( r -\frac{y_1^2+y_2^2}{2r} \right) \\ 
\;\;\;\simeq r^2 - \frac{1}{2} (x_1^2+x_2^2 +y_1^2+y_2^2) + x_1 y_1+x_2 y_2 \\ 
\;\;\; = r^2 - \frac{1}{2}[ (x_1-x_2)^2 +(y_1-y_2)^2]\).

\sphinxAtStartPar
So it is (for points close to the pole) the constant \( r ^ 2 \) minus half the square of the distance between the points \( (x_1, x_2) \) and \( (y_1, y_2) \) on the plane! It then follows that instead of finding a minimum distance for points on the plane, as in the previous algorithm, we can find a maximum scalar product for their 3D extensions to a hemisphere.

\sphinxAtStartPar
With the extension of the data to a hemisphere, the appropriate neural network can be viewed as follows:

\noindent\sphinxincludegraphics{{unsupervised_76_0}.png}

\sphinxAtStartPar
Thanks to our efforts, the signal in the intermediate layer is now just a dot product of the input and the weights, as it should be in the artificial neuron. The unit in the last layer (MAX) indicates where the dot product is largest.

\sphinxAtStartPar
This MAX unit is still problematic to interpret within our present framework. Actually, it is possible, but requires going beyond feed\sphinxhyphen{}forward networks. When the neurons in the layer can communicate (recurrent \sphinxhref{https://en.wikipedia.org/wiki/Hopfield\_network}{Hopfield networks}), they can compete, and with proper feed\sphinxhyphen{}back it is possible to enforce the winner\sphinxhyphen{}take\sphinxhyphen{}all mechanism.

\begin{sphinxadmonition}{note}{Hebbian rule}

\sphinxAtStartPar
On the conceptul side, we here touch upon a very important and intuitve principle in neural networks, known as the \sphinxhref{https://en.wikipedia.org/wiki/Hebbian\_theory}{Hebbian rule}/ Essentially, it applies the truth “What is used, gets stronger” no synaptic connections. A repeated use of a connection makes it stronger.

\sphinxAtStartPar
In our formulation, if a signal passes through a given connection, its weight changes accordingly, and other connections remain the same. The process takes place in an unsupervised manner and its implementation is biologically well motivated.

\sphinxAtStartPar
On the other hand, it is difficult to find a biological justification for backpropagation in supervised learning, where all weights are updated, also in layers very distant from the output. According to many researchers, it is rather a mathematical concept (but nevertheless extremely useful)
\end{sphinxadmonition}


\subsection{Scalar product maximization}
\label{\detokenize{docs/unsupervised:scalar-product-maximization}}
\sphinxAtStartPar
The algorithm is now as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Extend the points from the sample with the third coordinate, \( x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \), choosing appropriately large \( r \) so that \( r ^ 2> x_1 ^ 2 + x_2 ^ 2 \) for all sample points.

\item {} 
\sphinxAtStartPar
Initialize the weights such that \( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \).

\end{itemize}

\sphinxAtStartPar
Then loop over the data points:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Find the neuron in the intermediate layer for which the dot product \( x \cdot \vec {w} _i \) is the largest. Change the weights of this neuron according to the recipe

\end{itemize}
\begin{equation*}
\begin{split} \vec {w} ^ i \to \vec {w} ^ i + \varepsilon (\vec {x} - \vec {w} ^ i). \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Renormalize the updated weight vector \( \vec {w_i} \) such that \( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \):

\end{itemize}
\begin{equation*}
\begin{split} \vec {w} ^ i \to \vec {w} ^ i \frac {r} {\sqrt {\vec {w} _i \cdot \vec {w} _i}}. \end{split}
\end{equation*}
\sphinxAtStartPar
The remaining steps of the algorithm, such as determining the initial positions of the representative points, their dynamic creation as they encounter successive data points, etc., remain as in the previously discussed procedure.

\sphinxAtStartPar
The generalization for \( n \) dimensions is obvious: we enter an additional coordinate
\begin{equation*}
\begin{split} x_ {n + 1} = \sqrt {r ^ 2 - x_1 ^ 2 -...- x_n ^ 2},\end{split}
\end{equation*}
\sphinxAtStartPar
hence we have a point on the hyper\sphinxhyphen{}hemisphere \( x_1 ^ 2 + \dots + x_n ^ 2 + x_ {n + 1} ^ 2 = r ^ 2 \),  \(x_ {n + 1} >0\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mf}{0.25}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{n}{rad}\PYG{o}{=}\PYG{l+m+mi}{2} \PYG{c+c1}{\PYGZsh{} radius r}

\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} 
                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
          \PYG{c+c1}{\PYGZsh{} rozszerzenie do półsfery}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}max} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}                    \PYG{c+c1}{\PYGZsh{} maximum}
        \PYG{k}{if} \PYG{n}{dist}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{:}
             \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}   
            \PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of representative points: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of representative points:  4
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{unsupervised_82_0}.png}

\sphinxAtStartPar
We can see that the dot product maximization algorithm yields an almost exactly the same result as the distance squared minimization (cf. \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Fig.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}.

\begin{sphinxadmonition}{note}{Exercises}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The city (Manhattan) metric is defined as
\( d (\vec {x}, \vec {y}) = | x_1-y_1 | + | x_2 - y_2 | \) for points \( \vec {x} \) and \( \vec {y} \).
Repeat the simulations of this lecture with this metric. Draw conclusions.

\item {} 
\sphinxAtStartPar
Run the classification algorithms for more categories in the data sample (generate your own sample).

\item {} 
\sphinxAtStartPar
One\sphinxhyphen{}dimensional data variant: Consider the problem of clustering points of different grayscale. To do this, enter the grayscale as \( s \) from the \( [0,1] \) range and the character data vector
\( (s, \sqrt {1-s ^ 2}) \) which is normalized to 1 (such points lie on a semicircle). Generate a sample of these points and run dynamical clusterization.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Self Organizing Maps}
\label{\detokenize{docs/som:self-organizing-maps}}\label{\detokenize{docs/som::doc}}
\sphinxAtStartPar
A very important and ingenious application of unsupervised learning are the so\sphinxhyphen{}called \sphinxstylestrong{Kohonen nets} (\sphinxhref{https://en.wikipedia.org/wiki/Teuvo\_Kohonen}{Teuvo Kohonen}, i.e. \sphinxstylestrong{self\sphinxhyphen{}organizing mappings (SOM)}. Consider a mapping \(f\) between a \sphinxstylestrong{discrete} \(k\)\sphinxhyphen{}dimensional set (\sphinxstylestrong{grid}) of neurons and \(n\)\sphinxhyphen{}dimensional input data \(D\) (continuous or discrete),
\begin{equation*}
\begin{split}
f: N \to D.
\end{split}
\end{equation*}
\sphinxAtStartPar
Since \(N\) is descrete, each neuron carrries an index consisting of \(k\) natural numbers, denoted as \(\bar {i} = (i_1, i_2, ..., i_k)\). Typically, the dimensions satisfy \(n \ge k\). When \(n > k\), one talks about \sphinxstylestrong{reduction of dimensionality}, as the input space \(D\) has more dimensions than the grid of neurons.

\sphinxAtStartPar
Two examples of such networks are visualized in \{numref{]}\sphinxcode{\sphinxupquote{koh\sphinxhyphen{}fig}}. The left panel shows a 2\sphinxhyphen{}dim. input space \(D\), and a one dimensional grid on neurons. The input point \((x_1,x_2)\) enters all the neuron of the grid, and one of them (with best\sphinxhyphen{}suited weights) becomes the winner (red dot). The gray oval indicates the neighborhood of the winner. The right panel shows an analogous situation for the case of a 3\sphinxhyphen{}dim. input and 2\sphinxhyphen{}dim. grid of neurons. Here, for clarity, we only indicated the edges entering the winner, but they also enter all the other neurons, as in the left panel.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{koha}.png}
\caption{Example of Kohonen networks. Left: 1\sphinxhyphen{}dim. grid of neurons \(N\) and 2\sphinxhyphen{}dim. input space \(D\). Right: 2\sphinxhyphen{}dim. grid of neurons \(N\) and 3\sphinxhyphen{}dim. input space \(D\). The red dot indicates the winner, and the gray oval marks its neighborhood.}\label{\detokenize{docs/som:koh-fig}}\end{figure}

\sphinxAtStartPar
One defines the neuron \sphinxstylestrong{proximity function}, \(\phi (\bar {i}, \bar {j})\), which assigns, to a pair of neurons, a real number depending on their relative position in the grid. This function must decrease with the distance between the neuron indices. A popular choice is a Gaussian,
\begin{equation*}
\begin{split} \phi(\bar{i}, \bar{j})=\exp\left [ -\frac{(i_1-j_1)^2+...+(i_k-j_k)^2}{2 \delta^2} \right ] ,\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\delta\) is the \sphinxstylestrong{neighborhood radius}. For a 1\sphinxhyphen{}dim. grid it becomes \( \phi(i,j)=\exp\left [ -\frac{(i-j)^2}{2 \delta^2} \right ]\).


\section{Kohonen’s algorithm}
\label{\detokenize{docs/som:kohonen-s-algorithm}}
\sphinxAtStartPar
The set up of Kohonen’s algorithm is similar to the unsupervised learning discussed in the previous chapter. Each neuron \(\bar{i}\) obtains weights \(f\left(\bar{i}\right)\), which are elements of \(D\), i.e. form \(n\)\sphinxhyphen{}dimensional vectors. One may simply think of this as placing the neurons in some locations in \(D\). When an input point \(P\) form \(D\) is fed into the network, one looks for a closest neuron, which becomes the \sphinxstylestrong{winner}, exactly as in the algorithm from section {\hyperref[\detokenize{docs/unsupervised:inn-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Interpretation via neural networks}}}}. However, here comes an important difference: Not only the winner is attracted (updated) a bit towards \(P\), but also its neighbors, to a lesser and lesser extent the farther they are from the winner.

\begin{sphinxadmonition}{note}{Winner\sphinxhyphen{}take\sphinxhyphen{}most strategy}

\sphinxAtStartPar
Kohonen’s algorithm involves the “winner take most” strategy, where not only the winner neuron is updated (as in the winner\sphinxhyphen{}take\sphinxhyphen{}all case), but also its neighbors. The neighbors update is strongest for the nearest neighbors, and gradually weakens with the distance from the winner.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Kohnen’s algorithm}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialize (for instance randomly) \(n\)\sphinxhyphen{}dimensional weights \(w_i\), \(i-1,\dots,m\) for all the \(m\) neurons in the grid. Set an initial neighborhood radius \( \delta \) and an initial learning speed \( \varepsilon \).

\item {} 
\sphinxAtStartPar
Choose (randomly) a data point \(P\) wigh coordinates \(x\) from the input space (possibly with an appropriate probability distribution).

\item {} 
\sphinxAtStartPar
Find the \( \bar {l} \) neuron (the winner) for which the distance from \(P\) is the smallest.

\item {} 
\sphinxAtStartPar
The weights of the winner and its neighbors are updated according to the \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}most} recipe:

\end{enumerate}
\begin{equation*}
\begin{split}w_{\bar{i}} \to w_{\bar{i}} + \varepsilon \phi(\bar{i}, \bar{l})(x - w_{\bar{i}}), \hspace{1cm} i=1, . . . , m. 
\end{split}
\end{equation*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Loop from \(1.\) for a specified number of points.

\item {} 
\sphinxAtStartPar
Repeat from \(1.\) in rounds, until a satisfactory result is obtained or the maximum number of rounds is reached. In each round  \sphinxstylestrong{reduce} \( \varepsilon \) and \( \delta \) according to a chosen policy.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
The way the reduction of \( \varepsilon \) and \( \delta \) is done is very important for the desired outcome of the algorithm (see exercises).


\subsection{2\sphinxhyphen{}dim. data and 1\sphinxhyphen{}dim. neuron grid}
\label{\detokenize{docs/som:dim-data-and-1-dim-neuron-grid}}
\sphinxAtStartPar
Let us see how the procedure works on a simple example. We map a grid of \sphinxstylestrong{num} neurons into (our favorite!) circle. So we have here the reduction of dimensions: \(n=2\), \(k=1\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{num}\PYG{o}{=}\PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of neurons}
\end{sphinxVerbatim}

\sphinxAtStartPar
The proximity function

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{phi}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{k}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}                       \PYG{c+c1}{\PYGZsh{} proximity function}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{n}{k}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Gaussian}
\end{sphinxVerbatim}

\sphinxAtStartPar
looks as follows around \(k=50\) and for the width parameter \(\delta=5\):

\noindent\sphinxincludegraphics{{som_16_0}.png}

\sphinxAtStartPar
As a feature of the Gaussian, at \(|k-i|=\delta\) it drops to \(~60\%\) of the central value, and at \(|k-i|=3\delta\) to \(~1\%\), a tiny fraction. Hence \(\delta\) controls the size of the neighborhood of the winner. The neuron farther away from the winner than \(3\delta\) are practically left unupdated.

\sphinxAtStartPar
We initiate the network by by placing the grid on the plane (on the square \([0,1]\times [0,1]\)), with a random location of each neuron. The line is drawn to guide the eye along the neuron indices: \(1,2,3,\dots m\),  which are chaotically distributed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random initialization of weights}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_20_0}.png}

\sphinxAtStartPar
The line connects the subsequent neurons … They are chaotically scattered around the region.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial learning speed }
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} initial neighborhood distance}
\PYG{n}{ste}\PYG{o}{=}\PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} inital number of caried out steps}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Kohonen\PYGZsq{}s algorithm}
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.98}      \PYG{c+c1}{\PYGZsh{} dicrease learning speed}
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.95}        \PYG{c+c1}{\PYGZsh{} ... and the neighborhood distance}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{ste}\PYG{o}{=}\PYG{n}{ste}\PYG{o}{+}\PYG{l+m+mi}{1}               \PYG{c+c1}{\PYGZsh{} steps}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{]} 
         \PYG{c+c1}{\PYGZsh{} array of squares of Euclidean disances between p and the neuron locations}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the winner}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} for all the neurons}
            \PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{,}\PYG{n}{k}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} 
             \PYG{c+c1}{\PYGZsh{} update of the neuron locations (weights), depending on proximity}
\end{sphinxVerbatim}

\sphinxAtStartPar
As the algorith progresses (see \hyperref[\detokenize{docs/som:kohstory-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory-fig}}}) the neuron grid first “straightens up”, and then gradually fills the whole space \(D\) (circle) in such a way that the neurons with adjacent indices are located close to each other.
Figuratively speaking, a new point \(x\) attracts towerds itself the nearest neuron (the winner), and to a weaker extent its neighbors. At the beginning of the algorithm the neighborhood distance \sphinxstylestrong{de} is large, so large chunks of the nighboring point in input grid are pulled together, and the arrangement looks as the top right corner of \hyperref[\detokenize{docs/som:kohstory-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory-fig}}}. At later stages \sphinxstylestrong{de} is smaller, so only the winner and possibly its immediate neighbors are attracted to a new point.
After completion, individual neurons “specialize” (are close to) in a certain data area.

\sphinxAtStartPar
In the present example, after about 20000 steps the result practically stops to change.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{kaall}.png}
\caption{Progress of Kohonen’s algorithm. The lines, drawn to guide the eye, connects neurons with adjacent indices.}\label{\detokenize{docs/som:kohstory-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Kohonen network as a classifier}

\sphinxAtStartPar
Having the trained network, we may use it as classifier similarly as in chapter \{\textbackslash{}ref\}\sphinxcode{\sphinxupquote{un\sphinxhyphen{}lab}}. We label a point from \(D\) with the index of the nearest neuron.
\end{sphinxadmonition}

\sphinxAtStartPar
The plots in \hyperref[\detokenize{docs/som:kohstory-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory-fig}}} are plotted in coordinates \((x_1,x_2)\), that is, from the “point of view” of the \(D\)\sphinxhyphen{}space. One may also look at the result from the point of view of the \(N\)\sphinxhyphen{}space, i.e. plot \(x_1\) and \(x_2\) as functions of the neuron index \(i\):

\noindent\sphinxincludegraphics{{som_30_0}.png}

\sphinxAtStartPar
We note that the jumps in the above plotted curves are small. This can be seen quntitatively in the histogram below, where the average distance is about 0.07.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dd}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{p}{(}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} array of distances between subsequent neurons in the grid}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.8}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{dd}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{;}   \PYG{c+c1}{\PYGZsh{} histogram}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_32_0}.png}

\begin{sphinxadmonition}{note}{Remarks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
We took a situation in which the data space with the dimension \(n = 2\) is “sampled” by a discrete set of neurons forming  \(k=1\)\sphinxhyphen{}dimensional grid. Hence we have dimensional reduction.

\item {} 
\sphinxAtStartPar
The outcome of the algorithm is a network in which a given neuron “focuses” on data from its vicinity. In a general case where the data are non\sphinxhyphen{}uniformly distributed, the neurons would fill the area containing more data more densely.

\item {} 
\sphinxAtStartPar
The fact that there are no line intersections is a manifestation of topological features, discussed in detail below.

\item {} 
\sphinxAtStartPar
The policy of choosing initial \(\delta\) and \(\varepsilon \) parameters and reducing them appropriately in subsequent rounds is based on experience and non\sphinxhyphen{}trivial.

\item {} 
\sphinxAtStartPar
The final result is not unequivocal, i.e. running the algorithm with a different initialization of the weights (positions of neurons) yields a different outcome, equally “good”.

\item {} 
\sphinxAtStartPar
Finally, the progress and result of the algorithm is reminiscent of the construction of the \sphinxhref{https://en.wikipedia.org/wiki/Peano\_curve}{Peano curve} in mathematics, which fills an area with a line.

\end{itemize}
\end{sphinxadmonition}


\subsection{2 dim. color map}
\label{\detokenize{docs/som:dim-color-map}}
\sphinxAtStartPar
Now we come a case of 3\sphinxhyphen{}dim. data and 2\sphinxhyphen{}dim. neuron grid, which is a situation from the right panel of \hyperref[\detokenize{docs/som:koh-fig}]{Fig.\@ \ref{\detokenize{docs/som:koh-fig}}}. An RGB color is described with three numbers from \([0,1]\), so it can nicely serve as input in our example.

\sphinxAtStartPar
The distance squared between two colors (this is just a distance between two points in the 3\sphinxhyphen{}dim. space) is taken in the Euclidean form

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{dist3}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} 
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Square of the Euclidean distance between points p1 and p2}
\PYG{l+s+sd}{    in 3 dimensions.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{phi2}\PYG{p}{(}\PYG{n}{ix}\PYG{p}{,}\PYG{n}{iy}\PYG{p}{,}\PYG{n}{kx}\PYG{p}{,}\PYG{n}{ky}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} proximity function for 2\PYGZhy{}dim. grid}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{p}{(}\PYG{n}{ix}\PYG{o}{\PYGZhy{}}\PYG{n}{kx}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{iy}\PYG{o}{\PYGZhy{}}\PYG{n}{ky}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Gaussian}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{rgbn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{r}\PYG{p}{,}\PYG{n}{g}\PYG{p}{,}\PYG{n}{b}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{norm}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{r}\PYG{o}{*}\PYG{n}{r}\PYG{o}{+}\PYG{n}{g}\PYG{o}{*}\PYG{n}{g}\PYG{o}{+}\PYG{n}{b}\PYG{o}{*}\PYG{n}{b}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{r}\PYG{p}{,}\PYG{n}{g}\PYG{p}{,}\PYG{n}{b}\PYG{p}{]}\PYG{o}{/}\PYG{n}{norm}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.1234176 , 0.55504897, 0.82261093])
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we generate a sample of \sphinxstylestrong{ns} points with RGB colors:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns}\PYG{o}{=}\PYG{l+m+mi}{40}  \PYG{c+c1}{\PYGZsh{} number of colors in the sample}

\PYG{n}{samp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}
       \PYG{c+c1}{\PYGZsh{} random sample}

\PYG{n}{pls}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{;} 
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_41_0}.png}

\sphinxAtStartPar
We use \sphinxstylestrong{size} x \sphinxstylestrong{size} grid of neurons. Each neuron’s position (color, or weight) in the 3\sphinxhyphen{}dim. space is initialized randomly:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{40}  \PYG{c+c1}{\PYGZsh{} neuron array of size x size (40 x 40)}

\PYG{n}{tab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{p}{,}\PYG{n}{size}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} i index in the grid    }
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} j index in the grid}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} RGB: 0\PYGZhy{}red, 1\PYGZhy{}green, 2\PYGZhy{}blue}
            \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random number form [0,1]}
            \PYG{c+c1}{\PYGZsh{} 3 RGB components for neuron in the grid positin (i,j)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_44_0}.png}

\sphinxAtStartPar
Now we are ready to run Kohonen’s algorithm:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial parameters}
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{20}   
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.995}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.96}           \PYG{c+c1}{\PYGZsh{} de shrinks faster than eps     }
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the points in the data sample       }
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} point from the sample}
\PYG{c+c1}{\PYGZsh{}        p=[np.random.random() for \PYGZus{} in range(3)]}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{]} 
                        \PYG{c+c1}{\PYGZsh{} distance to all neurons}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} the winner index}
        \PYG{n}{ind\PYGZus{}x}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{/}\PYG{o}{/}\PYG{n}{size}       \PYG{c+c1}{\PYGZsh{} a trick to get a 2\PYGZhy{}dim index}
        \PYG{n}{ind\PYGZus{}y}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{\PYGZpc{}}\PYG{k}{size}

        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:} 
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi2}\PYG{p}{(}\PYG{n}{ind\PYGZus{}x}\PYG{p}{,}\PYG{n}{ind\PYGZus{}y}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update         }
\end{sphinxVerbatim}

\sphinxAtStartPar
As a result we get an arrangement of our color sample in two dimensions in such a way that the neighboring areas have a similar color:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Kohonen color map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_49_0}.png}

\begin{sphinxadmonition}{note}{Remarks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The areas for the individual colors of the sample have a comparable area. Generally, the area is proportional to the sample size.

\item {} 
\sphinxAtStartPar
To get sharper boundaries between regions, de has to shrink faster than eps. Then, in the final stage of learning, the neuron update process takes place with small neighborhood radius.

\end{itemize}
\end{sphinxadmonition}


\section{U\sphinxhyphen{}matrix}
\label{\detokenize{docs/som:u-matrix}}
\sphinxAtStartPar
A convenient way to present the results of the Kohonen algorithm when the grid is 2\sphinxhyphen{}dimensional is via the \sphinxstylestrong{unified distance matrix} (shortly \sphinxstylestrong{U\sphinxhyphen{}matrix}). The idea is to plot a 2\sphinxhyphen{}dimensional grayscale map with the intensity given by the averaged distance (in \(D\)\sphinxhyphen{}space) of the neuron to its immediate neighbors, and not the neuron poperty itself (such as the color in the plot above). This is particularly useful when the dimension of the input space is large, when it is difficult to visualize the results directly.

\sphinxAtStartPar
The definition of a U\sphinxhyphen{}matrix element \(U_{ij}\) is shown in \hyperref[\detokenize{docs/som:udm-fig}]{Fig.\@ \ref{\detokenize{docs/som:udm-fig}}}. Let \(d\) be the distance in \(D\)\sphinxhyphen{}space and \([i,j]\) denote the neuron of indices \(i,j\) . We take
\begin{equation*}
\begin{split}
U_{ij}=\sqrt{d\left([i,j],[i+1,j]\right)^2+d\left([i,j],[i-1,j]\right)^2+
        d\left([i,j],[i,j+1]\right)^2+d\left([i,j],[i,j-1]\right)^2 }
\end{split}
\end{equation*}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=150\sphinxpxdimen]{{udm}.png}
\caption{Construction of \(U_{ij}\): geometric average of the distance along the indicated links.}\label{\detokenize{docs/som:udm-fig}}\end{figure}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{udm}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} create U\PYGZhy{}matrix with elements = 0}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}
                            \PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
                     \PYG{c+c1}{\PYGZsh{} U\PYGZhy{}matrix as explained above}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{U\PYGZhy{}matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loops over indices exclude the first and last point of the grid}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 
            \PYG{c+c1}{\PYGZsh{} color format: [R,G,B,intensity], 2 just scales up}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_55_0}.png}

\sphinxAtStartPar
The white regions in the above figure correspond to clusters, separated with the darker regions. The higher is the boundary between clusters, the darker the plot.
The same may be visualized in a 3D figure:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{axes1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{111}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{3d}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{xx\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{xx\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{x\PYGZus{}1}\PYG{p}{,} \PYG{n}{x\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{xx\PYGZus{}1}\PYG{p}{,} \PYG{n}{xx\PYGZus{}2}\PYG{p}{)}

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{x\PYGZus{}1}\PYG{p}{,}\PYG{n}{x\PYGZus{}2}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{gray}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{U\PYGZhy{}matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_57_0}.png}

\sphinxAtStartPar
We an now classify a given (new) data point according to the obtained map.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nd}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_60_0}.png}

\sphinxAtStartPar
It is useful to obtain a distance map from from this point:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tad}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{p}{,}\PYG{n}{size}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{tad}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{=}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{nd}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        

\PYG{n}{ind\PYGZus{}m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{tad}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} winner}
\PYG{n}{in\PYGZus{}x}\PYG{o}{=}\PYG{n}{ind\PYGZus{}m}\PYG{o}{/}\PYG{o}{/}\PYG{n}{size}      
\PYG{n}{in\PYGZus{}y}\PYG{o}{=}\PYG{n}{ind\PYGZus{}m}\PYG{o}{\PYGZpc{}}\PYG{k}{size} 

\PYG{n}{da}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{tad}\PYG{p}{[}\PYG{n}{in\PYGZus{}x}\PYG{p}{]}\PYG{p}{[}\PYG{n}{in\PYGZus{}y}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Closest neuron grid coordinates: (}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{in\PYGZus{}x}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{in\PYGZus{}y}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Distance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{da}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Closest neuron grid coordinates: ( 28 , 5 )
Distance:  0.032
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_63_0}.png}

\sphinxAtStartPar
The lightest region indicates the cluster, to which the point belongs. The darker the region, the lager the distance to the corresponding neurons.


\subsection{Mapping colors on a line}
\label{\detokenize{docs/som:mapping-colors-on-a-line}}
\sphinxAtStartPar
In this subsection we present an example of reduction of 3\sphinxhyphen{}dim. data in a 1\sphinxhyphen{}dim. neuron grid. This proceeds along the lines of the previous evaluation, so we are brief in comments.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns}\PYG{o}{=}\PYG{l+m+mi}{8}

\PYG{n}{samp}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample colors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)} 

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_67_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{si}\PYG{o}{=}\PYG{l+m+mi}{50}  \PYG{c+c1}{\PYGZsh{} 1D grid of si neurons}

\PYG{n}{tab2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{si}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{:}      
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} RGB components}
        \PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random initialization}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_69_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}    
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{20}   
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:} 
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.99}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.96}        
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}       
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}          
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_72_0}.png}

\sphinxAtStartPar
We note smooth transitions between colors. The formation of clusters can be seen with \(U\)\sphinxhyphen{}matrix, which now is of course one\sphinxhyphen{}dimensional:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ta2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{si}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{si}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ta2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_75_0}.png}

\sphinxAtStartPar
The minima (there are 8 of them, which is the multiplicity of the sample) indicate the clusters. The height of the separating peaks shows how much the colors differ.


\subsection{Wikipedia articles’ similarity}
\label{\detokenize{docs/som:wikipedia-articles-similarity}}
\sphinxAtStartPar
The input space may have very large dimensions. In the \sphinxhref{https://en.wikipedia.org/wiki/Self-organizing\_map}{Wikipedia example} below, one takes articles from various fields and computes frequencies of words (for instance, how  many times the word “goalkeeper” has been used, divided by the total number of words in the article). So essentially the dimensionality is of the oder of the number of English words, a huge number \(\sim 10^5\)! Then one uses Kohonen’s algorith to carry out reduction into a 2\sphinxhyphen{}dim. grid of neurons. The U\sphinxhyphen{}matrix is following:

\noindent\sphinxincludegraphics{{som_79_0}.jpg}

\sphinxAtStartPar
In particular we note that articles on sports are special and form a well\sphinxhyphen{}defined cluster. This is not surprising, as the jargon is very specific.


\section{Mapping 2\sphinxhyphen{}dim. data into a 2\sphinxhyphen{}dim. grid}
\label{\detokenize{docs/som:mapping-2-dim-data-into-a-2-dim-grid}}
\sphinxAtStartPar
Finally, we come to a very important case of mapping 2\sphinxhyphen{}dim. data on a 2\sphinxhyphen{}dim. grid, as this is realized in our vision system between the retina and the visual cortex.

\sphinxAtStartPar
The algorithm proceedes analogously to the previous cases. We initialize an \(n \times n\) grid of neurons, placing them randomly in the square \([0,1]\times [0,1]\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}
\PYG{n}{sam}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
The lines, drawn to guide the eye, join the adjacent index pairs {[}i,j{]} and {[}i+1,j{]}, or {[}i,j{]} and {[}i,j+1{]}. The neurons in the interior of the grid have 4 nearest neighbors, those at the boundary 3, except for the corners, which have 2.

\noindent\sphinxincludegraphics{{som_85_0}.png}

\sphinxAtStartPar
We note a total initial “chaos”, as the neurons are located randomly. Now comes Kohonen’s miracle:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{3}   \PYG{c+c1}{\PYGZsh{} initial neighborhood distance}
\PYG{n}{nr} \PYG{o}{=} \PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of rounds}
\PYG{n}{rep}\PYG{o}{=} \PYG{l+m+mi}{300} \PYG{c+c1}{\PYGZsh{} number of points in each round}
\PYG{n}{ste}\PYG{o}{=}\PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} inital number of caried out steps}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} analogous to the previous codes}
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.97}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.98}         
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{rep}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} repeat for rep points}
        \PYG{n}{ste}\PYG{o}{=}\PYG{n}{ste}\PYG{o}{+}\PYG{l+m+mi}{1}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} 
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{l} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} 
        \PYG{n}{ind\PYGZus{}i}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{\PYGZpc{}}\PYG{k}{n}
        \PYG{n}{ind\PYGZus{}j}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{/}\PYG{o}{/}\PYG{n}{n}       
        
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:} 
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{sam}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{n}\PYG{o}{*}\PYG{n}{j}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi2}\PYG{p}{(}\PYG{n}{ind\PYGZus{}i}\PYG{p}{,}\PYG{n}{ind\PYGZus{}j}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{n}\PYG{o}{*}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} fl.savefig(\PYGZsq{}images/kb30000.png\PYGZsq{})}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{kball}.png}
\caption{Progress of Kohonen’s algorithm. The lines, drawn to guide the eye, connects neurons with adjacent indices.}\label{\detokenize{docs/som:kohstory2-fig}}\end{figure}

\sphinxAtStartPar
As the algorithm progresses, chaos changes into nearly perfect order, with the grid placed unifomly in the square of the data, with only slight displacements from a regular arrangement. On the way, at 600 steps, we notice a phenomenon called “twist”, where many neurons have a close location an the grid is crumpled.


\section{Topological properties}
\label{\detokenize{docs/som:topological-properties}}
\sphinxAtStartPar
Recall the Voronoi construction of categories introduced in section {\hyperref[\detokenize{docs/unsupervised:vor-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Voronoi areas}}}}. One can simply use it now, treating the neurons from a grid as the Voronoi points. The Voronoi construction provides a mapping \(v\) from the data space \(D\) to the neuron space \(N\),
\begin{equation*}
\begin{split} 
v: D \to N 
\end{split}
\end{equation*}
\sphinxAtStartPar
(note that this goes in the opposite direction than function \(f\) defined at the beginning of this chapter).

\sphinxAtStartPar
For instance, we take the bottom right panel of \hyperref[\detokenize{docs/som:kohstory2-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory2-fig}}}, construct the Voronoi areas for all the neurons, and thus obtain a mapping for all poins in the \((x_1,x_2)\) square. The reader may notice that there is an ambiguity for points lying exactly at boundaries between areas, but this can be taken care of by using an additional prescription, for instance, selecting a neuron lying at direction which has the lowest angle, etc.

\sphinxAtStartPar
Now a key observation:

\begin{sphinxadmonition}{note}{Topological property}

\sphinxAtStartPar
For situations such as in the bottom right panel of \hyperref[\detokenize{docs/som:kohstory2-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory2-fig}}}, mapping \(v\) has the property that when \(d_1\) and \(d_2\) from \(D\) are close to each other, then also their corresponding neurons are close, i.e. the indices \(v(d_1)\) and \(v(d_2)\) are close.
\end{sphinxadmonition}

\sphinxAtStartPar
This observation is obvious. Since \(d_1\) and \(d_2\) are close (and we mean very close, closer than the resolution of the grid), then they must belong either to
\begin{itemize}
\item {} 
\sphinxAtStartPar
the same Voronoi area, where \(v(d_1)=v(d_2)\), or

\item {} 
\sphinxAtStartPar
neighboring Voronoi areas.

\end{itemize}

\sphinxAtStartPar
Since for the considered situation the neighboring areas have the grid indices differing by 1, the conclusion that \(v(d_1)\) and \(v(d_2)\) are close follows.

\sphinxAtStartPar
Note that this feature of Hohonen’s maps is far from trivial and does not hold for a general mapping. Imagine for instance that we stop our simulations for \hyperref[\detokenize{docs/som:kohstory2-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory2-fig}}} after 40 steps (top central panel) and are left with a “twisted” grid. In the vicinity of the twist, the indices of the adjacent Voronoi areas differ largely, and the considered topological property no longer holds.

\sphinxAtStartPar
The discussed topological propery has general and far\sphinxhyphen{}reaching consequences. First, it allows to carry over “shapes” from \(D\) to \(N\). Imagine that we have a circle in the \(D\)\sphinxhyphen{}space:

\noindent\sphinxincludegraphics{{som_97_0}.png}

\sphinxAtStartPar
where the red points indicate the winners for certain sections of the circle. When we draw these points alone in the \(N\) space, we get

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{ci}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{ci}\PYG{o}{\PYGZpc{}}\PYG{k}{10},c=\PYGZsq{}red\PYGZsq{},s=5)

        
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_99_0}.png}

\sphinxAtStartPar
This looks as a (rough and discrete) circle. Note that in our example we only have \(n^2=100\) pixels to our disposal, and the image would look better and better with increasing \(n\). At some point one would reach the 10M pixels resolution of typical camera, and then the image would look smooth.

\begin{sphinxadmonition}{note}{Vision}

\sphinxAtStartPar
The topological property of the Kohonen mapping is believed to have a prime importance in our vision system and the perception of objects. …
\end{sphinxadmonition}

\sphinxAtStartPar
Another key topological feature is the preservation of connectedness. If an area \(A\) in \(D\) is connected (is in one piece), then its image \(v(A)\) in \(N\) is also connected (we ignore the desired rigor here as to what “connected” means in a discrete space and rely on intuition). So things do not get “warped up” when transforming form \(D\) to \(N\).

\sphinxAtStartPar
Note that the discussed topological features need not be present when dimensionality is reduced, as in our previous examples. Take for instance the bottom right panel of \hyperref[\detokenize{docs/som:kohstory-fig}]{Fig.\@ \ref{\detokenize{docs/som:kohstory-fig}}}. There, many neighboring pairs of Voronoi areas correspond to distant indices, and it is no longer true that \(v(d_1)\) and \(v(d_2)\) are close for close \(d_1\) and \(d_2\), as these points may belong to different Voronoi areas with distant indices.

\sphinxAtStartPar
Our example with the circle now looks like this:

\noindent\sphinxincludegraphics{{som_104_0}.png}

\sphinxAtStartPar
When we go along the grid indices, we see the image of our circle (red dots) as a bunch of \sphinxstylestrong{disconnected} sections. Topology is not preserved.

\noindent\sphinxincludegraphics{{som_106_0}.png}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Topological features of Kohonen’s maps hold for equal dimensionalities of the input space and the neuron grid, \(n=k\), and in general do not hold for the reduced dimensionality case, \(k<n\).
\end{sphinxadmonition}


\section{Lateral inhibition}
\label{\detokenize{docs/som:lateral-inhibition}}
\sphinxAtStartPar
In a last topic of these lectures, we return to the issue of how the competition for the “winner” is realized in ANNs. Above, we have just used the minimum (or maximum, when the signal was extended to a hyperphere) functions, but this is embarrasingly outside of the framework. Such an insection of which neuron yields the strongest signal would require an “external wizard”, or some sort of a control unit. Mathematically, it is easy to imagine, but the challenge is to build it from neurons.

\sphinxAtStartPar
Actually, if the neurons in a layer “talk” to one another, we can have a contest. An architecture as in \hyperref[\detokenize{docs/som:lat-fig}]{Fig.\@ \ref{\detokenize{docs/som:lat-fig}}} allows for an arrangement of competition and a natural realization of the winner\sphinxhyphen{}take\sphinxhyphen{}most mechanism.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=220\sphinxpxdimen]{{lat3}.png}
\caption{Network with inter\sphinxhyphen{}neuron coupling used for modeling lateral inhibition. All the neurons are connected in both directions (lines without arrows).}\label{\detokenize{docs/som:lat-fig}}\end{figure}

\sphinxAtStartPar
Neuron number \(i\) receives the signal \(s_i = x w_i\), where \(x\) is the input (the same for all the neurons), and \(w_i\) is the weight. It produces an output \(y_i\), but part of it is sent neuron \(j\) as \(F_{ji} y_i\), where \(F_{ij}\) is the coupling strength (we assume \(F_{ii}=0\) \sphinxhyphen{} no self coupling). Neuron \(i\) also receives output from neuron \(j\) in the form \(F_{ij} y_j\). Summing over all the neurons yields
\begin{equation*}
\begin{split} 
y_i = s_i + \sum_{j\neq i} F_{ij} y_j, 
\end{split}
\end{equation*}
\sphinxAtStartPar
which in the matrix notation becomes \( y = s + F y\), or \(y(I-F)=s\), where \(I\) is the identity matrix. Solving for \(y\) gives
\begin{equation}\label{equation:docs/som:eq-lat}
\begin{split}y= (I-F)^{-1} s.\end{split}
\end{equation}
\sphinxAtStartPar
One needs to model appropriately the coupling matrix \(F\). We take

\sphinxAtStartPar
\( F_ {ii} = \) 0,

\sphinxAtStartPar
\( F_ {ij} = - a \exp (- | i-j | / b) ~~ \) for \( i \neq j \), \( ~~ a, b> 0 \),

\sphinxAtStartPar
i.e. assume attenuation (negative values), which is strongest for close neighbors and decreases with distance with a characteristic scale \(b\).

\sphinxAtStartPar
The Python implemetation is as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns} \PYG{o}{=} \PYG{l+m+mi}{30}\PYG{p}{;}       \PYG{c+c1}{\PYGZsh{} number of neurons}
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mi}{4}\PYG{p}{;}         \PYG{c+c1}{\PYGZsh{} parameter controling the decrease of damping with distance}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}         \PYG{c+c1}{\PYGZsh{} magnitude of damping}

\PYG{n}{F}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{a}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{n}{j}\PYG{p}{)}\PYG{o}{/}\PYG{n}{b}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} exponential fall\PYGZhy{}off}
    
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{F}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{0}       \PYG{c+c1}{\PYGZsh{} no self\PYGZhy{}damping}
    
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{som_115_0}.png}

\sphinxAtStartPar
We assume a bell\sphinxhyphen{}shaped Lorentzian input signal \(s\), with a maximum in the middle neuron. The width is controled with \sphinxstylestrong{D}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{D}\PYG{o}{=}\PYG{l+m+mi}{2}
\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{D}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{p}{(}\PYG{p}{(}\PYG{n}{i} \PYG{o}{\PYGZhy{}} \PYG{n}{ns}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{D}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Lorentzian function}
\end{sphinxVerbatim}

\sphinxAtStartPar
and solve Eq. \DUrole{xref,std,std-ref}{eq\sphinxhyphen{}lat} via inverting the \((I-F)\) matrix:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{invF}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{identity}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{F}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} matrix inversion}
\PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{invF}\PYG{p}{,}\PYG{n}{s}\PYG{p}{)}                      \PYG{c+c1}{\PYGZsh{} multiplication}
\PYG{n}{y}\PYG{o}{=}\PYG{n}{y}\PYG{o}{/}\PYG{n}{y}\PYG{p}{[}\PYG{l+m+mi}{15}\PYG{p}{]}                             \PYG{c+c1}{\PYGZsh{} normalization (inessential) }
\end{sphinxVerbatim}

\sphinxAtStartPar
What follows is quite remarkable: the output signal \(y\) is much narrower form the input signal, which is a realization of the “winner\sphinxhyphen{}take\sphinxhyphen{}all” scenario.

\noindent\sphinxincludegraphics{{som_121_0}.png}

\sphinxAtStartPar
Pyramidal neurons

\begin{sphinxadmonition}{note}{Exercises}

\sphinxAtStartPar
Construct a Kohonen mapping form a 2D shape (square, cicle, 2 disjoint squares) on a 2D grid of neurons.
\end{sphinxadmonition}


\chapter{Concluding remarks}
\label{\detokenize{docs/conclusion:concluding-remarks}}\label{\detokenize{docs/conclusion::doc}}
\sphinxAtStartPar
… Conclude here our discussion of the supervised learning and the back proparation, we provide a number of remarks nd hints. First, in programmers life, bulding a well\sphinxhyphen{}functioning ANN, even for simple problems as used for illustrations up to now, can be a frustrating eperience! …

\sphinxAtStartPar
Neurological impossiblity of backprop.

\sphinxAtStartPar
HCP \sphinxhyphen{} Human Connectome Project \sphinxurl{http://www.humanconnectomeproject.org/}

\sphinxAtStartPar
Initial conditions for minimization, basen of convergence, learning stategy, improvements of steepest descent …

\sphinxAtStartPar
Professional libraries, experience verified with success …

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The references provided in the text as hyperlinks are not repeated in the list below.
\end{sphinxadmonition}

\sphinxAtStartPar



\chapter{Appendix}
\label{\detokenize{docs/lib_app:appendix}}\label{\detokenize{docs/lib_app:app-lab}}\label{\detokenize{docs/lib_app::doc}}

\section{\sphinxstylestrong{neural} package}
\label{\detokenize{docs/lib_app:neural-package}}
\sphinxAtStartPar
The structure of the library tree is as follows:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
lib\PYGZus{}nn
└── neural
    ├── \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}.py
    ├── draw.py
    └── func.py
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{func.py} module}
\label{\detokenize{docs/lib_app:func-py-module}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Contains functions repeatedy used in the lecture}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}


\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} step \PYGZdq{}\PYGZdq{}\PYGZdq{}} \PYG{c+c1}{\PYGZsh{} komentarz w potrójnym cudzysłowie}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
        
\PYG{k}{def} \PYG{n+nf}{neuron}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} (in the neural library)}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    MCP neuron}

\PYG{l+s+sd}{    x: array of inputs  [x1, x2,...,xn]}
\PYG{l+s+sd}{    w: array of weights [w0, w1, w2,...,wn]}
\PYG{l+s+sd}{    f: activation function, with step as default}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{f}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert x0=1, signal s=x.w, output f(s)}
    
\PYG{k}{def} \PYG{n+nf}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{T}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} sigmoid \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{o}{/}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
    
\PYG{k}{def} \PYG{n+nf}{dsig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{T}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}derivative of sigmoid\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{T}
    
\PYG{k}{def} \PYG{n+nf}{lin}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} linear function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{s}
    
\PYG{k}{def} \PYG{n+nf}{dlin}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} derivative of linear function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}
    
\PYG{k}{def} \PYG{n+nf}{relu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} ReLU function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{s}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}

\PYG{k}{def} \PYG{n+nf}{drelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} derivative of ReLU function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
        
\PYG{k}{def} \PYG{n+nf}{lrelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} leaky ReLU function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{s}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{a}\PYG{o}{*}\PYG{n}{s}

\PYG{k}{def} \PYG{n+nf}{dlrelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} derivative of leaky ReLU function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{a}
        
\PYG{k}{def} \PYG{n+nf}{softplus}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} softplus function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{dsoftplus}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} derivative softplus function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{eucl}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} square of the Euclidean distance}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Squqre of Euclidean distance between to points in 2\PYGZhy{}dim. space}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: p1, p1 \PYGZhy{} arrays in the format [x1,x2]}
\PYG{l+s+sd}{    return: square of Euclidean distance}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}

\PYG{k}{def} \PYG{n+nf}{l2}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}for separating line\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{\PYGZhy{}}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{]}
    
\PYG{k}{def} \PYG{n+nf}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    random number from [\PYGZhy{}0.5,0.5]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}
    
\PYG{k}{def} \PYG{n+nf}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    random point from a cirle}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{y}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{:}
            \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}\PYG{p}{)}
    
\PYG{k}{def} \PYG{n+nf}{point}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    random point from [0,1]x[0,1]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}\PYG{p}{)}
    
\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Set weights randomly}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s \PYGZhy{} scale factor: each weight is in the range [\PYGZhy{}0.s, 0.5s]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    w \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[}\PYG{n}{s}\PYG{o}{*}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{w}


\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}val\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Set weights to a constant value}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    a \PYGZhy{} value for each weight}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    w \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[}\PYG{n}{a} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{w}
    

\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{ff}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    ff \PYGZhy{} activation function (default: step)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over layers till before last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication}
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

    \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}

\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{dsig}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} disctionary of weights}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed}
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivaive of f}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer}
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                    \PYG{n}{df}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{ff}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{ffo}\PYG{o}{=}\PYG{n}{lin}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation with different output activation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    f  \PYGZhy{} activation function (default: sigmoid)}
\PYG{l+s+sd}{    fo \PYGZhy{} activation function in the output layer (default: linear)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over layers till before last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication}
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

    \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ffo}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output activation function}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}                    \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}


\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{dsig}\PYG{p}{,} \PYG{n}{fo}\PYG{o}{=}\PYG{n}{lin}\PYG{p}{,} \PYG{n}{dfo}\PYG{o}{=}\PYG{n}{dlin}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} disctionary of weights}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed}
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivaive of f}
\PYG{l+s+sd}{    fo  \PYGZhy{} activation function in the output layer (default: linear)}
\PYG{l+s+sd}{    dfo \PYGZhy{} derivative of fo}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer}
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{,}\PYG{n}{ffo}\PYG{o}{=}\PYG{n}{fo}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                   \PYG{n}{dfo}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}

    
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{draw.py} module}
\label{\detokenize{docs/lib_app:draw-py-module}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Plotting functions used in the lecture.}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}


\PYG{k}{def} \PYG{n+nf}{plot}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{activation function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x\PYGZus{}label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y\PYGZus{}label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{start}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{stop}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{samples}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Wrapper on matplotlib.pyplot library.}
\PYG{l+s+sd}{    Plots functions passed as *args.}
\PYG{l+s+sd}{    Functions need to accept a single number argument and return a single number.}
\PYG{l+s+sd}{    Example usage: plot(lambda x: x * x)}
\PYG{l+s+sd}{                   plot(func.step,func.sig)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} defines range and detail level of the plot}
    \PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{stop}\PYG{p}{,} \PYG{n}{samples}\PYG{p}{)}

    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.8}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{x\PYGZus{}label}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{n}{y\PYGZus{}label}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{fun} \PYG{o+ow}{in} \PYG{n}{args}\PYG{p}{:}
        \PYG{n}{data\PYGZus{}to\PYGZus{}plot} \PYG{o}{=} \PYG{p}{[}\PYG{n}{fun}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{data\PYGZus{}to\PYGZus{}plot}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}simp}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture without bias nores}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: array of numbers of nodes in subsequent layers [n0, n1, n2,...]}
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l\PYGZus{}layer}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l\PYGZus{}layer}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            
\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l\PYGZus{}layer}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw network without bias nodes}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{wid}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture with weights}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    wid \PYGZhy{} controls the width of the lines}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{th}\PYG{o}{=}\PYG{n}{wid}\PYG{o}{*}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
                \PYG{k}{if} \PYG{n}{th}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{n}{th}\PYG{o}{=}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{th}\PYG{p}{)}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{,}\PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{th}\PYG{p}{)}
 
\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{wid}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture with weights and signals}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    wid \PYGZhy{} controls the width of the lines}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x \PYGZhy{} dictionary the the signal in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} input layer}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.27}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} intermediate layer}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} output layer}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{th}\PYG{o}{=}\PYG{n}{wid}\PYG{o}{*}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
                \PYG{k}{if} \PYG{n}{th}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{n}{th}\PYG{o}{=}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{th}\PYG{p}{)}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{,}\PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{th}\PYG{p}{)}
 
\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}
    
    
\PYG{k}{def} \PYG{n+nf}{l2}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{\PYGZhy{}}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{]}

\end{sphinxVerbatim}

\begin{sphinxthebibliography}{Gut16}
\bibitem[Gut16]{docs/conclusion:id2}
\sphinxAtStartPar
John Guttag. \sphinxstyleemphasis{Introduction to computation and programming using Python: With application to understanding data}. MIT Press, 2016.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}