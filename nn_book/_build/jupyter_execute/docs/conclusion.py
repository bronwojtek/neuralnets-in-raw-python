#!/usr/bin/env python
# coding: utf-8

# # Concluding remarks

# In a programmer's life, building a well-functioning ANN, even for simple problems as used for illustrations in these lectures, can be a truly frustrating experience! There are many subtleties involved on the way. To list a few that we have encountered in this course, one faces a choice of the network architecture, freedom in the initialization of hyperparameters, a choice of the initial learning speed or the neighborhood distance and their update strategy, one tackles with the emergence of many local minima, a choice of the neuron activation function, or deals with the problem of "dead neurons". Taking the right decisions is an art more than science, based on long experience of multitudes of code developers and piles of empty pizza boxes!
# 
# Now, having understood the basic principles behind the simplest ANNs inside out, the reader may safely jump to using professional tools of modern machine learning, with the conviction that inside the black boxes there sit essentially the same little codes he met here, but with all the knowledge, tricks, provisions, and options built in. Achieving this conviction, through appreciation of simplicity, has been one of the main goals of this course. 

# ## Acknowledgments

# The author thanks [Jan Broniowski](https://www.linkedin.com/in/janbroniowski) for priceless technical help and for remarks to the text.

# ```{note}
# The references provided in the text as hyperlinks are not repeated in the bibliography list below.
# ```

# ```{bibliography} ../_bibliography/references.bib
# ```
