
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Perceptron &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../_static/koh.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="More layers" href="more_layers.html" />
    <link rel="prev" title="Models of memory" href="memory.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="more_layers.html">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/perceptron.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python//issues/new?title=Issue%20on%20page%20%2Fdocs/perceptron.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/bronwojtek/neuralnets-in-raw-python/edit/master/nn_book/docs/perceptron.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bronwojtek/neuralnets-in-raw-python/master?urlpath=tree/nn_book/docs/perceptron.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-learning">
   Supervised learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#perceptron-as-a-binary-classifier">
   Perceptron as a binary classifier
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sample-with-a-known-classification-rule">
     Sample with a known classification rule
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sample-with-an-unknown-classification-rule">
     Sample with an unknown classification rule
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#perceptron-algorithm">
   Perceptron algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-the-classifier">
     Testing the classifier
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="perceptron">
<span id="perc-lab"></span><h1>Perceptron<a class="headerlink" href="#perceptron" title="Permalink to this headline">¶</a></h1>
<div class="section" id="supervised-learning">
<h2>Supervised learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h2>
<p>We have shown in the previous chapters that even the simplest ANNs can carry out useful tasks (emulate logical networks or provide simple memory models). Generally, each ANN has</p>
<ul class="simple">
<li><p>a certain <strong>architecture</strong>, i.e. the number of layers, number of neurons in each layer, scheme of connections between the neurons (fully connected or not, feed forward, recurrent, …);</p></li>
<li><p><strong>weights (hyperparameters)</strong>, with specific values, defining the network’s functionality.</p></li>
</ul>
<p>The prime practical question is how to set (for a given architecture) the weights such that a requested goal is realized, i.e., a given input yields a desired output.
In the tasks discussed earlier, the weights could be constructed <em>a priori</em>, be it for the logical gates or for the memory models. However, for more involved applications we want to have an “easier” way of determining the weights. Actually, for complicated problems a “theoretical” a priori determination of weights is not possible at all. This is the basic reason for inventing <strong>learning algorithms</strong>, which automatically adjust the weights with the help of the available data.</p>
<p>In this chapter we begin to explore such algorithms with the <strong>supervised learning</strong> approach, used i.a. for data classification.</p>
<div class="important admonition">
<p class="admonition-title">Supervised learning</p>
<p>In this strategy, the data must possess <strong>labels</strong> which a priori determine the correct category for each point. Think for example of pictures of animals (data) and their descriptions (cat, dog,…), which are called the labels.
The labeled data are then split into a <strong>training</strong> sample and a <strong>test</strong> sample.</p>
<p>The basic steps of supervised learning for a given ANN are following:</p>
<ul class="simple">
<li><p>Initialize somehow the weights, for instance randomly or to zero.</p></li>
<li><p>Read subsequently the data points from the training sample and pass them through your ANN. The obtained answer may differ from the correct one, given by the label, in which case the weights are adjusted according to a specific prescription (to be discussed later on).</p></li>
<li><p>Repeat, if needed, the previous step. Typically, the weights are changed less and less as the algorithm proceeds.</p></li>
<li><p>Finish the training when a stopping criterion is reached (weights do not change much any more or the maximum number of iterations has been completed).</p></li>
<li><p>Test the trained ANN on a test sample.</p></li>
</ul>
<p>If satisfied, you have a desired trained ANN performing a specific task (like classification), which can be used on new, unlabeled data. If not, you can split the sample in the training and the test parts in a different way and repeat the procedure from the beginning. Also, you may try to acquire more data (which may be expensive), or change your network’s architecture.</p>
<p>The term “supervised” comes form an interpretation of the procedure where the labels are held by a “teacher”, who thus knows which answers are correct and which are wrong, and who <strong>supervises</strong> that way the training process. Of course, a computer program “supervises itself”.</p>
</div>
</div>
<div class="section" id="perceptron-as-a-binary-classifier">
<h2>Perceptron as a binary classifier<a class="headerlink" href="#perceptron-as-a-binary-classifier" title="Permalink to this headline">¶</a></h2>
<p>The simplest supervised learning algorithm
is the <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a>, invented in 1958 by Frank Rosenblatt. It can be used, i.a., to
construct <strong>binary classifiers</strong> of the data. <em>Binary</em> means that the network
is used to assess if a data item has a particular feature, or not - just two possibilities. Multi-label classification is also possible with ANNs (see exercises), but we do not discuss it in these lectures.</p>
<div class="note admonition">
<p class="admonition-title">Remark</p>
<p>The term <em>perceptron</em> is also used for ANNs (without or with intermediate layers) consisting of the MCP neurons (cf. Fig. <a class="reference internal" href="intro.html#ffnn-fig"><span class="std std-numref">Fig. 3</span></a> and <a class="reference internal" href="mcp.html#mcp1-fig"><span class="std std-numref">Fig. 4</span></a>), on which the perceptron algorithm is executed.</p>
</div>
<div class="section" id="sample-with-a-known-classification-rule">
<h3>Sample with a known classification rule<a class="headerlink" href="#sample-with-a-known-classification-rule" title="Permalink to this headline">¶</a></h3>
<p>To begin, we need some training data, which we will generate as random points in a square. Thus the coordinates of the point, <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>, are taken in the range <span class="math notranslate nohighlight">\([0,1]\)</span>. We define two categories: one for the points lying above the line <span class="math notranslate nohighlight">\(x_1=x_2\)</span> (call them pink), and the other for the points lying below (blue). During the generation, we check whether <span class="math notranslate nohighlight">\(x_2 &gt; x_1\)</span> or not, and assign a <strong>label</strong> to each data point equal to, correspondingly, 1 or 0. These labels are “true” answers.</p>
<p>The function generating the above described data point with a label is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">point</span><span class="p">():</span>     <span class="c1"># generates random coordinates x1, x2, and 1 if x2&gt;x1, 0 otherwise</span>
    <span class="n">x1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>          <span class="c1"># random number from the range [0,1]</span>
    <span class="n">x2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="k">if</span><span class="p">(</span><span class="n">x2</span><span class="o">&gt;</span><span class="n">x1</span><span class="p">):</span>                     <span class="c1"># condition met</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># add label 1</span>
    <span class="k">else</span><span class="p">:</span>                          <span class="c1"># not met</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># add label 0</span>
</pre></div>
</div>
</div>
</div>
<p>We generate a <strong>training sample</strong> of <strong>npo</strong>=300 labeled data points:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">npo</span><span class="o">=</span><span class="mi">300</span> <span class="c1"># number of data points in the training sample</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  x1         x2         label&#39;</span><span class="p">)</span>       <span class="c1"># header</span>
<span class="n">samp</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">point</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npo</span><span class="p">)])</span> <span class="c1"># training sample, _ is dummy iterator</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samp</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>                           <span class="c1"># first 5 data points</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  x1         x2         label
[[0.39421929 0.18790361 0.        ]
 [0.93838154 0.78690602 0.        ]
 [0.56680192 0.46849309 0.        ]
 [0.89119929 0.60750201 0.        ]
 [0.04872589 0.40065797 1.        ]]
</pre></div>
</div>
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">Loops in arrays</p>
<p>In Python, one can conveniently define arrays with a loop inside, e.g.</p>
<p>[i**2 for i in range(4)] yields [1,4,9].</p>
<p>In loops, if the index does not explicitly show in the expression, one can use a dummy index <strong>_</strong>, as for instance in the above code:</p>
<p>[point() for _ in range(npo)]</p>
</div>
<div class="warning admonition">
<p class="admonition-title">Ranges in arrays</p>
<p>Not to print unnecessarily the very long table, we have used above for the first time the <strong>ranges for array indices</strong>. For example, 2:5 means from 2 to 4 (recall the last one is excluded!), :5  - from 0 to 4, 5: - from 5 to the end, and : - all the indices.</p>
</div>
<p>Graphically, our data are shown in the figure below. We also plot the line <span class="math notranslate nohighlight">\(x_2=x_1\)</span>, which separates the blue and purple points. In this case the division is a priori possible (we know the rule) in an exact manner.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>                 
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>                                  <span class="c1"># axes limits</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samp</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">samp</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">samp</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span>       <span class="c1"># label determines the color</span>
            <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">cool</span><span class="p">)</span>                  <span class="c1"># point size and color</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">])</span>                 <span class="c1"># separating line</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>                    
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/perceptron_16_0.png" src="../_images/perceptron_16_0.png" />
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Linearly separable sets</p>
<p>Two sets of points (as here blue and pink) on a plane which are possible to separate with a straight line are called <strong>linearly separable</strong>. In three dimensions, the sets must be separable with a plane, in general in <span class="math notranslate nohighlight">\(n\)</span> dimensions the sets must must be separable with an <span class="math notranslate nohighlight">\(n-1\)</span> dimensional hyperplane.</p>
</div>
<p>Analytically, if the points in the <span class="math notranslate nohighlight">\(n\)</span> dimensional space have coordinates <span class="math notranslate nohighlight">\((x_1,x_2,\dots,x_n)\)</span>, one may chose the parameters <span class="math notranslate nohighlight">\((w_0,w_1,\dots,w_n)\)</span> in such a way that one set of points must satisfy the condition</p>
<div class="math notranslate nohighlight" id="equation-eq-linsep">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-linsep" title="Permalink to this equation">¶</a></span>\[w_0+x_1 w_1+x_2 w_2 + \dots x_n w_n &gt; 0\]</div>
<p>and the other one the opposite condition, with <span class="math notranslate nohighlight">\(&gt;\)</span> replaced with <span class="math notranslate nohighlight">\(\le\)</span>.</p>
<p>Now a crucial, albeit obvious observation: the above inequality is precisely the condition implemented in the <a class="reference internal" href="mcp.html#mcp-lab"><span class="std std-ref">MCP neuron</span></a> (with the step activation function) in the convention of <a class="reference internal" href="mcp.html#mcp2-fig"><span class="std std-numref">Fig. 5</span></a>! We may thus enforce condition <a class="reference internal" href="#equation-eq-linsep">(3)</a> with the <strong>neuron</strong> function from the <strong>neural</strong> library.</p>
<p>In our example, we have for the pink points, by construction,</p>
<div class="math notranslate nohighlight">
\[
x_2&gt;x_1 \to s=-x_1+x_2 &gt;0
\]</div>
<p>from where, using  Eq. <a class="reference internal" href="#equation-eq-linsep">(3)</a>, we can immediately read out</p>
<div class="math notranslate nohighlight">
\[
w_0=0, \;\; w_1=-1, w_2=1.
\]</div>
<p>Thus the <strong>neuron</strong> function is used on a sample point p as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.8</span><span class="p">]</span>      <span class="c1"># sample point with x_2 &gt; x_1</span>
<span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>       <span class="c1"># weights as given above</span>

<span class="n">func</span><span class="o">.</span><span class="n">neuron</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<p>The neuron fired, so point p is pink.</p>
<div class="important admonition">
<p class="admonition-title">Observation</p>
<p>A single MCP neuron with properly chosen weights can be used as a binary classifier for <span class="math notranslate nohighlight">\(n\)</span>-dimensional separable data.</p>
</div>
</div>
<div class="section" id="sample-with-an-unknown-classification-rule">
<h3>Sample with an unknown classification rule<a class="headerlink" href="#sample-with-an-unknown-classification-rule" title="Permalink to this headline">¶</a></h3>
<p>At this point the reader may be a bit misled by the apparent triviality of the results. The confusion may stem from the fact that in the above example we knew from the outset the rule defining the two classes of points (<span class="math notranslate nohighlight">\(x_2&gt;x_1\)</span>, or opposite). However, in a general “real life” situation this is usually not the case! Imagine that we encounter the (labeled) data <strong>samp2</strong> looking like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">samp2</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.34238434 0.06775293 0.        ]
 [0.2771834  0.1100432  0.        ]
 [0.66800363 0.16046044 0.        ]
 [0.1681795  0.41494989 1.        ]
 [0.837124   0.80643535 1.        ]]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/perceptron_27_0.png" src="../_images/perceptron_27_0.png" />
</div>
</div>
<p>The situation is in some sense inverted now. We have obtained from somewhere the (linearly separable) data, and want to find the rule that defines the two classes. In other words, we need to draw a dividing line, which is equivalent to finding the weights of the MCP neuron of <a class="reference internal" href="mcp.html#mcp2-fig"><span class="std std-numref">Fig. 5</span></a> that would carry out the binary classification.</p>
</div>
</div>
<div class="section" id="perceptron-algorithm">
<span id="lab-pa"></span><h2>Perceptron algorithm<a class="headerlink" href="#perceptron-algorithm" title="Permalink to this headline">¶</a></h2>
<p>We could still try to figure out somehow the proper weights for the present example and find the dividing line, for instance with a ruler and pencil, but this is not the point. We wish to have a systematic algorithmic procedure that will effortlessly work for this one and any similar situation. The answer is the already mentioned <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">perceptron algorithm</a>.</p>
<p>Before presenting the algorithm, let us remark that the MCP neuron with some set of weights <span class="math notranslate nohighlight">\(w_0, w_1, w_2\)</span> always yields some answer for a labeled data point, correct or wrong. For example</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>           <span class="c1"># arbitrary choice of weights</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label  answer&quot;</span><span class="p">)</span> <span class="c1"># header</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="c1"># look at first 5 points</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]),</span><span class="s2">&quot;    &quot;</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">neuron</span><span class="p">(</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,:</span><span class="mi">2</span><span class="p">],</span><span class="n">w</span><span class="p">))</span> 
            <span class="c1"># samp2[i,2] is the label, samp2[i,:2] is [x_1,x_2]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>label  answer
0      0
0      0
0      1
1      0
1      1
</pre></div>
</div>
</div>
</div>
<p>We can see that some answers are equal to the corresponding labels (correct), and some are different (wrong). The general idea now is to <strong>use the wrong answers</strong> to adjust cleverly, in small steps, the weights, such that after sufficiently many iterations we get all the answers for the training sample correct!</p>
<div class="important admonition">
<p class="admonition-title">Perceptron algorithm</p>
<p>We iterate over the points of the training data sample.
If for a given point  the obtained result <span class="math notranslate nohighlight">\(y_o\)</span> is equal to the true value <span class="math notranslate nohighlight">\(y_t\)</span> (the label), i.e. the answer is  correct, we do nothing. However, if it is wrong, we change the weights a bit, such that the chance of getting the wrong answer decreases. The explicit recipe is as follows:</p>
<p><span class="math notranslate nohighlight">\(w_i \to w_i  +  \varepsilon  (y_t - y_o)  x_i\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\( \varepsilon \)</span> is a small number (called the <strong>learning speed</strong>) and <span class="math notranslate nohighlight">\(x_i\)</span> are the coordinates of the input point, with <span class="math notranslate nohighlight">\(i=0,\dots,n\)</span>.</p>
</div>
<p>Let us follow how it works. Suppose first that <span class="math notranslate nohighlight">\( x_i&gt; 0\)</span>. Then if the label <span class="math notranslate nohighlight">\( y_t = 1 \)</span> is greater than the obtained answer <span class="math notranslate nohighlight">\( y_o = 0\)</span>, the weight <span class="math notranslate nohighlight">\(w_i\)</span> is increased. Then <span class="math notranslate nohighlight">\( w \cdot x \)</span> also increases and <span class="math notranslate nohighlight">\( y_o = f (w \cdot x) \)</span> is more likely to acquire the correct value of 1 (we remember how the step function <span class="math notranslate nohighlight">\(f\)</span> looks like). If, on the other hand, the label <span class="math notranslate nohighlight">\( y_t = 0 \)</span> is less than the obtained answer <span class="math notranslate nohighlight">\( y_o = 1 \)</span>, then the weight <span class="math notranslate nohighlight">\(w_i\)</span> is decreased, <span class="math notranslate nohighlight">\( w \cdot x \)</span> decreases, and <span class="math notranslate nohighlight">\( y_o = f (w \cdot x) \)</span> has a better chance of achieving the correct value of 0.</p>
<p>If <span class="math notranslate nohighlight">\( x_i &lt; 0 \)</span> it is easy to analogously check that the recipe also works properly.</p>
<p>When the answer is correct, <span class="math notranslate nohighlight">\(y_t=y_0\)</span>, then <span class="math notranslate nohighlight">\( w_i \to w_i\)</span>, so nothing changes. We do not “spoil” the perceptron!</p>
<p>The above formula can be used many times for the same point from the training sample. Next,  we loop over all the points of the sample, and the whole procedure can still be repeated in many rounds to obtain stable weights (not changing any more as we continue the procedure, or changing only slightly).</p>
<p>Typically, in such algorithms the learning speed <span class="math notranslate nohighlight">\( \varepsilon \)</span> is being decreased in successive rounds. This is technically very important, because too large updates could spoil the obtained solution.</p>
<p>The Python implementation of the perceptron algorithm for the 2-dimensional data is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span><span class="o">-</span><span class="mf">0.5</span>  <span class="c1"># initialize weights randomly in the range [-0.5,0.5]</span>
<span class="n">w1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">w2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span><span class="o">-</span><span class="mf">0.5</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">.3</span>                     <span class="c1"># initial  learning speed </span>
   
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>        <span class="c1"># loop over rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.9</span><span class="o">*</span><span class="n">eps</span>            <span class="c1"># in each round decrease the learning speed </span>
        
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npo</span><span class="p">):</span>   <span class="c1"># loop over the points from the data sample</span>
        
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="c1"># repeat 5 times for each points</span>
            
            <span class="n">yo</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">neuron</span><span class="p">(</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,:</span><span class="mi">2</span><span class="p">],[</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">])</span> <span class="c1"># obtained answer</span>
            
            <span class="n">w0</span><span class="o">=</span><span class="n">w0</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span>   <span class="c1"># weight update (the perceptron formula)</span>
            <span class="n">w1</span><span class="o">=</span><span class="n">w1</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">w2</span><span class="o">=</span><span class="n">w2</span><span class="o">+</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">yo</span><span class="p">)</span><span class="o">*</span><span class="n">samp2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Obtained weights:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  w0     w1     w2&quot;</span><span class="p">)</span>        <span class="c1"># header </span>
<span class="n">w_o</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">])</span>           <span class="c1"># obtained weights</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w_o</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>             <span class="c1"># result, rounded to 3 decimal places </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Obtained weights:
  w0     w1     w2
[-0.525 -1.078  2.086]
</pre></div>
</div>
</div>
</div>
<p>The obtained weights, as we know, define the dividing line. Thus, geometrically, the algorithm produces the dividing line as drawn below, together with the training sample as plotted above.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/perceptron_38_0.png" src="../_images/perceptron_38_0.png" />
</div>
</div>
<p>We can see that the algorithm works! All the pink points are above the dividing line, and all the blue ones below. Let us emphasize that the dividing line, given by the equation</p>
<div class="math notranslate nohighlight">
\[ w_0+x_1 w_1 + x_2 w_2=0,\]</div>
<p>does not result from our a priori knowledge, but from the training of the MCP neuron which sets its weights.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One can prove that the perceptron algorithm converges if and only if the data are linearly separable.</p>
</div>
<p>We may now reveal our secret! The data of the training sample <strong>samp2</strong> were labeled at the time of creation with the rule</p>
<div class="math notranslate nohighlight">
\[ x_2&gt;0.25+0.52 x_1, \]</div>
<p>which corresponds to the weights <span class="math notranslate nohighlight">\(w_0^c=0.25\)</span>, <span class="math notranslate nohighlight">\(w_1^c=-0.52\)</span>, <span class="math notranslate nohighlight">\(w_2^c=1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="o">-</span><span class="mf">0.52</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># weights used for labeling the training sample</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w_c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.25 -0.52  1.  ]
</pre></div>
</div>
</div>
</div>
<p>Note that these are not at all the same as the weights obtained from the training:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w_o</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.525 -1.078  2.086]
</pre></div>
</div>
</div>
</div>
<p>The reason is twofold. First, note that the inequality condition <a class="reference internal" href="#equation-eq-linsep">(3)</a> is unchanged if we multiply both sides by a <strong>positive</strong> constant <span class="math notranslate nohighlight">\(c\)</span>. We may therefore scale all the weight by <span class="math notranslate nohighlight">\(c\)</span>, and the situation (the answers of the MCP neuron, the dividing line) remains exactly the same (we encounter here an <strong>equivalence class</strong> of weights scaled with a positive factor).</p>
<p>For that reason, when we divide correspondingly the obtained weights by the weights used to label the sample, we get (almost) constant values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">w_o</span><span class="o">/</span><span class="n">w_c</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2.099 2.073 2.086]
</pre></div>
</div>
</div>
</div>
<p>The reason why the ratio values for <span class="math notranslate nohighlight">\(i=0,1,2\)</span> are not exactly the same is that the sample has a finite number of points (here 300). Thus, there is always some gap between the two classes of points and there is some room for “jiggling” the separating line a bit. With more data points this mismatch effect decreases (see the exercises).</p>
<div class="section" id="testing-the-classifier">
<h3>Testing the classifier<a class="headerlink" href="#testing-the-classifier" title="Permalink to this headline">¶</a></h3>
<p>Due to the limited size of the training sample and the “jiggling” effect desribed above, the classification result on a test sample is sometimes wrong. This always applies to the points near the dividing line, which is determined with accuracy depending on the multiplicity of the training sample. The code below carries out the check on a test sample. The test sample consists of labeled data generated randomly “on the flight” with the same function <strong>point2</strong> used to generate the training data before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">point2</span><span class="p">():</span>
    <span class="n">x1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>          <span class="c1"># random number from the range [0,1]</span>
    <span class="n">x2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="k">if</span><span class="p">(</span><span class="n">x2</span><span class="o">&gt;</span><span class="n">x1</span><span class="o">*</span><span class="mf">0.52</span><span class="o">+</span><span class="mf">0.25</span><span class="p">):</span>           <span class="c1"># condition met</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># add label 1</span>
    <span class="k">else</span><span class="p">:</span>                          <span class="c1"># not met</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># add label 0</span>
</pre></div>
</div>
</div>
</div>
<p>The code for testing is as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>  <span class="c1"># initialize an empty 1 x 3 array to store misclassified points</span>

<span class="n">ner</span><span class="o">=</span><span class="mi">0</span>                 <span class="c1"># initial number of misclassified points</span>
<span class="n">nt</span><span class="o">=</span><span class="mi">10000</span>               <span class="c1"># number of test points</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nt</span><span class="p">):</span>   <span class="c1"># loop over the test points</span>
    <span class="n">ps</span><span class="o">=</span><span class="n">point2</span><span class="p">()</span>       <span class="c1"># a test point </span>
    <span class="k">if</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">neuron</span><span class="p">(</span><span class="n">ps</span><span class="p">[:</span><span class="mi">2</span><span class="p">],[</span><span class="n">w0</span><span class="p">,</span><span class="n">w1</span><span class="p">,</span><span class="n">w2</span><span class="p">])</span><span class="o">!=</span><span class="n">ps</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span> <span class="c1"># if wrong answer                                      </span>
        <span class="n">er</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">er</span><span class="p">,[</span><span class="n">ps</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>           <span class="c1"># add the point to er</span>
        <span class="n">ner</span><span class="o">+=</span><span class="mi">1</span>                                 <span class="c1"># count the number of errors</span>
        
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of misclassified points = &quot;</span><span class="p">,</span><span class="n">ner</span><span class="p">,</span><span class="s2">&quot; per &quot;</span><span class="p">,</span><span class="n">nt</span><span class="p">,</span><span class="s2">&quot; (&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">ner</span><span class="o">/</span><span class="n">nt</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="s2">&quot;% )&quot;</span><span class="p">)</span>        
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>number of misclassified points =  11  per  10000  ( 0.1 % )
</pre></div>
</div>
</div>
</div>
<p>As we can see, a small number of test points are misclassified. All these points lie near the separating line.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/perceptron_53_0.png" src="../_images/perceptron_53_0.png" />
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Misclassification</p>
<p>As it became clear, the reason for misclassification comes from the fact that the training sample does not determine the separating line precisely, but with some uncertainty, as there is a gap between the points of the training sample. For a better result, the training points would have to be “denser” in the vicinity of the separating line, or the training sample would have to be larger.</p>
</div>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="warning admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(~\)</span></p>
<ul class="simple">
<li><p>Play with the lecture code and see how the percentage of misclassified points decreases with the increasing size of the training sample.</p></li>
<li><p>As the perceptron algorithm converges, at some point the weights stop to change. Improve the lecture code by implementing stopping when the weights do not change more than some value when passing to the next round.</p></li>
<li><p>Generalize the above classifier to points in 3-dimensional space.</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="memory.html" title="previous page">Models of memory</a>
    <a class='right-next' id="next-link" href="more_layers.html" title="next page">More layers</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>