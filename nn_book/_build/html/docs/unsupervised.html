
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Unsupervised learning &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Self Organizing Maps" href="som.html" />
    <link rel="prev" title="Rectification" href="rectification.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="more_layers.html">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lib_app.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/unsupervised.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/nn_book/docs/unsupervised.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clusters-of-points">
   Clusters of points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#naive-clusterization">
   Naive clusterization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#clustering-scale">
   Clustering scale
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation-via-steepest-descent">
     Interpretation via steepest descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretation-via-neural-networks">
   Interpretation via neural networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation-with-spherical-coordinates">
     Representation with spherical coordinates
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scalar-product-maximization">
     Scalar product maximization
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="unsupervised-learning">
<span id="un-lab"></span><h1>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Permalink to this headline">¶</a></h1>
<div class="tip admonition">
<p class="admonition-title">Motto</p>
<p><em>teachers! leave those kids alone!</em></p>
<p style="text-align:right;">Pink Floyd, Another Brick In The Wall)</p>
</div>
<p>Supervised learning discussed in previous lectures needs a teacher or a training sample with labels, where we know a priori the characteristics of the data (e.g., in our example, whether the point is inside or outside the circle).</p>
<p>However, this is quite a special situation, because most often the data that we encounter do not have pre-assigned labels and “are what they are”. Also, from the neurobiological or methodological point of view, we learn many facts and activities “on an ongoing basis”, classifying and then recognizing them, whilst the process goes on without any external supervision.</p>
<p>Imagine an alien botanist who enters a meadow and encounters various species of flowers. He has no idea what they are and what to expect at all, as he has no prior knowledge on earthly matters. After finding the first flower, he records its features: color, size, number of petals, etc. He goes on, and finds a different flower, and records its features, and so on with the next flowers. At some point, however, he finds a flower that he already had met. More precisely, its features will be close, though not identical (the size may easily differ somewhat, so the color, etc.), to the previous instance. Hence he concludes it belongs to the same category. The exploration goes on, and new flowers either start a new category, of join one already present. At the end he has a catalog of flowers and now he can assign names (labels) to each species: corn poppy, bluebottle, mullein,…  They are useful in sharing the knowledge with ohers, as they summarize, so to speak, the features of the flower. Note, however, that these labels have actually never been used in the meadow exploration (learning) process.</p>
<p>Formally, this problem of <strong>unsupervised learning</strong> is related to data classification (division into categories, or <strong>clusters</strong>, i.e. subsets of tah sample where the suitably defined distances between individual data are small, smaller than the assumed distances between clusters). Colloquially speaking, we are looking for similarities between individual data points and try to divide the sample into groups of similar objects.</p>
<div class="section" id="clusters-of-points">
<h2>Clusters of points<a class="headerlink" href="#clusters-of-points" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">pA</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.75</span><span class="p">,</span> <span class="mf">.95</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.7</span><span class="p">,</span> <span class="mf">.9</span><span class="p">)]</span> 

<span class="k">def</span> <span class="nf">pB</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.4</span><span class="p">,</span> <span class="mf">.6</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.6</span><span class="p">,</span> <span class="mf">.75</span><span class="p">)]</span> 

<span class="k">def</span> <span class="nf">pC</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.4</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)]</span> 

<span class="k">def</span> <span class="nf">pD</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">.7</span><span class="p">,</span> <span class="mf">.9</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.2</span><span class="p">)]</span> 
</pre></div>
</div>
</div>
</div>
<p>Let us create data samples with a few points from each category:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samA</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pA</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
<span class="n">samB</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pB</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">)])</span>
<span class="n">samC</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pC</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)])</span>
<span class="n">samD</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pD</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>Our data looks like this:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_10_0.png" src="../_images/unsupervised_10_0.png" />
</div>
</div>
<p>If we show the above picture to someone, he will undoubtedly state that the are four clusters. But what algorithm is he using to determine this? We will construct such an algorithm shortly and will be able to carry out clusterization. For the moment let us jump ahead and assume we know the clusters. Clearly, in our example the clusters are well defined, i.e. visibly separated from each other.</p>
<p>One can represent clusters with <strong>representative points</strong> that lie somewher within the cluster. For example, one could take an item belonging to a given cluster as its representative, or in each cluster one can evaluate the mean position of its points and use it as representative points:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rA</span><span class="o">=</span><span class="p">[</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samA</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">rB</span><span class="o">=</span><span class="p">[</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samB</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samB</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">rC</span><span class="o">=</span><span class="p">[</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samC</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samC</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">rD</span><span class="o">=</span><span class="p">[</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samD</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samD</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<p>We add thus defined characteristic points to our graphics. For visual convenience, we assign a color for each category (after having the clusters, we may assign labels, and the color here serves this purpose).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">col</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="s1">&#39;green&#39;</span><span class="p">,</span><span class="s1">&#39;magenta&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Clusters with representative points&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samA</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">samA</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samB</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">samB</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samC</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">samC</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">samD</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">samD</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">rA</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">rA</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">rB</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">rB</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">rC</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">rC</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">rD</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">rD</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_15_0.png" src="../_images/unsupervised_15_0.png" />
</div>
</div>
</div>
<div class="section" id="naive-clusterization">
<h2>Naive clusterization<a class="headerlink" href="#naive-clusterization" title="Permalink to this headline">¶</a></h2>
<p>Now the “real” botanist’s problem:  imagine we have our sample, but we know nothing about how its points were generated (we do not have any labels A, B, C, D, nor colors of the points). Moreover, the data is mixed, i.e., the data points appear in a random order. So we merge our points,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alls</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">samA</span><span class="p">,</span> <span class="n">samB</span><span class="p">,</span> <span class="n">samC</span><span class="p">,</span> <span class="n">samD</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>and shuffle them,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">alls</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The data visualization looks as in the first plot of thsi chapter.</p>
<p>We now want to somehow create representative points, but a priori we don’t know where they should be, or even how many of them there are. Very different strategies are possible here. Their common feature is that the position of the representative points is updated as the sample data is processed.</p>
<p>Let us start with just one representative point, <span class="math notranslate nohighlight">\(\vec{R}\)</span>. Not very ambitious, but in the end we will at least know some mean characteristics of the sample. The initial position  position is <span class="math notranslate nohighlight">\( R=(R_1, R_2) \)</span>, a two dimensional vector in <span class="math notranslate nohighlight">\([0,1]\times [0,1]\)</span>. After reading a data point <span class="math notranslate nohighlight">\(P\)</span> with coordinates <span class="math notranslate nohighlight">\( (x_1 ^ P, x_2 ^ P) \)</span>, <span class="math notranslate nohighlight">\(R\)</span> changes as follows:</p>
<div class="math notranslate nohighlight">
\[ (R_1, R_2) \to (R_1, R_2) + \varepsilon (x_1 ^P-R_1, x_2 ^P-R_2), \]</div>
<p>or in the vector notation</p>
<div class="math notranslate nohighlight">
\[ \vec {R} \to \vec {R} + \varepsilon (\vec {x}^P - \vec {R}). \]</div>
<p>The step is repeated for all points of the sample, and then many such round may be carried out. As in the previous chapters, <span class="math notranslate nohighlight">\( \varepsilon \)</span> is the learning rate that (preferably) decreases
as the algorithm proceeds. The above formula realizes the “snapping” of the point <span class="math notranslate nohighlight">\(\vec{R}\)</span> by the data point <span class="math notranslate nohighlight">\(\vec{P}\)</span>.</p>
<p>The following code implements the above prescription:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial location:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rounds  location&quot;</span><span class="p">)</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>

<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span> <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.85</span><span class="o">*</span><span class="n">eps</span>    <span class="c1"># decrease the update speed </span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">alls</span><span class="p">)</span> <span class="c1"># reshuffle the sample</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alls</span><span class="p">)):</span> <span class="c1"># loop over points of the whole sample</span>
        <span class="n">R</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">R</span><span class="p">)</span> <span class="c1"># update/learning</span>
    <span class="k">if</span> <span class="n">j</span><span class="o">%</span><span class="k">5</span>==4: print(j+1, &quot;    &quot;,np.round(R,3))   
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial location:
[0.005 0.512]
rounds  location
5      [0.553 0.517]
10      [0.637 0.503]
15      [0.612 0.481]
20      [0.602 0.48 ]
25      [0.604 0.481]
30      [0.603 0.482]
35      [0.603 0.482]
40      [0.603 0.482]
45      [0.603 0.482]
50      [0.603 0.482]
</pre></div>
</div>
</div>
</div>
<p>We can see that the position of the characteristic point converges. Actually, it becomes very close to the average of the locatio of all points,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R_mean</span><span class="o">=</span><span class="p">[</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">alls</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span><span class="n">st</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">alls</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">R_mean</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.602 0.481]
</pre></div>
</div>
</div>
</div>
<p>We have decided a priori to have one category, and here is our plot of the result fot the characeristic point, indicated with a gray blob:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_27_0.png" src="../_images/unsupervised_27_0.png" />
</div>
</div>
<p>Let us try to generalize the above algorithm for the case of several (<span class="math notranslate nohighlight">\( n_R&gt; \)</span> 1) representative points.</p>
<ul class="simple">
<li><p>We initialize randomly representative vectors <span class="math notranslate nohighlight">\( \vec{R}^i \)</span>, <span class="math notranslate nohighlight">\(i = 1, \dots, n_R \)</span>.</p></li>
<li><p>Round: We take the sample points P one by one and update only the <strong>closest</strong> point <span class="math notranslate nohighlight">\(R^m\)</span> to the point P in a given step:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \vec{R}^m \to \vec{R}^m + \varepsilon (\vec{x} - \vec{R}^m). \]</div>
<ul class="simple">
<li><p>The position of the other representative points remains the same. This strategy is called the <strong>winner-take-all</strong>.</p></li>
<li><p>We repeat the rounds, reducing the learning speed <span class="math notranslate nohighlight">\( \varepsilon \)</span> each time.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <strong>winner-take-all</strong> strategy is an important concept in ANN modeling. The competing neurons in a layer fight for the signal, and the one that wins, takes it all (its weighs get updated), while the loosers get nothing.</p>
</div>
<p>So let’s consider two representative points that we initialize randomly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">R1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()])</span>
<span class="n">R2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()])</span>
</pre></div>
</div>
</div>
</div>
<p>Then we carry out the above algorithm. For each data point we find the nearest representative point out of the two, and update only this one:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial locations:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">R1</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">R2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rounds  locations&quot;</span><span class="p">)</span>

<span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>

<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span> <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.85</span><span class="o">*</span><span class="n">eps</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">alls</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alls</span><span class="p">)):</span>
        <span class="n">p</span><span class="o">=</span><span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># data point</span>
<span class="c1">#        print(p)</span>
        <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">eucl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">R1</span><span class="p">),</span> <span class="n">func</span><span class="o">.</span><span class="n">eucl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">R2</span><span class="p">)]</span> <span class="c1"># squares of distances</span>
        <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>    <span class="c1"># minimum</span>
        <span class="k">if</span> <span class="n">ind_min</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span> <span class="c1"># if R1 closer to the new data point</span>
            <span class="n">R1</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">R1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>          <span class="c1"># if R2 closer ...</span>
            <span class="n">R2</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">R2</span><span class="p">)</span>            

    <span class="k">if</span> <span class="n">j</span><span class="o">%</span><span class="k">5</span>==4: print(j+1,&quot;    &quot;, np.round(R1,3), np.round(R2,3))  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>initial locations:
[0.905 0.549] [0.419 0.255]
rounds  locations
5      [0.836 0.104] [0.476 0.613]
10      [0.837 0.107] [0.559 0.678]
15      [0.838 0.101] [0.509 0.645]
20      [0.837 0.102] [0.504 0.643]
25      [0.837 0.102] [0.507 0.644]
30      [0.837 0.102] [0.505 0.643]
35      [0.837 0.102] [0.505 0.642]
40      [0.837 0.102] [0.504 0.642]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_34_0.png" src="../_images/unsupervised_34_0.png" />
</div>
</div>
<p>One of the characteristic poits “specializes” in the lower right cluster, and the other in the remaining three.</p>
<p>We continue, anologously, with four representative points.</p>
<div class="figure align-default" id="p-fig">
<a class="reference internal image-reference" href="../_images/cl4_2.jpg"><img alt="../_images/cl4_2.jpg" src="../_images/cl4_2.jpg" style="width: 500px;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Left: proper characteristic points. Right: one “dead body”.</span><a class="headerlink" href="#p-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Running the case for four categories, we notice that it does not always give the correct answer. Quite often one of the representative points is not updated at all and becomes the so-called <strong>dead body</strong>. This is because the other representative points always “win”, i.e. one of them is always closer to each point of the sample than the “corpse”.</p>
<p>When we set up five characteristic points, several situation may occur, as shown in the figure below. Sometimes, depending on the initializtion, a cluster is split into two smaller ones, sometimes dead bodies occur.</p>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="../_images/cl5.jpg"><img alt="../_images/cl5.jpg" src="../_images/cl5.jpg" style="width: 870px;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">From left to right: 5 characteristic points with one cluster split into two, with another cluster split into two, one dead body, and two dead bodies.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>Enforcing more representative points leads to the formation of dead bodies even more often. Of course, we may disregard them, but the example shows that the current startegy is problematic.</p>
</div>
<div class="section" id="clustering-scale">
<h2>Clustering scale<a class="headerlink" href="#clustering-scale" title="Permalink to this headline">¶</a></h2>
<p>In the previous section we were trying to guess from the outset how many clusters there are in the data. This lead to problems, also many times we do not really know how many clusters there are. Actually, up to now we have not even defined what precisely a cluster is, using some intuition only. This intuition told us that the points in the same cluster must be close to one another, or close to a characteristi point, but how close? Actually, the definition must involve a scale telling us “how close is close”. Fo instance, in our example we may take a scale of about 0.2, where there are 4 clusters, but we may take a smaller one and resolve the bigger clusters inti smaller ones, as in the  left panels of <a class="reference internal" href="#id1"><span class="std std-numref">Fig. 12</span></a>.</p>
<div class="note admonition">
<p class="admonition-title">Definition of cluster</p>
<p>A cluster of scale <span class="math notranslate nohighlight">\(d\)</span> associated with a characteristic point <span class="math notranslate nohighlight">\(R\)</span> is the set of data points <span class="math notranslate nohighlight">\(P\)</span>, whose distance from <span class="math notranslate nohighlight">\(R\)</span> is less than <span class="math notranslate nohighlight">\(d\)</span>, whereas the distance from other characteristic points is <span class="math notranslate nohighlight">\(\ge d\)</span>. The characteristic points must be selected in such a way that each data point belongs to a cluster, and no characteristic point is a dead body (i.e., its cluster must contain at least one data point).</p>
</div>
<p>Various strategies can be used to implement this prescription. We use here the <strong>dynamical clusterization</strong>, where a new cluster/representative point is created whenever an encoutered data point is farther than <span class="math notranslate nohighlight">\(d\)</span> from any present characteristic point up to now.</p>
<div class="important admonition">
<p class="admonition-title">Dynamical clusterization</p>
<ol class="simple">
<li><p>Set the clustering scale <span class="math notranslate nohighlight">\(d\)</span> and the learning speed <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Shuffle the sample.</p></li>
<li><p>Read the first data point <span class="math notranslate nohighlight">\(P_1\)</span> and set the first characteristic point <span class="math notranslate nohighlight">\(R^1=P_1\)</span>. Add it to an array <span class="math notranslate nohighlight">\(R\)</span>. Mark <span class="math notranslate nohighlight">\(P_1\)</span> as belonging to cluster <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p>Read the next data points <span class="math notranslate nohighlight">\(P\)</span>. If the distance of <span class="math notranslate nohighlight">\(P\)</span> to the <strong>closest</strong> characteristic point, <span class="math notranslate nohighlight">\(R^m\)</span>, is <span class="math notranslate nohighlight">\(\le d\)</span>, then</p>
<ul class="simple">
<li><p>mark <span class="math notranslate nohighlight">\(P\)</span> as belonging to cluster <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p>move <span class="math notranslate nohighlight">\(R^m\)</span> towards <span class="math notranslate nohighlight">\(P\)</span> with the learning speed <span class="math notranslate nohighlight">\(\varepsilon\)</span>.<br />
Otherwise, add to <span class="math notranslate nohighlight">\(R\)</span> a new characteristic point a location of the point <span class="math notranslate nohighlight">\(P\)</span>.</p></li>
</ul>
</li>
<li><p>Repeat from <span class="math notranslate nohighlight">\(2.\)</span> until all the data points are read.</p></li>
<li><p>Repeat from <span class="math notranslate nohighlight">\(2.\)</span> a number of rounds, decreasing each time <span class="math notranslate nohighlight">\(\varepsilon\)</span>. The result is a division of the sample into a number of clusters, and ghe location of corresponding charactristic points. The result may depend on the reshuffling, hence does not have to the same when the procedure is repeated.</p></li>
</ol>
</div>
<p>A Python implementation finding dynamically the representative points is following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="o">=</span><span class="mf">0.2</span>  <span class="c1"># clustering scale</span>
<span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span> <span class="c1"># initial learning speed</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>               <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.85</span><span class="o">*</span><span class="n">eps</span>                  <span class="c1"># decrease the learning speed </span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">alls</span><span class="p">)</span>       <span class="c1"># shuffle the sample</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>                      <span class="c1"># in the first round</span>
        <span class="n">R</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">alls</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>     <span class="c1"># R - array of representative points</span>
                                  <span class="c1"># initialized to the first data point</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alls</span><span class="p">)):</span>    <span class="c1"># loop over the sample points</span>
        <span class="n">p</span><span class="o">=</span><span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>                 <span class="c1"># new data point</span>
        <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">eucl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">R</span><span class="p">))]</span> 
         <span class="c1"># array of squares of distances of p from the current repr. points in R</span>
        <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="c1"># index of the closest repr. point</span>
        <span class="k">if</span> <span class="n">dist</span><span class="p">[</span><span class="n">ind_min</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">d</span><span class="o">*</span><span class="n">d</span><span class="p">:</span>   <span class="c1"># if its distance square &gt; d*d</span>
                                  <span class="c1"># dynamical creation of a new category</span>
            <span class="n">R</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># add new repr. point to R</span>
        <span class="k">else</span><span class="p">:</span>   
            <span class="n">R</span><span class="p">[</span><span class="n">ind_min</span><span class="p">]</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">R</span><span class="p">[</span><span class="n">ind_min</span><span class="p">])</span> <span class="c1"># otherwise, apdate the &quot;old&quot; repr. point</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of representative points: &quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">R</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of representative points:  4
</pre></div>
</div>
</div>
</div>
<p>The outcome for various values of the clustering scale <span class="math notranslate nohighlight">\(d\)</span> is shown in <a class="reference internal" href="#dyn-fig"><span class="std std-numref">Fig. 13</span></a>. At very low values of <span class="math notranslate nohighlight">\(d\)</span>, smaller than the minimum separation betwenn the points, there are as many clusters as the data points. Then, as we increase <span class="math notranslate nohighlight">\(d\)</span>, the number of clusters decreases. At very large <span class="math notranslate nohighlight">\(d\)</span>, order of the of the span of the sample, there is only one cluster.</p>
<div class="figure align-default" id="dyn-fig">
<a class="reference internal image-reference" href="../_images/cd.jpg"><img alt="../_images/cd.jpg" src="../_images/cd.jpg" style="width: 770px;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Dynamical clustering for various values of the scale <span class="math notranslate nohighlight">\(d\)</span>.</span><a class="headerlink" href="#dyn-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Certainly, an algorithm will not tell us which clustering scale to use. The proper value depends on the nature of the problem. Recall our botanist. If he used a very small <span class="math notranslate nohighlight">\(d\)</span>, he would get as many categories as there are flowers in the meadow, as all flowers, even of the same species, are slightly different from one another. That would be useless. On the other extreme, if his <span class="math notranslate nohighlight">\(d\)</span> is too large, then the classification is too crude. Something in between is just right!</p>
<div class="note admonition">
<p class="admonition-title">Labels</p>
<p>After forming the clusters, we may assign them <strong>labels</strong> for convenience. They are not used in the learning (cluster formation) process.</p>
</div>
<p>Having determined the clusters, we have a <strong>classifier</strong>. We may use it in a two-fold way:</p>
<ul class="simple">
<li><p>continue the dynamical update as new data are encountered, or</p></li>
<li><p>“close” it, and see where the new data falls in.</p></li>
</ul>
<p>In the first case, we assign the corresponding cluster label to the data point (our botanist knows what new flower he found), or initiate a new category if the point does not belong to any of the existing clusters. This is just a continuation of the dynamical algorithm descibed above on the new incoming data</p>
<p>In the latter case (we bought the ready botanist’s catalogue), a data point may</p>
<ul class="simple">
<li><p>belong to a cluster (we know its label),</p></li>
<li><p>fall outside any cluster, then we just do not know what it is, or</p></li>
<li><p>fall into an overlapping region of two or more clusters (cf. <a class="reference internal" href="#dyn-fig"><span class="std std-numref">Fig. 13</span></a>, where we only get “partial” classification.</p></li>
</ul>
<div class="section" id="interpretation-via-steepest-descent">
<h3>Interpretation via steepest descent<a class="headerlink" href="#interpretation-via-steepest-descent" title="Permalink to this headline">¶</a></h3>
<p>Let us denote a given cluster with <span class="math notranslate nohighlight">\(C_i\)</span>, <span class="math notranslate nohighlight">\(i = 1, ..., n\)</span>, where <span class="math notranslate nohighlight">\( n \)</span> is the total number of clusters. The sum of the squared distances of data points in <span class="math notranslate nohighlight">\( C_i \)</span> to its representative point <span class="math notranslate nohighlight">\( R ^ i \)</span> is</p>
<div class="math notranslate nohighlight">
\[
\sum_{P \in C_i} | \vec{R}^i- \vec{x}^P|^2.
\]</div>
<p>Summing up over all clusters, we obtain a function analogous to the previously discussed error function:</p>
<div class="math notranslate nohighlight">
\[E (\{R \}) = \sum_{i = 1}^ n \sum_ {P \in C_i} |\vec{R}^i- \vec{x}^P |^2 .\]</div>
<p>Its derivative with respect to <span class="math notranslate nohighlight">\( \vec{R}_i \)</span> is</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial E (\{R \})}{\partial \vec{R}^i}
= 2 \sum_{P \in C_i} (\vec{R}^i- \vec{x}^P). \]</div>
<p>The steepest descent method results <strong>exactly</strong> in the recipe used in the
dynamic clasterization algorithm presented above, i.e.</p>
<div class="math notranslate nohighlight">
\[ \vec{R} \to \vec{R} - \varepsilon (\vec{R} - \vec {x}^P). \]</div>
<p>To summarize, the algorithm used here actually involves the steepest descent method for the function <span class="math notranslate nohighlight">\( E (\{R \})\)</span>, as discussed in previous lectures.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note, however, that the minimization used in the present algorithms also takes into account different division of points into clusters. In particular, a given data point may change its cluster assignment during the execution of the algorithm. This happens when the closest representative point is changed.</p>
</div>
</div>
</div>
<div class="section" id="interpretation-via-neural-networks">
<span id="inn-sec"></span><h2>Interpretation via neural networks<a class="headerlink" href="#interpretation-via-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>We will now interpret the above unsupervised learning algorithm with the winner-take-all strategy in the neural network language.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_60_0.png" src="../_images/unsupervised_60_0.png" />
</div>
</div>
<p>Our example network has four neurons in the intermediate neuron layer, each corresponding to one characteristic point <span class="math notranslate nohighlight">\(\vec{R}^i\)</span>. The weights are the coordinates of <span class="math notranslate nohighlight">\(\vec{R}^i\)</span>. There is one node in the output layer. We note significant differences from the perceptron discussed earlier.</p>
<ul class="simple">
<li><p>There are no threshold nodes.</p></li>
<li><p>In the intermediate layer, the signal equals the distance squared of the input from the correspoding characteristic point. It is not a weighted sum.</p></li>
<li><p>The node in the last layer (MIN) indicates in which neuron of the intermediate layer the signal is the smallest, i.e., where we have the shortest distance. Hence it works as a control unit selecting the minimum.</p></li>
</ul>
<p>During (unsupervised) learning, an input point “attracts” the closest characteristic point, whose weights are updated.</p>
<p>The application of the above network classifies the point with the coordinates <span class="math notranslate nohighlight">\((x_1, x_2)\)</span>, assigning it the index of the representative point of a given category (here it is the number 1, 2, 3, or 4).</p>
<div class="section" id="representation-with-spherical-coordinates">
<h3>Representation with spherical coordinates<a class="headerlink" href="#representation-with-spherical-coordinates" title="Permalink to this headline">¶</a></h3>
<p>Even with our vast “mathematical freedom”, calling the above system a neural network is quite abusive, as it seems very far away from any neurobiological pattern. In particular, the use of a (non-linear) signal of the form <span class="math notranslate nohighlight">\(\left(\vec{R}^i-\vec{x}\right)^2\)</span> contrasts with the perceptron, where the signal entering the neurons is a (linear) weighted sum of inputs, i.e.</p>
<div class="math notranslate nohighlight">
\[ s ^ i = x_1 w_1 ^ i + x_2 w_2 ^ i + ... + w_1 ^ m x_m = \vec {x} \cdot \vec {w} ^ i. \]</div>
<p>We can alter our problem with a simple geometric construction to make it similar to the perceptron principle. For this purpose we enter a (spurious) third coordinate defined as</p>
<div class="math notranslate nohighlight">
\[ x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2}, \]</div>
<p>where <span class="math notranslate nohighlight">\( r \)</span> is chosen such that for all data points <span class="math notranslate nohighlight">\( r ^ 2 \ge x_1 ^ 2 + x_2 ^ 2 \)</span>.
From the construction <span class="math notranslate nohighlight">\( \vec {x} \cdot \vec {x} = x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 = r ^ 2 \)</span>, so the data points lie on the hemisphere (<span class="math notranslate nohighlight">\( x_3 \ge 0 \)</span>) of radius <span class="math notranslate nohighlight">\( r \)</span>. Similarly, for the representative points we introduce:</p>
<div class="math notranslate nohighlight">
\[ w_1 ^ i = R_1 ^ i,  \; w_2 ^ i = R_2 ^ i,  \; 
w_3 ^ i = \sqrt {r ^ 2-(R_1 ^i)^2 -(R_2 ^i)^2}. \]</div>
<p>It is geometrically obvious that two points in a plane in coordinates 1 and 2 are close to each other if and only if their extensions to the hemisphere are close. We support this statement with a simple calculation:</p>
<p>The dot product of two points <span class="math notranslate nohighlight">\( \vec {x} \)</span> and <span class="math notranslate nohighlight">\( \vec {y} \)</span> on a hemisphere can be written as</p>
<div class="math notranslate nohighlight">
\[ \vec {x} \cdot \vec {y} = x_1 y_1 + x_2 y_2 + \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \sqrt {r ^ 2-y_1 ^ 2-y_2 ^ 2}. \]</div>
<p>For simplicity, let us consider a situation when <span class="math notranslate nohighlight">\( x_1 ^ 2 + x_2 ^ 2 \ll r ^ 2 \)</span> and <span class="math notranslate nohighlight">\( y_1 ^ 2 + y_2 ^ 2 \ll r ^ 2 \)</span>, i.e. both points lie near the pole of the hemisphere. Using your knowledge of mathematical analysis</p>
<div class="math notranslate nohighlight">
\[ \sqrt{r^2-a^2} \simeq r - \frac{a^2}{2r},  \;\;\;a \ll r, \]</div>
<p>hence</p>
<p><span class="math notranslate nohighlight">\(\vec{x} \cdot \vec{y} \simeq x_1 y_1 + x_2 y_2 + \left( r -\frac{x_1^2+x_2^2}{2r} \right) \left( r -\frac{y_1^2+y_2^2}{2r} \right) \\ 
\;\;\;\simeq r^2 - \frac{1}{2} (x_1^2+x_2^2 +y_1^2+y_2^2) + x_1 y_1+x_2 y_2 \\ 
\;\;\; = r^2 - \frac{1}{2}[ (x_1-x_2)^2 +(y_1-y_2)^2]\)</span>.</p>
<p>So it is (for points close to the pole) the constant <span class="math notranslate nohighlight">\( r ^ 2 \)</span> minus half the square of the distance between the points <span class="math notranslate nohighlight">\( (x_1, x_2) \)</span> and <span class="math notranslate nohighlight">\( (y_1, y_2) \)</span> on the plane! It then follows that instead of finding a minimum distance for points on the plane, as in the previous algorithm, we can find a maximum scalar product for their 3D extensions to a hemisphere.</p>
<p>With the extension of the data to a hemisphere, the appropriate neural network can be viewed as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_66_0.png" src="../_images/unsupervised_66_0.png" />
</div>
</div>
<p>Thanks to our efforts, the signal in the intermediate layer is now just a dot product of the input and the weights, as it should be in the artificial neuron. The unit in the last layer (MAX) indicates where the dot product is largest.</p>
<p>This MAX unit is still problematic to interpret within our present framework. Actually, it is possible, but requires going beyond feed-forward networks. When the neurons in the layer can communicate (recurrent <a class="reference external" href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield networks</a>), they can compete, and with proper feed-back it is possible to enforce the winner-take-all mechanism.</p>
<div class="important admonition">
<p class="admonition-title">Hebbian rule</p>
<p>On the conceptul side, we here touch upon a very important and intuitve principle in neural networks, known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Hebbian_theory">Hebbian rule</a>/ Essentially, it applies the truth “What is used, gets stronger” no synaptic connections. A repeated use of a connection makes it stronger.</p>
<p>In our formulation, if a signal passes through a given connection, its weight changes accordingly, and other connections remain the same. The process takes place in an unsupervised manner and its implementation is biologically well motivated.</p>
<p>On the other hand, it is difficult to find a biological justification for backpropagation in supervised learning, where all weights are updated, also in layers very distant from the output. According to many researchers, it is rather a mathematical concept (but nevertheless extremely useful)</p>
</div>
</div>
<div class="section" id="scalar-product-maximization">
<h3>Scalar product maximization<a class="headerlink" href="#scalar-product-maximization" title="Permalink to this headline">¶</a></h3>
<p>The algorithm is now as follows:</p>
<ul class="simple">
<li><p>Extend the points from the sample with the third coordinate, <span class="math notranslate nohighlight">\( x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \)</span>, choosing appropriately large <span class="math notranslate nohighlight">\( r \)</span> so that <span class="math notranslate nohighlight">\( r ^ 2&gt; x_1 ^ 2 + x_2 ^ 2 \)</span> for all sample points.</p></li>
<li><p>Initialize the weights such that <span class="math notranslate nohighlight">\( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \)</span>.</p></li>
</ul>
<p>Then loop over the data points:</p>
<ul class="simple">
<li><p>Find the neuron in the intermediate layer for which the dot product <span class="math notranslate nohighlight">\( x \cdot \vec {w} _i \)</span> is the largest. Change the weights of this neuron according to the recipe</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \vec {w} ^ i \to \vec {w} ^ i + \varepsilon (\vec {x} - \vec {w} ^ i). \]</div>
<ul class="simple">
<li><p>Renormalize the updated weight vector <span class="math notranslate nohighlight">\( \vec {w_i} \)</span> such that <span class="math notranslate nohighlight">\( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \vec {w} ^ i \to \vec {w} ^ i \frac {r} {\sqrt {\vec {w} _i \cdot \vec {w} _i}}. \]</div>
<p>The remaining steps of the algorithm, such as determining the initial positions of the representative points, their dynamic creation as they encounter successive data points, etc., remain as in the previously discussed procedure.</p>
<p>The generalization for <span class="math notranslate nohighlight">\( n \)</span> dimensions is obvious: we enter an additional coordinate</p>
<div class="math notranslate nohighlight">
\[ x_ {n + 1} = \sqrt {r ^ 2 - x_1 ^ 2 -...- x_n ^ 2},\]</div>
<p>hence we have a point on the hyper-hemisphere <span class="math notranslate nohighlight">\( x_1 ^ 2 + \dots + x_n ^ 2 + x_ {n + 1} ^ 2 = r ^ 2 \)</span>,  <span class="math notranslate nohighlight">\(x_ {n + 1} &gt;0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d</span><span class="o">=</span><span class="mf">0.25</span>
<span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>

<span class="n">rad</span><span class="o">=</span><span class="mi">2</span> <span class="c1"># radius r</span>

<span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.85</span><span class="o">*</span><span class="n">eps</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">alls</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">p</span><span class="o">=</span><span class="n">alls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">R</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rad</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)])])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alls</span><span class="p">)):</span>
        <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> 
                    <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">rad</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">alls</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)])</span>
          <span class="c1"># rozszerzenie do półsfery</span>
        <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">R</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">R</span><span class="p">))]</span> 
        <span class="n">ind_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>                    <span class="c1"># maximum</span>
        <span class="k">if</span> <span class="n">dist</span><span class="p">[</span><span class="n">ind_max</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">rad</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">:</span>
             <span class="n">R</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="p">[</span><span class="n">p</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>   
            <span class="n">R</span><span class="p">[</span><span class="n">ind_max</span><span class="p">]</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">R</span><span class="p">[</span><span class="n">ind_max</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of representative points: &quot;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">R</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of representative points:  4
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/unsupervised_72_0.png" src="../_images/unsupervised_72_0.png" />
</div>
</div>
<p>We can see that the dot product maximization algorithm yields an almost exactly the same result as the distance squared minimization (cf. <a class="reference internal" href="#dyn-fig"><span class="std std-numref">Fig. 13</span></a>.</p>
<div class="warning admonition">
<p class="admonition-title">Exercises</p>
<ol class="simple">
<li><p>The city (Manhattan) metric is defined as
<span class="math notranslate nohighlight">\( d (\vec {x}, \vec {y}) = | x_1-y_1 | + | x_2 - y_2 | \)</span> for points <span class="math notranslate nohighlight">\( \vec {x} \)</span> and <span class="math notranslate nohighlight">\( \vec {y} \)</span>.
Repeat the simulations of this lecture with this metric. Draw conclusions.</p></li>
<li><p>Run the classification algorithms for more categories in the data sample (generate your own sample).</p></li>
<li><p>One-dimensional data variant: Consider the problem of clustering points of different grayscale. To do this, enter the grayscale as <span class="math notranslate nohighlight">\( s \)</span> from the <span class="math notranslate nohighlight">\( [0,1] \)</span> range and the character data vector
<span class="math notranslate nohighlight">\( (s, \sqrt {1-s ^ 2}) \)</span> which is normalized to 1 (such points lie on a semicircle). Generate a sample of these points and run dynamical clusterization.</p></li>
</ol>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="rectification.html" title="previous page">Rectification</a>
    <a class='right-next' id="next-link" href="som.html" title="next page">Self Organizing Maps</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>