
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>MCP Neuron &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../_static/koh.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Models of memory" href="memory.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="more_layers.html">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/mcp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python//issues/new?title=Issue%20on%20page%20%2Fdocs/mcp.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/bronwojtek/neuralnets-in-raw-python/edit/master/nn_book/docs/mcp.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bronwojtek/neuralnets-in-raw-python/master?urlpath=tree/nn_book/docs/mcp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definition">
   Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mcp-neuron-in-python">
   MCP neuron in Python
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#boolean-functions">
   Boolean functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-with-xor">
     Problem with XOR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xor-from-composition-of-and-nand-and-or">
     XOR from composition of AND, NAND and OR
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xor-composed-from-nand">
     XOR composed from NAND
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="mcp-neuron">
<span id="mcp-lab"></span><h1>MCP Neuron<a class="headerlink" href="#mcp-neuron" title="Permalink to this headline">¶</a></h1>
<section id="definition">
<h2>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h2>
<p>We need a basic building block of ANNs: the artificial neuron. The first mathematical model dates back to Warren McCulloch and Walter Pitts (MCP)<span id="id1">[<a class="reference internal" href="conclusion.html#id9">MP43</a>]</span>, who proposed it in 1942, hence at the very beginning of the electronic computer age during World War II. The MCP neuron depicted in <a class="reference internal" href="#mcp1-fig"><span class="std std-numref">Fig. 4</span></a> is a basic ingredient of all ANNs discussed in this course. It is built on very simple general rules, inspired neatly by the biological neuron:</p>
<ul class="simple">
<li><p>The signal enters the nucleus via dendrites from other neurons.</p></li>
<li><p>The synaptic connection for each dendrite may have a different (and adjustable) strength (weight).</p></li>
<li><p>In the nucleus, the signal from all the dendrites is combined (summed up) into <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>If the combined signal is stronger than a given threshold, then the neuron fires along the axon, in the opposite case it remains still.</p></li>
<li><p>In the simplest realization, the strength of the fired signal has two possible levels: on or off, i.e. 1 or 0. No intermediate values are needed.</p></li>
<li><p>Axon terminal connects to dendrites of other neurons.</p></li>
</ul>
<figure class="align-default" id="mcp1-fig">
<a class="reference internal image-reference" href="../_images/mcp-1a.png"><img alt="../_images/mcp-1a.png" src="../_images/mcp-1a.png" style="width: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">MCP neuron: <span class="math notranslate nohighlight">\(x_i\)</span> is the input, <span class="math notranslate nohighlight">\(w_i\)</span> are the weights, <span class="math notranslate nohighlight">\(s\)</span> is the signal, <span class="math notranslate nohighlight">\(b\)</span> is the bias, and <span class="math notranslate nohighlight">\(f(s;b)\)</span> represents an activation function, yielding the output <span class="math notranslate nohighlight">\(y=f(s;b)\)</span>. The blue oval encircles the whole neuron, as used e.g. in <a class="reference internal" href="intro.html#ffnn-fig"><span class="std std-numref">Fig. 3</span></a>.</span><a class="headerlink" href="#mcp1-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Translating this into a mathematical prescription, one assigns to the input cells the numbers <span class="math notranslate nohighlight">\(x_1, x_2 \dots, x_n\)</span> (input data point). The strength of the synaptic connections is controlled with the <strong>weights</strong> <span class="math notranslate nohighlight">\(w_i\)</span>. Then the combined signal is defined as the weighted sum</p>
<div class="math notranslate nohighlight">
\[s=\sum_{i=1}^n x_i w_i.\]</div>
<p>The signal becomes an argument of the <strong>activation function</strong>, which, in the simplest case, takes the form of the step function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(s;b) = \left \{ \begin{array}{l} 1 {\rm ~for~} s \ge b \\ 0 {\rm ~for~} s &lt; b \end{array} \right .
\end{split}\]</div>
<p>When the combined signal <span class="math notranslate nohighlight">\(s\)</span> is larger than the bias (threshold) <span class="math notranslate nohighlight">\(b\)</span>, the nucleus fires. i.e. the signal passed along the axon is 1. in the opposite case, the generated signal value is 0 (no firing). This is precisely what we need to mimic the biological prototype.</p>
<p>There is a convenient notational convention which is frequently used. Instead of splitting the bias from the input data, we may treat all uniformly. The condition for firing may be trivially transformed as</p>
<div class="math notranslate nohighlight">
\[
s \ge b  \to s-b \ge 0 \to \sum_{i=1}^n x_i w_i - b \ge 0 \to \sum_{i=1}^n x_i w_i +x_0 w_0 \ge 0 
\to \sum_{i=0}^n x_i w_i \ge 0,
\]</div>
<p>where <span class="math notranslate nohighlight">\(x_0=1\)</span> and <span class="math notranslate nohighlight">\(w_0=-b\)</span>. In other words, we may treat the bias as a weight on the edge connected to an additional cell with the input always fixed to 1. This notation is shown in <a class="reference internal" href="#mcp2-fig"><span class="std std-numref">Fig. 5</span></a>. Now, the activation function is simply</p>
<div class="math notranslate nohighlight" id="equation-eq-f">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-f" title="Permalink to this equation">¶</a></span>\[\begin{split}f(s) = \left \{ \begin{array}{l} 1 {\rm ~for~} s \ge 0 \\ 0 {\rm ~for~} s &lt; 0 \end{array} \right .,\end{split}\]</div>
<p>with the summation index in <span class="math notranslate nohighlight">\(s\)</span> starting from <span class="math notranslate nohighlight">\(0\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-f0">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-f0" title="Permalink to this equation">¶</a></span>\[s=\sum_{i=0}^n x_i w_i = x_0 w_0+x_1 w_1 + \dots + x_n w_n.\]</div>
<figure class="align-default" id="mcp2-fig">
<a class="reference internal image-reference" href="../_images/mcp-2a.png"><img alt="../_images/mcp-2a.png" src="../_images/mcp-2a.png" style="width: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Alternative, more uniform representation of the MCP neuron, with <span class="math notranslate nohighlight">\(x_0=1\)</span> and <span class="math notranslate nohighlight">\(w_0=-b\)</span>.</span><a class="headerlink" href="#mcp2-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition-hyperparameters admonition">
<p class="admonition-title">Hyperparameters</p>
<p>The weights <span class="math notranslate nohighlight">\(w_0=-b,w_1,\dots,w_n\)</span> are generally referred to as <strong>hyperparameters</strong>. They determine the functionality of the MCP neuron and may be changed during the learning (training) process (see the following). However, they are kept fixed when using the trained neuron on a particular input data sample.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>An essential property of neurons in ANNs is <strong>nonlinearity</strong> of the activation function. Without this feature, the MCP neuron would simply represent a scalar product, and the feed-forward networks would just involve trivial matrix multiplications.</p>
</div>
</section>
<section id="mcp-neuron-in-python">
<span id="mcp-p-lab"></span><h2>MCP neuron in Python<a class="headerlink" href="#mcp-neuron-in-python" title="Permalink to this headline">¶</a></h2>
<p>We now implement the mathematical model of the neuron of Sec. <a class="reference internal" href="#mcp-lab"><span class="std std-ref">MCP Neuron</span></a> in Python. First, we obviously need arrays (vectors), which are represented as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">2.5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>and (<strong>important</strong>) are indexed starting from 0, e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<p>(note that typing a variable at the end of a notebook cell prints out its content). The numpy library functions carry the prefix <strong>np</strong>, which is the alias given at import. Note that these functions act <em>distributively</em> over arrays, e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.84147098, 0.14112001, 0.6569866 ])
</pre></div>
</div>
</div>
</div>
<p>which is a very convenient feature when programming. We also have the scalar product <span class="math notranslate nohighlight">\(x \cdot w = \sum_i x_i w_i\)</span> handy, which we use to build the combined signal <span class="math notranslate nohighlight">\(s\)</span> entering the MCP neuron:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21.5
</pre></div>
</div>
</div>
</div>
<p>Next, we need to construct the neuron activation function, which presently is just the step function <a class="reference internal" href="#equation-eq-f">(1)</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>       <span class="c1"># step function (also in neural library package)</span>
     <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>     <span class="c1"># condition satisfied</span>
        <span class="k">return</span> <span class="mi">1</span>
     <span class="k">else</span><span class="p">:</span>         <span class="c1"># otherwise</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<p>where in the top comments we have indicated that the function is also defined in the <strong>neural</strong> library package, cf. <a class="reference internal" href="appendix.html#app-lab"><span class="std std-ref">Appendix</span></a>. For the visualizers, the plot of the step function is following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span> <span class="c1"># set the size and resolution of the figure</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>   <span class="c1"># array of 100+1 equally spaced points in [-2, 2]</span>
<span class="n">fs</span> <span class="o">=</span> <span class="p">[</span><span class="n">step</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>     <span class="c1"># corresponding array of function values</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;signal s&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>      <span class="c1"># axes labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;response f(s)&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;step function&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>  <span class="c1"># plot title</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">fs</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/mcp_24_0.png" src="../_images/mcp_24_0.png" />
</div>
</div>
<p>Since <span class="math notranslate nohighlight">\(x_0=1\)</span> always, we do not want to explicitly carry this over in the arguments of functions that will follow. We will be frequently inserting <span class="math notranslate nohighlight">\(x_0=1\)</span> into the input, for instance:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span>
<span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># insert 1 in x at position 0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 5, 7])
</pre></div>
</div>
</div>
</div>
<p>Now we are ready to construct the <a class="reference internal" href="#mcp1-fig"><span class="std std-ref">MCP neuron</span></a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neuron</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">f</span><span class="o">=</span><span class="n">step</span><span class="p">):</span> <span class="c1"># (in the neural library)</span>
    <span class="sd">&quot;&quot;&quot;                 </span>
<span class="sd">    MCP neuron</span>

<span class="sd">    x: array of inputs  [x1, x2,...,xn]</span>
<span class="sd">    w: array of weights [w0, w1, w2,...,wn]</span>
<span class="sd">    f: activation function, with step as default</span>
<span class="sd">    </span>
<span class="sd">    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w</span>
<span class="sd">    &quot;&quot;&quot;</span> 
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">w</span><span class="p">))</span> <span class="c1"># insert x0=1 into x, output f(x.w)</span>
</pre></div>
</div>
</div>
</div>
<p>We diligently put the comments in triple quotes to be able to get a useful help when needed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">neuron</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Help on function neuron in module __main__:

neuron(x, w, f=&lt;function step at 0x7fb295f785f0&gt;)
    MCP neuron
    
    x: array of inputs  [x1, x2,...,xn]
    w: array of weights [w0, w1, w2,...,wn]
    f: activation function, with step as default
    
    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w
</pre></div>
</div>
</div>
</div>
<p>Note that function <strong>f</strong> is an argument of <strong>neuron</strong>. It is by default set to <strong>step</strong>, thus does not have to be present in the argument list. A sample usage with <span class="math notranslate nohighlight">\(x_1=3\)</span>, <span class="math notranslate nohighlight">\(w_0=-b=-2\)</span>, <span class="math notranslate nohighlight">\(w_1=1\)</span> is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neuron</span><span class="p">([</span><span class="mi">3</span><span class="p">],[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<p>As we can see, the neuron fired in this case, because <span class="math notranslate nohighlight">\(s=1*(-2)+3*1&gt;0\)</span>.</p>
<p>Next, we show how the neuron operates on an input sample <span class="math notranslate nohighlight">\(x_1\)</span> taken in the range <span class="math notranslate nohighlight">\([-2,2]\)</span>. We also change the bias parameter, to illustrate its role. It is clear that the bias works as the threshold: if the signal <span class="math notranslate nohighlight">\(x_1 w_1\)</span> is above <span class="math notranslate nohighlight">\(b=-x_0\)</span>, then the neuron fires.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span> 

<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">fs1</span> <span class="o">=</span> <span class="p">[</span><span class="n">neuron</span><span class="p">([</span><span class="n">x1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x1</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>      <span class="c1"># more function on one plot</span>
<span class="n">fs0</span> <span class="o">=</span> <span class="p">[</span><span class="n">neuron</span><span class="p">([</span><span class="n">x1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x1</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="n">fsm12</span> <span class="o">=</span> <span class="p">[</span><span class="n">neuron</span><span class="p">([</span><span class="n">x1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x1</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;response&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Change of bias&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">fs1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b=-1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">fs0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b=0&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">fsm12</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;b=1/2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>                               <span class="c1"># legend</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/mcp_34_0.png" src="../_images/mcp_34_0.png" />
</div>
</div>
<p>When the sign of the weight <span class="math notranslate nohighlight">\(w_1\)</span> is negative, we get a <strong>reverse</strong> behavior, where the neuron fires when <span class="math notranslate nohighlight">\(x_1 |w_1| &lt; w_0\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/mcp_36_0.png" src="../_images/mcp_36_0.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>From now on, for the brevity of presentation, we hide some cells of the code with repeated structure. The reader may find the complete code in the corresponding Jupyter notebooks.</p>
</div>
<p>Admittedly, in the last example one departs from the biological pattern, as negative weights are not possible to realize in a biological neuron. However, this freedom enriches the mathematical model, which clearly can be built without biological constraints.</p>
</section>
<section id="boolean-functions">
<span id="bool-sec"></span><h2>Boolean functions<a class="headerlink" href="#boolean-functions" title="Permalink to this headline">¶</a></h2>
<p>Having constructed the MCP neuron in Python, the question is: <em>What is the simplest (but still non-trivial) application we can use it for?</em> We show here that one can easily construct <a class="reference external" href="https://en.wikipedia.org/wiki/Boolean_function">boolean functions</a>, or logical networks, with the help of networks of MCP neurons. Boolean functions, by definition, have arguments and values in the set <span class="math notranslate nohighlight">\(\{ 0,1 \}\)</span>, or {True, False}.</p>
<p>To warm up, let us start with some guesswork, where we take the neuron with the weights <span class="math notranslate nohighlight">\(w=[w_0,w_1,w_2]=[-1,0.6,0.6]\)</span> (why not). We shall here denote <span class="math notranslate nohighlight">\(x_1=p\)</span>, <span class="math notranslate nohighlight">\(x_2=q\)</span>, in accordance with the traditional notation for logical variables, where <span class="math notranslate nohighlight">\(p,q \in \{0,1\}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p q n(p,q)&quot;</span><span class="p">)</span> <span class="c1"># print the header</span>
<span class="nb">print</span><span class="p">()</span>             <span class="c1"># print space</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>       <span class="c1"># loop over p</span>
    <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span>   <span class="c1"># loop over q</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="n">neuron</span><span class="p">([</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mf">.6</span><span class="p">,</span><span class="mf">.6</span><span class="p">]))</span> <span class="c1"># print all cases</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p q n(p,q)

0 0  0
0 1  0
1 0  0
1 1  1
</pre></div>
</div>
</div>
</div>
<p>We immediately recognize in the above output the logical table for the conjunction, <span class="math notranslate nohighlight">\(n(p,q)=p \land q\)</span>, or the logical <strong>AND</strong> operation. It is clear how the neuron works. The condition for the firing <span class="math notranslate nohighlight">\(n(p,q)=1\)</span> is <span class="math notranslate nohighlight">\(-1+p*0.6+q*0.6 \ge 0\)</span>, and it is satisfied if and only if <span class="math notranslate nohighlight">\(p=q=1\)</span>, which is the definition of the logical conjunction. Of course, we could have used here 0.7 instead of 0.6, or in general <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span> such that <span class="math notranslate nohighlight">\(w_1&lt;1, w_2&lt;1, w_1+w_2 \ge 1\)</span>. In the electronics terminology, we can call the present neuron the <strong>AND gate</strong>.</p>
<p>We can thus define the short-hand</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neurAND</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">):</span> <span class="k">return</span> <span class="n">neuron</span><span class="p">([</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mf">.6</span><span class="p">,</span><span class="mf">.6</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Quite similarly, we may define other boolean functions (or logical gates) of two logical variables. In particular, the NAND gate (the negation of conjunction) and the OR gate (alternative) are realized with the following MCP neurons:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neurNAND</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">):</span> <span class="k">return</span> <span class="n">neuron</span><span class="p">([</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.6</span><span class="p">,</span><span class="o">-</span><span class="mf">0.6</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">neurOR</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">):</span>   <span class="k">return</span> <span class="n">neuron</span><span class="p">([</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.2</span><span class="p">,</span><span class="mf">1.2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>They correspond to the logical tables</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p q  NAND OR&quot;</span><span class="p">)</span> <span class="c1"># print the header</span>
<span class="nb">print</span><span class="p">()</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span> 
    <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="s2">&quot; &quot;</span><span class="p">,</span><span class="n">neurNAND</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">),</span><span class="s2">&quot; &quot;</span><span class="p">,</span><span class="n">neurOR</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p q  NAND OR

0 0   1   0
0 1   1   1
1 0   1   1
1 1   0   1
</pre></div>
</div>
</div>
</div>
<section id="problem-with-xor">
<h3>Problem with XOR<a class="headerlink" href="#problem-with-xor" title="Permalink to this headline">¶</a></h3>
<p>The XOR gate, or the <strong>exclusive alternative</strong>, is defined with the following logical table:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ccc}
p &amp; q &amp; p \oplus q \\
0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{array}
\end{split}\]</div>
<p>This is one of possible boolean functions of two arguments (in total, we have 16 different functions of this kind, why?). We could now try very hard to adjust the weights in our neuron to make it behave as the XOR gate, but we are doomed to fail. Here is the reason:</p>
<p>From the first row of the above table it follows that for the input 0, 0 (first row) the neuron should not fire. Hence</p>
<p><span class="math notranslate nohighlight">\(w_0  + 0* w_1 + 0*w_2  &lt; 0\)</span>, or <span class="math notranslate nohighlight">\(-w_0&gt;0\)</span>.</p>
<p>For the cases of rows 2 and 3 the neuron must fire, therefore</p>
<p><span class="math notranslate nohighlight">\(w_0+w_2 \ge 0\)</span> and <span class="math notranslate nohighlight">\(w_0+w_1 \ge 0\)</span>.</p>
<p>Adding side-by-side the three obtained inequalities we get <span class="math notranslate nohighlight">\(w_0+w_1+w_2 &gt; 0\)</span>. However, the fourth row yields
<span class="math notranslate nohighlight">\(w_0+w_1+w_2&lt;0\)</span> (no firing), so we encounter a contradiction. Therefore no choice of <span class="math notranslate nohighlight">\(w_0, w_1, w_2\)</span> exists to do the job!</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A single MCP neuron cannot represent the <strong>XOR</strong> gate.</p>
</div>
</section>
<section id="xor-from-composition-of-and-nand-and-or">
<h3>XOR from composition of AND, NAND and OR<a class="headerlink" href="#xor-from-composition-of-and-nand-and-or" title="Permalink to this headline">¶</a></h3>
<p>One can solve the XOR problem by composing three MCP neurons, for instance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neurXOR</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">):</span> <span class="k">return</span> <span class="n">neurAND</span><span class="p">(</span><span class="n">neurNAND</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">),</span><span class="n">neurOR</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p q XOR&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">()</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span> 
    <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="n">neurXOR</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
</pre></div>
</div>
</div>
</div>
<p>The above construction corresponds to the simple network of <a class="reference internal" href="#xor-fig"><span class="std std-numref">Fig. 6</span></a>.</p>
<figure class="align-default" id="xor-fig">
<a class="reference internal image-reference" href="../_images/xor.png"><img alt="../_images/xor.png" src="../_images/xor.png" style="width: 260px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">The XOR gate composed of the NAND, OR, and AND MCP neurons.</span><a class="headerlink" href="#xor-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Note that we are dealing here, for the first time, with a network having an intermediate layer, consisting of the NAND and OR neurons. This layer is indispensable to construct the XOR gate.</p>
</section>
<section id="xor-composed-from-nand">
<h3>XOR composed from NAND<a class="headerlink" href="#xor-composed-from-nand" title="Permalink to this headline">¶</a></h3>
<p>Within the theory of logical networks, one proves that any network (or any boolean function) can be composed of only NAND gates, or only the NOR gates. One says that the NAND (or NOR) gates are <strong>complete</strong>. In particular, the XOR gate can be constructed as</p>
<p>[ p NAND ( p NAND q ) ] NAND [ q NAND ( p NAND q ) ],</p>
<p>which we can write in Python as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">nXOR</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">):</span> <span class="k">return</span> <span class="n">neurNAND</span><span class="p">(</span><span class="n">neurNAND</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">neurNAND</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)),</span><span class="n">neurNAND</span><span class="p">(</span><span class="n">j</span><span class="p">,</span><span class="n">neurNAND</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;p q XOR&quot;</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span> 
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]:</span> 
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="s2">&quot;&quot;</span><span class="p">,</span><span class="n">nXOR</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">))</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One proves that logical networks are complete in the <a class="reference external" href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis">Church-Turing</a> sense, i.e., (when sufficiently large) may carry over any possible calculation. This feature directly carries over to ANNs. Historically, that was the basic finding of the seminal MCP paper <span id="id2">[<a class="reference internal" href="conclusion.html#id9">MP43</a>]</span>.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Conclusion</p>
<p>ANNs (sufficiently large) can perform any calculation!</p>
</div>
</section>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="warning admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(~\)</span></p>
<p>Construct (all in Python)</p>
<ul class="simple">
<li><p>a gate realizing conjunction of multiple boolean variables;</p></li>
<li><p>gates NOT, NOR;</p></li>
<li><p>gates OR, AND, NOT by <a class="reference external" href="https://en.wikipedia.org/wiki/NAND_logic">composing NAND gates</a>;</p></li>
<li><p>the <a class="reference external" href="https://en.wikipedia.org/wiki/Adder_(electronics)">half adder and full adder</a>,</p></li>
</ul>
<p>as networks of MCP neurons.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="memory.html" title="next page">Models of memory</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>