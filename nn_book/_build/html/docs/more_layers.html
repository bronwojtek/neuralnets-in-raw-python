
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>More layers &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Back propagation" href="backprop.html" />
    <link rel="prev" title="Perceptron" href="perceptron.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hopfield.html">
   Hopfield networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lib_app.html">
   Appendix
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="refs.html">
   Literature
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/more_layers.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/nn_book/docs/more_layers.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-layers-of-neurons">
   Two layers of neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#three-or-more-layers-of-neurons">
   Three or more layers of neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feeding-forward-in-python">
   Feeding forward in Python
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualization">
   Visualization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classifier-with-three-neuron-layers">
   Classifier with three neuron layers
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="more-layers">
<span id="more-lab"></span><h1>More layers<a class="headerlink" href="#more-layers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="two-layers-of-neurons">
<h2>Two layers of neurons<a class="headerlink" href="#two-layers-of-neurons" title="Permalink to this headline">¶</a></h2>
<p>In the previous chapter we have seen that the MCP neuron with the step activation fuction realizes the inequality <span class="math notranslate nohighlight">\(x \cdot w=w_0+x_1 w_1 + \dots x_n w_n &gt; 0\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> in the dimensionality of the input space. It is instructive to come up here with a geometric interpretation. Taking for definiteness <span class="math notranslate nohighlight">\(n=2\)</span> (the plane), the above inequality corresponds to a division into two half-planes. The line given by the equation</p>
<div class="math notranslate nohighlight">
\[x \cdot w=w_0+x_1 w_1 + \dots x_n w_n = 0\]</div>
<p>is the <strong>dividing line</strong>.</p>
<p>Imagine now that we have more such conditions: two, three, etc., in general <span class="math notranslate nohighlight">\(k\)</span> independent conditions. Taking a conjunction of these conditions, we can build regions as shown, e.g., in <a class="reference internal" href="#regions-fig"><span class="std std-numref">Fig. 8</span></a>.</p>
<div class="figure align-default" id="regions-fig">
<a class="reference internal image-reference" href="../_images/regions.png"><img alt="../_images/regions.png" src="../_images/regions.png" style="width: 620px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Sample convex regions in the plane obtained, from left to right, with one inequality condition, and a conjunction of 2, 3, or 4 inequality conditions, yielding <strong>polygons</strong>.</span><a class="headerlink" href="#regions-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-convex-region admonition">
<p class="admonition-title">Convex region</p>
<p>By definition, region <span class="math notranslate nohighlight">\(A\)</span> is convex if and only if a straight line between any two points in <span class="math notranslate nohighlight">\(A\)</span> is contained in <span class="math notranslate nohighlight">\(A\)</span>. A region which is not convex is called <strong>concave</strong>.</p>
</div>
<p>Clearly, <span class="math notranslate nohighlight">\(k\)</span> inequality conditions can be imposed with <span class="math notranslate nohighlight">\(k\)</span> MCP neurons.
Recall from section <a class="reference internal" href="mcp.html#bool-sec"><span class="std std-ref">Boolean functions</span></a> that we can straightforwardly build boolean functions with the help of the neural networks. In particular, we can make a conjunction of <span class="math notranslate nohighlight">\(k\)</span> conditions by taking a neuron with the weights <span class="math notranslate nohighlight">\(w_0=-1\)</span> and <span class="math notranslate nohighlight">\(1/k &lt; w_i &lt; 1/(k-1)\)</span>, where <span class="math notranslate nohighlight">\(i=1,\dots,k\)</span>. One possibility is, e.g.,</p>
<div class="math notranslate nohighlight">
\[w_i=\frac{1}{k-\frac{1}{2}}.\]</div>
<p>Indeed, let <span class="math notranslate nohighlight">\(p_0=0\)</span>, and the conditions imposed by the inequalities be denoted as <span class="math notranslate nohighlight">\(p_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,k\)</span>, which may take values 1 or 0 (true or false). Then</p>
<div class="math notranslate nohighlight">
\[p \cdot w =-1 + p_1 w_1 + \dots + p_k w_k = -1+\frac{p_1+\dots p_k}{k-\frac{1}{2}} &gt; 0\]</div>
<p>if and only if all <span class="math notranslate nohighlight">\(p_i=1\)</span>, i.e. all the conditions are true. This builds th AND gate for the incoming <span class="math notranslate nohighlight">\(k\)</span> boolean signals.</p>
<p>The architecture of networks for <span class="math notranslate nohighlight">\(k=1\)</span>, 2, 3, or 4 conditions is shown in <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 9</span></a>. Starting from the second panel, we have networks with two layers of neurons, with <span class="math notranslate nohighlight">\(k\)</span> neurons in the intermediate layer, providing the inequality conditions, and one neuron in the output layer, acting as the AND gate. Of course, for one condition it is sufficient to have a single neuron, as in the left panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 9</span></a>.</p>
<div class="figure align-default" id="nfn-fig">
<a class="reference internal image-reference" href="../_images/nf1-4.png"><img alt="../_images/nf1-4.png" src="../_images/nf1-4.png" style="width: 820px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Networks capable of classifying data in the corresponding regions of <a class="reference internal" href="#regions-fig"><span class="std std-numref">Fig. 8</span></a>.</span><a class="headerlink" href="#nfn-fig" title="Permalink to this image">¶</a></p>
</div>
<p>With the geometric interpretation, the first neuron layer represents the <span class="math notranslate nohighlight">\(k\)</span> half-planes, and the neuron in the second layer correspond to a convex region with <span class="math notranslate nohighlight">\(k\)</span> sides.</p>
<p>The situation generalizes in an obvious way to data in more dimensions. In that case we have more black dots in the inputs in  <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 9</span></a>. Geometrically, non <span class="math notranslate nohighlight">\(n=3\)</span> we deal with dividing planes and convex <a class="reference external" href="https://en.wikipedia.org/wiki/Polyhedron">polyhedrons</a>, and for <span class="math notranslate nohighlight">\(n&gt;3\)</span> with dividing <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperplane">hyperplanes</a> and convex <a class="reference external" href="https://en.wikipedia.org/wiki/Polytope">polytopes</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If there are numerous neurons <span class="math notranslate nohighlight">\(k\)</span> in the intermediate layer, the resulting polygon has many sides which may approximate a smooth boundary, such as an arc. The approximation is better and better as <span class="math notranslate nohighlight">\(k\)</span> increases.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A percepton with two-layers of neurons can classify points belonging to a convex region in <span class="math notranslate nohighlight">\(n\)</span>-dimensional space.</p>
</div>
</div>
<div class="section" id="three-or-more-layers-of-neurons">
<h2>Three or more layers of neurons<a class="headerlink" href="#three-or-more-layers-of-neurons" title="Permalink to this headline">¶</a></h2>
<p>We have just shown that a two-layer network may classify a convex polygon. Imagine now that we produce two such figures in the second layer of neurons, for instane as in the following network:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/more_layers_13_0.png" src="../_images/more_layers_13_0.png" />
</div>
</div>
<p>Note that the first and second neuron layers are not fully connected here, as we “stack on top of each other” two networks producing triangles, as in the third panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 9</span></a>. Next, in the third neuron layer (here having a single neuron) we implement a <span class="math notranslate nohighlight">\(p \,\wedge \!\sim\!q\)</span> gate, i.e. the conjunction of the conditions that the points belong to one triangle and do not belong to the other one. As we will show shortly, with appropriate weights, the above network may produce a concave region, for example a triangle with a triangular hollow:</p>
<div class="figure align-default" id="tri-fig">
<a class="reference internal image-reference" href="../_images/tritri.png"><img alt="../_images/tritri.png" src="../_images/tritri.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Triangle with a tringular hollow.</span><a class="headerlink" href="#tri-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Generalizing this argument to other shapes, one can show an important theorem that</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A perceptron with three or more layers with sufficiently many neurons can classify points belonging to <strong>any</strong> region in <span class="math notranslate nohighlight">\(n\)</span>-dimensional space with <span class="math notranslate nohighlight">\(n-1\)</span>-dimensional hyperplane boundaries.</p>
</div>
<p>It is worth stressing here that three layers provide full functionality! Adding more layers to a classifier does not increase its capabilities.</p>
</div>
<div class="section" id="feeding-forward-in-python">
<h2>Feeding forward in Python<a class="headerlink" href="#feeding-forward-in-python" title="Permalink to this headline">¶</a></h2>
<p>Before proceeding with an explicit example, we need a Python code for propagation of the signal in a general fully-connected feed-forward network. First, we represent the architecture of a network with <span class="math notranslate nohighlight">\(l\)</span> neuron layers as an array of the form</p>
<div class="math notranslate nohighlight">
\[[n_0,n_1,n_2,...,n_l],\]</div>
<p>where <span class="math notranslate nohighlight">\(n_0\)</span> in the number of the input nodes, and <span class="math notranslate nohighlight">\(n_i\)</span> are the numbers of neurons in layers <span class="math notranslate nohighlight">\(i=1,\dots,l\)</span>. For instance, the architecture of the network from the fourth panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 9</span></a> is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arch</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> 
<span class="n">arch</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 4, 1]
</pre></div>
</div>
</div>
</div>
<p>In the codes of this course we use the convention of <a class="reference internal" href="mcp.html#mcp2-fig"><span class="std std-numref">Fig. 6</span></a>, namely, the bias is treated uniformly with the remaining signal. However, the bias notes are not included in the numbers <span class="math notranslate nohighlight">\(n_i\)</span> defined above. In particular, a more detailed view of the fourth panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 9</span></a> is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net</span><span class="p">(</span><span class="n">arch</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_22_0.png" src="../_images/more_layers_22_0.png" />
</div>
</div>
<p>Here, black dots mean input, gray dots indicate the bias nodes carrying input =1, and the blue blobs are the neurons.</p>
<p>Next, we need the weights of the connections. There are <span class="math notranslate nohighlight">\(l\)</span> sets of weights, each one corresponding to the set of edges entering a given neuron layer from the left.
In the above example, the first neuron layer (blue blobs to the left) has weights which form a <span class="math notranslate nohighlight">\(3 \times 4\)</span> matrix. Here 3 is the number of nodes in the preceding layer (including the bias node) and 4 is the number of neurons in the first neuron layer. Similarly, the weights associated with the second (output) layer form a <span class="math notranslate nohighlight">\(4 \times 1\)</span> matrix. Hence, in our convention, the weight matrices corresponding to subsequent neuron layers <span class="math notranslate nohighlight">\(1, 2, \dots, l\)</span> have dimensions</p>
<div class="math notranslate nohighlight">
\[
(n_0+1)\times n_1, \; (n_1+1)\times n_2, \; \dots \; (n_{l-1}+1)\times n_l.
\]</div>
<p>To store all the weights of a network we actually need <strong>three</strong> indices: one for the layer, one for the number of nodes in the preceding layer, and one for the number of nodes in the given layer. We could have used a three-dimensional array here, but since we number the neuron layers staring from 1, and arrays start numbering from 0, it is more convenient to use the Python <strong>dictionary</strong> structure. We will then store the weights as</p>
<div class="math notranslate nohighlight">
\[w=\{1: arr^1, 2: arr^2, ..., l: arr^l\},\]</div>
<p>where <span class="math notranslate nohighlight">\(arr^i\)</span> is a <strong>two-dimensional</strong> array (matrix) of weights for the neuron layer <span class="math notranslate nohighlight">\(i\)</span>. For the case of the above figure we could thus take, for instance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]]),</span><span class="mi">2</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]])}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.   2.   1.   1. ]
 [ 2.  -3.   0.2  2. ]
 [-3.  -3.   5.   7. ]]

[[ 1. ]
 [ 0.2]
 [ 2. ]
 [ 2. ]
 [-0.5]]
</pre></div>
</div>
</div>
</div>
<p>For the signal propagating along the network we also use a dictionary of the form</p>
<div class="math notranslate nohighlight">
\[x=\{0: x^0, 1: x^1, 2: x^2, ..., l: x^l\},\]</div>
<p>where <span class="math notranslate nohighlight">\(x^0\)</span> is the input, and <span class="math notranslate nohighlight">\(x^i\)</span> is the output leaving the neuron layer <span class="math notranslate nohighlight">\(i\)</span>, with <span class="math notranslate nohighlight">\(i=1, \dots, l\)</span>. All symbols <span class="math notranslate nohighlight">\(x^j\)</span>, <span class="math notranslate nohighlight">\(j=0, \dots, l\)</span>, are one-dimensional arrays. The bias nodes are included, hence the dimensions of <span class="math notranslate nohighlight">\(x^j\)</span> are <span class="math notranslate nohighlight">\(n_j+1\)</span>, except for the ouput layer which has no bias node, hence <span class="math notranslate nohighlight">\(x^l\)</span> has dimension <span class="math notranslate nohighlight">\(n_l\)</span>. In other words, the dimensions of the signal arrays are equal to the total number of nodes in each layer.</p>
<p>Next, we present the corresponding formulas in rather painful detail, as this is key to avoid any possible confusion related to the notation.
We already know from <a class="reference internal" href="mcp.html#equation-eq-f0">(2)</a> that for a single neuron with <span class="math notranslate nohighlight">\(n\)</span> inputs its incoming signal is calculated as</p>
<div class="math notranslate nohighlight">
\[s = x_0 w_0 + x_1 w_1 + x_2 w_2 + ... + x_n w_n = \sum_{\beta=0}^n x_\beta w_\beta .\]</div>
<p>With more layers (index <span class="math notranslate nohighlight">\(i\)</span>) and neurons (<span class="math notranslate nohighlight">\(n_i\)</span> in layer <span class="math notranslate nohighlight">\(i\)</span>),
the notation generalizes into</p>
<div class="math notranslate nohighlight">
\[
s^i_\alpha=\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l.
\]</div>
<p>Note that the summation starts from <span class="math notranslate nohighlight">\(\beta=0\)</span> to account for the bias node in the preceding layer <span class="math notranslate nohighlight">\((i-1)\)</span>, but <span class="math notranslate nohighlight">\(\alpha\)</span> starts from 1, as only neurons (and not the the bias node) in layer <span class="math notranslate nohighlight">\(i\)</span> receive the signal (see the figure below).</p>
<p>In the algebraic matrix notation, we can also write more compactly
<span class="math notranslate nohighlight">\(s^{iT} = x^{(i-1)T} W^i\)</span>, with <span class="math notranslate nohighlight">\(T\)</span> denoting transposition. Explicitly,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} s^i_1 &amp; s^i_2 &amp; ...&amp; s^i_{n_i} \end{pmatrix} = 
\begin{pmatrix} x^{i-1}_0 &amp; x^{i-1}_1 &amp; ...&amp; x^{i-1}_{n_{i-1}} \end{pmatrix}
\begin{pmatrix} w^i_{01} &amp; w^i_{02} &amp; ...&amp; w^i_{0,n_i} \\ w^i_{11} &amp; w^i_{12} &amp; ...&amp; w^i_{1,n_i} \\ 
 ... &amp; ... &amp; ...&amp; ... \\ w^i_{n_{i-1}1} &amp; w^i_{n_{i-1}2} &amp; ...&amp; w^i_{n_{i-1}n_i} \end{pmatrix}.
\end{split}\]</div>
<p>As we already know very well, the output from a neuron is obtained by acting on its incoming input with an activation function. Thus we finally have</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
x^i_\alpha  = f(s^i_\alpha) = f \left (\sum_{\beta=0}^{n_{i-1}} x^{(i-1)}_\beta w^i_{\beta \alpha} \right), \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l , \\
x^i_0 =1, \;\; i=1,\dots,l-1,  
\end{split}\]</div>
<p>with the bias nodes set to one.
The figure below illustrates the scheme:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/more_layers_28_0.png" src="../_images/more_layers_28_0.png" />
</div>
</div>
<p>The implementation of the feed-forward propagation explained above in Python is following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">we</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Feed-forward propagation</span>
<span class="sd">    </span>
<span class="sd">    input: </span>
<span class="sd">    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]</span>
<span class="sd">    (from input layer 0 to output layer l, bias nodes not counted)</span>
<span class="sd">    </span>
<span class="sd">    we - dictionary of weights for neuron layers 1, 2,...,l in the format</span>
<span class="sd">    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}</span>
<span class="sd">    </span>
<span class="sd">    x_in - input vector of length n_0 (bias not included)</span>
<span class="sd">    </span>
<span class="sd">    f - activation function (default: step)</span>
<span class="sd">    </span>
<span class="sd">    return: </span>
<span class="sd">    x - dictionary of signals leaving subsequent layers in the format</span>
<span class="sd">    {0: array[n_0+1],...,l-1: array[n_(l-1)+1], l: array[nl]}</span>
<span class="sd">    (the output layer carries no bias)</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">l</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ar</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>                   <span class="c1"># number of neuron layers</span>
    <span class="n">x_in</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># input, with the bias node inserted</span>
    
    <span class="n">x</span><span class="o">=</span><span class="p">{}</span>                          <span class="c1"># empty dictionary</span>
    <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_in</span><span class="p">)})</span> <span class="c1"># add input signal</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">l</span><span class="p">):</span>          <span class="c1"># loop over layers except the last one</span>
        <span class="n">s</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">we</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>    <span class="c1"># signal, matrix multiplication </span>
        <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span> <span class="c1"># output from activation</span>
        <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">i</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)})</span> <span class="c1"># add bias node and update x</span>

                                  <span class="c1"># the last layer - no adding of the bias node</span>
        <span class="n">s</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">we</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>    <span class="c1"># signal   </span>
        <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">q</span><span class="p">])</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="n">l</span><span class="p">])]</span> <span class="c1"># output</span>
        <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">l</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>          <span class="c1"># update x</span>
          
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we test how <strong>feed_forward</strong> works on a sample input. For brevity, we do not pass the input bias node of the input in the argument. It is added inside the function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xi</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">xi</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0: array([ 1,  2, -1]), 1: array([1, 1, 0, 0, 0]), 2: [1]}
</pre></div>
</div>
</div>
</div>
<p>The final output of this network is obtained as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualization">
<h2>Visualization<a class="headerlink" href="#visualization" title="Permalink to this headline">¶</a></h2>
<p>For visualization of simple networks, in the <strong>neural</strong> library we provide some drawing functions which show the weights, as well as the signals. Function <strong>plot_net_w</strong> draws positive weights in red and negative in blue, with the widths reflecting their magnitude. The last parameter, here 0.5, rescales  the widths such that the graphics looks nice. Function <strong>plot_net_w_x</strong>  prints in addition the values of the signal leaving the neurons in each layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_37_0.png" src="../_images/more_layers_37_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w_x</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="n">x</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_38_0.png" src="../_images/more_layers_38_0.png" />
</div>
</div>
</div>
<div class="section" id="classifier-with-three-neuron-layers">
<h2>Classifier with three neuron layers<a class="headerlink" href="#classifier-with-three-neuron-layers" title="Permalink to this headline">¶</a></h2>
<p>We are now ready to explicitly construct an example of a binary classifier of points in a concave region: a triagle with a triangular hollow of <a class="reference internal" href="#tri-fig"><span class="std std-numref">Fig. 10</span></a>.
The network architecture is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arch</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">draw</span><span class="o">.</span><span class="n">plot_net</span><span class="p">(</span><span class="n">arch</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_41_0.png" src="../_images/more_layers_41_0.png" />
</div>
</div>
<p>The geometric conditions and the corresponding weights for the first neuron layer are</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></th>
<th class="head"><p>inequality condition</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{0\alpha}^1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{1\alpha}^1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{2\alpha}^1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1&gt;0.1\)</span></p></td>
<td><p>-0.1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><span class="math notranslate nohighlight">\(x_2&gt;0.1\)</span></p></td>
<td><p>-0.1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1+x_2&lt;1\)</span></p></td>
<td><p>1</p></td>
<td><p>-1</p></td>
<td><p>-1</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1&gt;0.25\)</span></p></td>
<td><p>-0.25</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><span class="math notranslate nohighlight">\(x_2&gt;0.25\)</span></p></td>
<td><p>-0.25</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1+x_2&lt;0.8\)</span></p></td>
<td><p>0.8</p></td>
<td><p>-1</p></td>
<td><p>-1</p></td>
</tr>
</tbody>
</table>
<p>Conditions 1-3 provide boundaries for the bigger traingle, and 4-6 for the smaller one contained in the bigger one.
In the second neuron layer we need to realize two AND gates for conditions 1-3 and 4-6, espoectively, hence we take</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{0\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{1\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{2\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{3\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{4\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{5\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{6\alpha}^2\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>-1</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>-1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
</tr>
</tbody>
</table>
<p>Finally, in the output layer we take the <span class="math notranslate nohighlight">\(p \wedge \! \sim \! q\)</span>  gate, hence</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{0\alpha}^3\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{1\alpha}^3\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{2\alpha}^3\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>-1</p></td>
<td><p>1.2</p></td>
<td><p>-0.6</p></td>
</tr>
</tbody>
</table>
<p>Putting all togethr, the weight dictionary is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
   <span class="mi">2</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">]]),</span>
   <span class="mi">3</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mf">1.2</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">]])}</span>
</pre></div>
</div>
</div>
</div>
<p>Feeding forward a sample input yields</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xi</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">]</span>
<span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">xi</span><span class="p">)</span>
<span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w_x</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_45_0.png" src="../_images/more_layers_45_0.png" />
</div>
</div>
<p>We have just found that point [0.2,0.3] is within our region (1 from the output layer). Actually, we we have more information from the intermediae layers. From the second neuron layer we know that the point belongs to the bigger triangle (1 from the lower neuron) and does not belong to the smaller triangle (0 from the upper neuron). From the first neuron layer we may read the condition from the six inequalities.</p>
<p>Next, we will test how our network works for other points. First, we define a function generating a  random point in the square <span class="math notranslate nohighlight">\([0,1]\times [0,1]\)</span> and pass it through the network. We assign to it label 1 if it belongs to the requested triangle with the hollow, and 0 otherwise. Subsequently, we create a large sample of such points and generate the graphics, using pink for label 1 and blue for label 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">po</span><span class="p">():</span>
    <span class="n">xi</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()]</span> <span class="c1"># random point from the [0,1]x[0,1] square</span>
    <span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">xi</span><span class="p">)</span>             <span class="c1"># run feed forward</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">xi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xi</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>               <span class="c1"># the point&#39;s coordinates and label</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samp</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">po</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samp</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.83806748 0.62718519 0.        ]
 [0.46726263 0.34999938 1.        ]
 [0.69545367 0.95397076 0.        ]
 [0.19930639 0.52982584 1.        ]
 [0.41437336 0.69846145 0.        ]]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/more_layers_50_0.png" src="../_images/more_layers_50_0.png" />
</div>
</div>
<p>We can see that our little machine works perfectly well!</p>
<p>At this point the reader might rightly say that the preceding results are trivial: in essence, we have just been implementing some geometric conditions and their conjunctions.</p>
<p>However, there is an important case against this apparent triviality. Imagine we have the data sample with labels, and only this, as in the example of the single MCP neuron of chapter <a class="reference internal" href="mcp.html#mcp-lab"><span class="std std-ref">MCP Neuron</span></a>. Then we do not have the dividing conditions to begin with and need some efficient way to find them. This is exacly what teaching of classifiers does: its sets the weights in such a way that the proper conditions are implicitly built in. After the material of this section, the reader should be convinced that this is perfectly possible.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="perceptron.html" title="previous page">Perceptron</a>
    <a class='right-next' id="next-link" href="backprop.html" title="next page">Back propagation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>