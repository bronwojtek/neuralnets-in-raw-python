
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Interpolation &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Rectification" href="rectification.html" />
    <link rel="prev" title="Back propagation" href="backprop.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="more_layers.html">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/interpol.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/nn_book/docs/interpol.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulated-data">
   Simulated data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anns-for-interpolation">
   ANNs for interpolation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#backprop-for-one-dimensional-functions">
     Backprop for one-dimensional functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="interpolation">
<h1>Interpolation<a class="headerlink" href="#interpolation" title="Permalink to this headline">¶</a></h1>
<section id="simulated-data">
<h2>Simulated data<a class="headerlink" href="#simulated-data" title="Permalink to this headline">¶</a></h2>
<p>So far we have been concerned with <strong>classification</strong>, i.e. with networks recognizing whether a given object (in our examples a point on a plane) has certain features. Now we pass to another practical application, namely <strong>interpolating functions</strong>. This use of ANNs has become widely popular in scientific data analyses. We illustrate the method on a simple example, which explains the basic idea and shows how the method works.</p>
<p>Imagine you have some experimental data. Here we simulate them in an artificial way, e.g.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fi</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.2</span><span class="o">+</span><span class="mf">0.8</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span> <span class="c1"># a function</span>

<span class="k">def</span> <span class="nf">data</span><span class="p">():</span> 
    <span class="n">x</span> <span class="o">=</span> <span class="mf">7.</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="c1"># random x coordinate</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">fi</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="mf">0.4</span><span class="o">*</span><span class="n">func</span><span class="o">.</span><span class="n">rn</span><span class="p">()</span> <span class="c1"># y coordinate = the function value + noise from [-0.2,0.2]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We should now think in terms of supervised learning: <span class="math notranslate nohighlight">\(x\)</span> is the “feature”, and <span class="math notranslate nohighlight">\(y\)</span> is the “label”.</p>
<p>We table our noisy data points and plot them together with the function <strong>fi(x)</strong> around which they fluctuate. It is an imitation of an experimental measurement, which is always burdened with some error, here mimicked with a random noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tab</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">)])</span>    <span class="c1"># data sample</span>
<span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">tab</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># x coordinate</span>
<span class="n">labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">tab</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>                     <span class="c1"># y coordinate</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/interpol_7_0.png" src="../_images/interpol_7_0.png" />
</div>
</div>
<p>In our language of ANNs, we therefore have a training sample consisting of points with the input (feature) <span class="math notranslate nohighlight">\(x\)</span> and the true output (label) <span class="math notranslate nohighlight">\(y\)</span>. As before, we minimize the error function from an appropriate neural network,</p>
<div class="math notranslate nohighlight">
\[E(\{w \}) = \sum_p (y_o^{(p)} - y^{(p)})^2. \]</div>
<p>Since the generated <span class="math notranslate nohighlight">\(y_o\)</span> is a certain (weight-dependent) function of <span class="math notranslate nohighlight">\(x\)</span>, this method is a variant of the <strong>least squares fit</strong>, commonly used in data analysis. The difference is that in the standard least squares method the model function that we fit to the data has some simple analytic form (e.g. <span class="math notranslate nohighlight">\( f(x) = A + B x\)</span>), whereas now it is some “disguised” weight-dependent function provided by the neural network.</p>
</section>
<section id="anns-for-interpolation">
<h2>ANNs for interpolation<a class="headerlink" href="#anns-for-interpolation" title="Permalink to this headline">¶</a></h2>
<p>To understand the fundamental idea, consider a network with just two neurons in the middle layer, with the sigmoid activation function:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/interpol_11_0.png" src="../_images/interpol_11_0.png" />
</div>
</div>
<p>The signals entering the two neurons in the middle layer are, in the notation of chapter <a class="reference internal" href="more_layers.html#more-lab"><span class="std std-ref">More layers</span></a>,</p>
<div class="math notranslate nohighlight">
\[s_1^{1}=w_{01}^{1}+w_{11}^{1} x, \]</div>
<div class="math notranslate nohighlight">
\[s_2^{1}=w_{02}^{1}+w_{12}^{1} x, \]</div>
<p>and the outgoing signals are, correspondingly,</p>
<div class="math notranslate nohighlight">
\[\sigma \left( w_{01}^{1}+w_{11}^{1} x \right), \]</div>
<div class="math notranslate nohighlight">
\[\sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \]</div>
<p>Therefore the combined signal entering the output neuron is</p>
<div class="math notranslate nohighlight">
\[s_1^{1}=w_{01}^{2}+ w_{11}^{2}\sigma \left( w_{01}^{1}+w_{11}^{1} x \right)
+  w_{21}^{2}\sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \]</div>
<p>Taking, for illustration, the sample weight values</p>
<div class="math notranslate nohighlight">
\[w_{01}^{2}=0, \, w_{11}^{2}=1, \, w_{21}^{2}=-1, \,
w_{11}^{1}=w_{12}^{1}=1, \, w_{01}^{1}=-x_1,  \, w_{02}^{1}=-x_2, \]</div>
<p>where <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> is a short-hand notation, we get</p>
<p><span class="math notranslate nohighlight">\(s_1^{1}=\sigma(x-x_1)-\sigma(x-x_2)\)</span>.</p>
<p>This function is shown in the plot below, with <span class="math notranslate nohighlight">\(x_1=-1\)</span> and <span class="math notranslate nohighlight">\(x_2=4\)</span>.
It tends to 0 at <span class="math notranslate nohighlight">\(- \infty\)</span>, then grows with <span class="math notranslate nohighlight">\(x\)</span> to achieve a maximum at
<span class="math notranslate nohighlight">\((x_1+x_2)/2\)</span>, and then decreases, tending to 0 at <span class="math notranslate nohighlight">\(+\infty\)</span>. At <span class="math notranslate nohighlight">\(x=x_1\)</span> and <span class="math notranslate nohighlight">\(x=x_2\)</span>, the values are around 0.5, so one can say that the span of the function is between <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/interpol_13_0.png" src="../_images/interpol_13_0.png" />
</div>
</div>
<p>This is a straightforward but important finding:
We are able to form, with a pair of neurons, a “hump” signal located around a given value, here <span class="math notranslate nohighlight">\( (x_1 + x_2) / 2 = 2\)</span>, and with a a given spread of the order of <span class="math notranslate nohighlight">\(|x_2-x_1|\)</span>. Changing the weights, we are able to modify its shape, width, and height.</p>
<p>One may now think as follows: Imagine we have many neurons to our disposal in the intermediate layer. We can join them in pairs, forming humps “specializing” in particular regions of coordinates. Then, adjusting the heights of the humps, we can readily approximate a given function.</p>
<p>In an actual fitting procedure, we do not need to “join the neurons in pairs”, but make a combined fit of all the parameters simultaneously, as we did in the case of classifiers. The example below shows a composition of 8 sigmoids,</p>
<div class="math notranslate nohighlight">
\[
f = \sigma(z+3)-\sigma(z+1)+2 \sigma(z)-2\sigma(z-4)+
      \sigma(z-2)-\sigma(z-8)-1.3 \sigma(z-8)-1.3\sigma(z-10). 
\]</div>
<p>In the figure, the component functions (the thin lines representing single humps) add up to a function of a rather complicated shape, marked with a thick line.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/interpol_15_0.png" src="../_images/interpol_15_0.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the fitted function is regular, one expects that it can be approximated with a linear combination of sigmoids. With more sigmoids, a better accuracy can be accomplished.</p>
</div>
<p>There is an important difference in ANNs used for function approximation compared to the binary classifiers discussed earlier. There, the answers were 0 or 1, so we were using a step activation function in the output layer, or rather its smooth sigmoid variant. For function approximation, the answers typically form a continuum in the range of the function values. For that reason, in the output layer we just use the <strong>identity</strong> function, i.e., we just pass the incoming signal through. Of course, sigmoids remain to operate in the intermediate layers. Then, the formulas used for backprop are those from section <a class="reference internal" href="backprop.html#bpa-lab"><span class="std std-ref">Backprop algorithm</span></a>, with <span class="math notranslate nohighlight">\(f_l(s)=s\)</span> in the output layer.</p>
<div class="important admonition">
<p class="admonition-title">Output layer for function approximation</p>
<p>In ANNs used for function approximation, the activation function in the output layer is <strong>linear</strong>.</p>
</div>
<section id="backprop-for-one-dimensional-functions">
<h3>Backprop for one-dimensional functions<a class="headerlink" href="#backprop-for-one-dimensional-functions" title="Permalink to this headline">¶</a></h3>
<p>Let us take the architecture:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arch</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>and the random weights</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">set_ran_w</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As just discussed, the output is no longer between 0 and 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward_o</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span><span class="n">features</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">ff</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="n">ffo</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">lin</span><span class="p">)</span>
<span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w_x</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/interpol_25_0.png" src="../_images/interpol_25_0.png" />
</div>
</div>
<p>In the library module <strong>func</strong> we have the function for the backprop algorithm which allows for one activation function in the intermediate layers (we take sigmoid) and a different one in the output layer (we take the identity function). We carry out the training in two stages: First, we take the points from the training sample in a random order, and than we swap over all the points, also decreasing the learning speed in subsequent rounds. This strategy is one of many, but here it nicely does the job:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span><span class="o">=</span><span class="mf">0.02</span>                           <span class="c1"># initial learning speed</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>                <span class="c1"># rounds</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)):</span> <span class="c1"># loop over the data sample points</span>
        <span class="n">pp</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">))</span> <span class="c1"># random point</span>
        <span class="n">func</span><span class="o">.</span><span class="n">back_prop_o</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">pp</span><span class="p">,</span><span class="n">arch</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span>
                         <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="n">df</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">dsig</span><span class="p">,</span><span class="n">fo</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">lin</span><span class="p">,</span><span class="n">dfo</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">dlin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">400</span><span class="p">):</span>               <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">0.999</span><span class="o">*</span><span class="n">eps</span>                  <span class="c1"># dicrease of the learning speed</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)):</span> <span class="c1"># loop over points taken in sequence</span>
        <span class="n">func</span><span class="o">.</span><span class="n">back_prop_o</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="n">arch</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span>
                         <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="n">df</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">dsig</span><span class="p">,</span><span class="n">fo</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">lin</span><span class="p">,</span><span class="n">dfo</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">dlin</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/interpol_30_0.png" src="../_images/interpol_30_0.png" />
</div>
</div>
<p>We note that the obtained red curve is very close to the function used to generate the data sample (black line). This shows that the approximation works. A construction of a quantitative measure (least square sum) is a topic of an exercise.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The activation function in the output layer may be any smooth function with values containing the values of the interpolated function, not necessarily linear.</p>
</div>
<div class="important admonition">
<p class="admonition-title">More dimensions</p>
<p>To interpolate general functions of two or more arguments, one needs use ANNs with at least 3 neuron layers.</p>
</div>
<p>We may understand this as follows <span id="id1">[<a class="reference internal" href="conclusion.html#id7">MullerRS12</a>]</span>: Two neurons in the first neuron layer can form a hump in the <span class="math notranslate nohighlight">\(x_1\)</span>-coordinate, two other ones a hump in the <span class="math notranslate nohighlight">\(x_2\)</span>-coordinate, and so on for all the dimensions <span class="math notranslate nohighlight">\(n\)</span>. Taking a conjunction of these <span class="math notranslate nohighlight">\(n\)</span> humps in the second neuron layer yields a “basis” function specializing in a region around a certain point in the input space. A sufficiently large number of such basis functions can be used for <span class="math notranslate nohighlight">\(n\)</span>-dimensional approximation, in full analogy to the one-dimensional case.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The number of neurons needed in the procedure reflects the behavior of the interpolated function. If the function varies a lot, one needs more neurons. In one dimension, typically, at least twice as many as the number of extrema of the function.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Overfitting</p>
<p>There must be much more data for fitting than the network parameters, to avoid the so-called overfitting problem. Otherwise we can fit the data with a function “fluctuating from point to point”.</p>
</div>
</section>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="warning admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(~\)</span></p>
<ol class="simple">
<li><p>Fit the data points generated by your favorite function (of one variable) with noise. Play with the network architecture and draw conclusions.</p></li>
<li><p>Compute the sum of squared distances between the values of the data points and the corresponding approximating function, and use it as a measure of the goodness of the fit. Test how the number of neurons in the network affects the result.</p></li>
<li><p>Use a network with more layers (at least 3 neuron layers) to fit the data points generated with your favorite two-variable function. Make two-dimensional contour plots for this function and for the function obtained from the neural network and compare the results (of course, they should similar if everything works).</p></li>
</ol>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="backprop.html" title="previous page">Back propagation</a>
    <a class='right-next' id="next-link" href="rectification.html" title="next page">Rectification</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>