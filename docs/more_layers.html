
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>More layers &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../_static/koh.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Back propagation" href="backprop.html" />
    <link rel="prev" title="Perceptron" href="perceptron.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="som.html">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/more_layers.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python//issues/new?title=Issue%20on%20page%20%2Fdocs/more_layers.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/bronwojtek/neuralnets-in-raw-python/edit/master/nn_book/docs/more_layers.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bronwojtek/neuralnets-in-raw-python/master?urlpath=tree/nn_book/docs/more_layers.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-layers-of-neurons">
   Two layers of neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#three-or-more-layers-of-neurons">
   Three or more layers of neurons
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feeding-forward-in-python">
   Feeding forward in Python
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-on-linear-networks">
     Digression on linear networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualization">
   Visualization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classifier-with-three-neuron-layers">
   Classifier with three neuron layers
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="more-layers">
<span id="more-lab"></span><h1>More layers<a class="headerlink" href="#more-layers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="two-layers-of-neurons">
<h2>Two layers of neurons<a class="headerlink" href="#two-layers-of-neurons" title="Permalink to this headline">¶</a></h2>
<p>In the previous chapter we have seen that the MCP neuron with the step activation function realizes the inequality <span class="math notranslate nohighlight">\(x \cdot w=w_0+x_1 w_1 + \dots x_n w_n &gt; 0\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> in the dimensionality of the input space. It is instructive to follow up this geometric interpretation. Taking for definiteness <span class="math notranslate nohighlight">\(n=2\)</span> (the plane), the above inequality corresponds to a division into two half-planes. As we already know, the line given by the equation</p>
<div class="math notranslate nohighlight">
\[x \cdot w=w_0+x_1 w_1 + \dots x_n w_n = 0\]</div>
<p>is the <strong>dividing line</strong>.</p>
<p>Imagine now that we have more such conditions: two, three, etc., in general <span class="math notranslate nohighlight">\(k\)</span> independent conditions. Taking a conjunction of these conditions we can build regions as shown, e.g., in <a class="reference internal" href="#regions-fig"><span class="std std-numref">Fig. 7</span></a>.</p>
<div class="figure align-default" id="regions-fig">
<a class="reference internal image-reference" href="../_images/regions.png"><img alt="../_images/regions.png" src="../_images/regions.png" style="width: 620px;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Sample convex regions in the plane obtained, from left to right, with one inequality condition, and a conjunctions of 2, 3, or 4 inequality conditions, yielding <strong>polygons</strong>.</span><a class="headerlink" href="#regions-fig" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-convex-region admonition">
<p class="admonition-title">Convex region</p>
<p>By definition, region <span class="math notranslate nohighlight">\(A\)</span> is convex if and only if a straight line between any two points in <span class="math notranslate nohighlight">\(A\)</span> is contained in <span class="math notranslate nohighlight">\(A\)</span>. A region which is not convex is called <strong>concave</strong>.</p>
</div>
<p>Clearly, <span class="math notranslate nohighlight">\(k\)</span> inequality conditions can be imposed with <span class="math notranslate nohighlight">\(k\)</span> MCP neurons.
Recall from section <a class="reference internal" href="mcp.html#bool-sec"><span class="std std-ref">Boolean functions</span></a> that we can straightforwardly build boolean functions with the help of the neural networks. In particular, we can make a conjunction of <span class="math notranslate nohighlight">\(k\)</span> conditions by taking a neuron with the weights <span class="math notranslate nohighlight">\(w_0=-1\)</span> and <span class="math notranslate nohighlight">\(1/k &lt; w_i &lt; 1/(k-1)\)</span>, where <span class="math notranslate nohighlight">\(i=1,\dots,k\)</span>. One possibility is, e.g.,</p>
<div class="math notranslate nohighlight">
\[w_i=\frac{1}{k-\frac{1}{2}}.\]</div>
<p>Indeed, let <span class="math notranslate nohighlight">\(p_0=0\)</span>, and the conditions imposed by the inequalities be denoted as <span class="math notranslate nohighlight">\(p_i\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,k\)</span>, which may take values 1 or 0 (true or false). Then</p>
<div class="math notranslate nohighlight">
\[p \cdot w =-1 + p_1 w_1 + \dots + p_k w_k = -1+\frac{p_1+\dots p_k}{k-\frac{1}{2}} &gt; 0\]</div>
<p>if and only if all <span class="math notranslate nohighlight">\(p_i=1\)</span>, i.e. all the conditions are true.</p>
<p>Architectures of networks for <span class="math notranslate nohighlight">\(k=1\)</span>, 2, 3, or 4 conditions are shown in <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 8</span></a>. Going from left to right from the second panel, we have networks with two layers of neurons and with <span class="math notranslate nohighlight">\(k\)</span> neurons in the intermediate layer, providing the inequality conditions, and one neuron in the output layer, acting as the AND gate. Of course, for one condition it is sufficient to have a single neuron, as shown in the left panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 8</span></a>.</p>
<div class="figure align-default" id="nfn-fig">
<a class="reference internal image-reference" href="../_images/nf1-4.png"><img alt="../_images/nf1-4.png" src="../_images/nf1-4.png" style="width: 820px;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Networks capable of classifying data in the corresponding regions of <a class="reference internal" href="#regions-fig"><span class="std std-numref">Fig. 7</span></a>.</span><a class="headerlink" href="#nfn-fig" title="Permalink to this image">¶</a></p>
</div>
<p>In the geometric interpretation, the first neuron layer represents the <span class="math notranslate nohighlight">\(k\)</span> half-planes, and the neuron in the second layer correspond to a convex region with <span class="math notranslate nohighlight">\(k\)</span> sides.</p>
<p>The situation generalizes in an obvious way to data in more dimensions. In that case we have more black dots in the inputs in  <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 8</span></a>. Geometrically, for <span class="math notranslate nohighlight">\(n=3\)</span> we deal with dividing planes and convex <a class="reference external" href="https://en.wikipedia.org/wiki/Polyhedron">polyhedrons</a>, and for <span class="math notranslate nohighlight">\(n&gt;3\)</span> with dividing <a class="reference external" href="https://en.wikipedia.org/wiki/Hyperplane">hyperplanes</a> and convex <a class="reference external" href="https://en.wikipedia.org/wiki/Polytope">polytopes</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If there are numerous neurons <span class="math notranslate nohighlight">\(k\)</span> in the intermediate layer, the resulting polygon has many sides which may approximate a smooth boundary, such as an arc. The approximation is better and better as <span class="math notranslate nohighlight">\(k\)</span> increases.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A percepton with two neuron layers (with sufficiently many neurons in the intermediate layer) can classify points belonging to a convex region in <span class="math notranslate nohighlight">\(n\)</span>-dimensional space.</p>
</div>
</div>
<div class="section" id="three-or-more-layers-of-neurons">
<h2>Three or more layers of neurons<a class="headerlink" href="#three-or-more-layers-of-neurons" title="Permalink to this headline">¶</a></h2>
<p>We have just shown that a two-layer network may classify a convex polygon. Imagine now that we produce two such figures in the second layer of neurons, for instane as in the following network:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/more_layers_13_0.png" src="../_images/more_layers_13_0.png" />
</div>
</div>
<p>Note that the first and second neuron layers are not fully connected here, as we “stack on top of each other” two networks producing triangles, as in the third panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 8</span></a>. Next, in the third neuron layer (here having a single neuron) we implement a <span class="math notranslate nohighlight">\(p \,\wedge \!\sim\!q\)</span> gate, i.e. the conjunction of the conditions that the points belong to one triangle and do not belong to the other one. As we will show shortly, with appropriate weights, the above network may produce a concave region, for example a triangle with a triangular hollow:</p>
<div class="figure align-default" id="tri-fig">
<a class="reference internal image-reference" href="../_images/tritri.png"><img alt="../_images/tritri.png" src="../_images/tritri.png" style="width: 200px;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Triangle with a tringular hollow.</span><a class="headerlink" href="#tri-fig" title="Permalink to this image">¶</a></p>
</div>
<p>Generalizing this argument to other shapes, one can show an important theorem:</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A perceptron with three or more neuron layers (with sufficiently many neurons in intermediate layers) can classify points belonging to <strong>any</strong> region in <span class="math notranslate nohighlight">\(n\)</span>-dimensional space with <span class="math notranslate nohighlight">\(n-1\)</span>-dimensional hyperplane boundaries.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is worth stressing here that three layers provide full functionality! Adding more layers to a classifier does not increase its capabilities.</p>
</div>
</div>
<div class="section" id="feeding-forward-in-python">
<h2>Feeding forward in Python<a class="headerlink" href="#feeding-forward-in-python" title="Permalink to this headline">¶</a></h2>
<p>Before proceeding with an explicit example, we need a Python code for propagation of the signal in a general fully-connected feed-forward network. First, we represent the architecture of a network with <span class="math notranslate nohighlight">\(l\)</span> neuron layers as an array of the form</p>
<div class="math notranslate nohighlight">
\[[n_0,n_1,n_2,...,n_l],\]</div>
<p>where <span class="math notranslate nohighlight">\(n_0\)</span> in the number of the input nodes, and <span class="math notranslate nohighlight">\(n_i\)</span> are the numbers of neurons in layers <span class="math notranslate nohighlight">\(i=1,\dots,l\)</span>. For instance, the architecture of the network from the fourth panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 8</span></a> is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arch</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> 
<span class="n">arch</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 4, 1]
</pre></div>
</div>
</div>
</div>
<p>In the codes of this course we use the convention of <a class="reference internal" href="mcp.html#mcp2-fig"><span class="std std-numref">Fig. 5</span></a>, namely, the bias is treated uniformly with the remaining signal. However, the bias notes are not included in counting the numbers <span class="math notranslate nohighlight">\(n_i\)</span> defined above. In particular, a more detailed view of the fourth panel of <a class="reference internal" href="#nfn-fig"><span class="std std-numref">Fig. 8</span></a> is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net</span><span class="p">(</span><span class="n">arch</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_22_0.png" src="../_images/more_layers_22_0.png" />
</div>
</div>
<p>Here, black dots mean input, gray dots indicate the bias nodes carrying input =1, and the blue blobs are the neurons.</p>
<p>Next, we need the weights of the connections. There are <span class="math notranslate nohighlight">\(l\)</span> sets of weights, each one corresponding to the set of edges entering a given neuron layer from the left.
In the above example, the first neuron layer (blue blobs to the left) has weights which form a <span class="math notranslate nohighlight">\(3 \times 4\)</span> matrix. Here 3 is the number of nodes in the preceding (input) layer (including the bias node) and 4 is the number of neurons in the first neuron layer. Similarly, the weights associated with the second (output) neuron layer form a <span class="math notranslate nohighlight">\(4 \times 1\)</span> matrix. Hence, in our convention, the weight matrices corresponding to subsequent neuron layers <span class="math notranslate nohighlight">\(1, 2, \dots, l\)</span> have dimensions</p>
<div class="math notranslate nohighlight">
\[
(n_0+1)\times n_1, \; (n_1+1)\times n_2, \; \dots \; (n_{l-1}+1)\times n_l.
\]</div>
<p>Thus, to store all the weights of a network we actually need <strong>three</strong> indices: one for the layer, one for the number of nodes in the preceding layer, and one for the number of nodes in the given layer. We could have used a three-dimensional array here, but since we number the neuron layers staring from 1, and arrays start numbering from 0, it is somewhat more convenient to use the Python <strong>dictionary</strong> structure. We then store the weights as</p>
<div class="math notranslate nohighlight">
\[w=\{1: arr^1, 2: arr^2, ..., l: arr^l\},\]</div>
<p>where <span class="math notranslate nohighlight">\(arr^i\)</span> is a <strong>two-dimensional</strong> array (matrix) of weights for the neuron layer <span class="math notranslate nohighlight">\(i\)</span>. For the case of the above figure we can take, for instance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]]),</span><span class="mi">2</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mf">0.2</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">]])}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.   2.   1.   1. ]
 [ 2.  -3.   0.2  2. ]
 [-3.  -3.   5.   7. ]]

[[ 1. ]
 [ 0.2]
 [ 2. ]
 [ 2. ]
 [-0.5]]
</pre></div>
</div>
</div>
</div>
<p>For the signal propagating along the network we also correspondingly use a dictionary of the form</p>
<div class="math notranslate nohighlight">
\[x=\{0: x^0, 1: x^1, 2: x^2, ..., l: x^l\},\]</div>
<p>where <span class="math notranslate nohighlight">\(x^0\)</span> is the input, and <span class="math notranslate nohighlight">\(x^i\)</span> is the output leaving the neuron layer <span class="math notranslate nohighlight">\(i\)</span>, with <span class="math notranslate nohighlight">\(i=1, \dots, l\)</span>. All symbols <span class="math notranslate nohighlight">\(x^j\)</span>, <span class="math notranslate nohighlight">\(j=0, \dots, l\)</span>, are one-dimensional arrays. The bias nodes are included, hence the dimensions of <span class="math notranslate nohighlight">\(x^j\)</span> are <span class="math notranslate nohighlight">\(n_j+1\)</span>, except for the output layer which has no bias node, hence <span class="math notranslate nohighlight">\(x^l\)</span> has the dimension <span class="math notranslate nohighlight">\(n_l\)</span>. In other words, the dimensions of the signal arrays are equal to the total number of nodes in each layer.</p>
<p>Next, we present the corresponding formulas in a rather painful detail, as this is key to avoid any possible confusion related to the notation.
We already know from <a class="reference internal" href="mcp.html#equation-eq-f0">(2)</a> that for a single neuron with <span class="math notranslate nohighlight">\(n\)</span> inputs its incoming signal is calculated as</p>
<div class="math notranslate nohighlight">
\[s = x_0 w_0 + x_1 w_1 + x_2 w_2 + ... + x_n w_n = \sum_{\beta=0}^n x_\beta w_\beta .\]</div>
<p>With more layers (labeled with superscript <span class="math notranslate nohighlight">\(i\)</span>) and neurons (<span class="math notranslate nohighlight">\(n_i\)</span> in layer <span class="math notranslate nohighlight">\(i\)</span>),
the notation generalizes into</p>
<div class="math notranslate nohighlight">
\[
s^i_\alpha=\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l.
\]</div>
<p>Note that the summation starts from <span class="math notranslate nohighlight">\(\beta=0\)</span> to account for the bias node in the preceding layer <span class="math notranslate nohighlight">\((i-1)\)</span>, but <span class="math notranslate nohighlight">\(\alpha\)</span> starts from 1, as only neurons (and not the the bias node) in layer <span class="math notranslate nohighlight">\(i\)</span> receive the signal (see the figure below).</p>
<p>In the algebraic matrix notation, we can also write more compactly
<span class="math notranslate nohighlight">\(s^{iT} = x^{(i-1)T} W^i\)</span>, with <span class="math notranslate nohighlight">\(T\)</span> denoting transposition. Explicitly,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} s^i_1 &amp; s^i_2 &amp; ...&amp; s^i_{n_i} \end{pmatrix} = 
\begin{pmatrix} x^{i-1}_0 &amp; x^{i-1}_1 &amp; ...&amp; x^{i-1}_{n_{i-1}} \end{pmatrix}
\begin{pmatrix} w^i_{01} &amp; w^i_{02} &amp; ...&amp; w^i_{0,n_i} \\ w^i_{11} &amp; w^i_{12} &amp; ...&amp; w^i_{1,n_i} \\ 
 ... &amp; ... &amp; ...&amp; ... \\ w^i_{n_{i-1}1} &amp; w^i_{n_{i-1}2} &amp; ...&amp; w^i_{n_{i-1}n_i} \end{pmatrix}.
\end{split}\]</div>
<p>As we already know very well, the output from a neuron is obtained by acting on its incoming input with an activation function. Thus we finally have</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
x^i_\alpha  = f(s^i_\alpha) = f \left (\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha} \right), \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l , \\
x^i_0 =1, \;\; i=1,\dots,l-1,  
\end{split}\]</div>
<p>with the bias nodes set to one.
The figure below illustrates the notation.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/more_layers_28_0.png" src="../_images/more_layers_28_0.png" />
</div>
</div>
<p>The implementation of the feed-forward propagation in Python is following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feed_forward</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">we</span><span class="p">,</span> <span class="n">x_in</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Feed-forward propagation</span>
<span class="sd">    </span>
<span class="sd">    input: </span>
<span class="sd">    ar - array of numbers of nodes in subsequent layers [n_0, n_1,...,n_l]</span>
<span class="sd">    (from input layer 0 to output layer l, bias nodes not counted)</span>
<span class="sd">    </span>
<span class="sd">    we - dictionary of weights for neuron layers 1, 2,...,l in the format</span>
<span class="sd">    {1: array[n_0+1,n_1],...,l: array[n_(l-1)+1,n_l]}</span>
<span class="sd">    </span>
<span class="sd">    x_in - input vector of length n_0 (bias not included)</span>
<span class="sd">    </span>
<span class="sd">    f - activation function (default: step)</span>
<span class="sd">    </span>
<span class="sd">    return: </span>
<span class="sd">    x - dictionary of signals leaving subsequent layers in the format</span>
<span class="sd">    {0: array[n_0+1],...,l-1: array[n_(l-1)+1], l: array[nl]}</span>
<span class="sd">    (the output layer carries no bias)</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">l</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">ar</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>                   <span class="c1"># number of the neuron layers</span>
    <span class="n">x_in</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># input, with the bias node inserted</span>
    
    <span class="n">x</span><span class="o">=</span><span class="p">{}</span>                          <span class="c1"># empty dictionary x</span>
    <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_in</span><span class="p">)})</span> <span class="c1"># add input signal to x</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">l</span><span class="p">):</span>          <span class="c1"># loop over layers except the last one</span>
        <span class="n">s</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">we</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>    <span class="c1"># signal, matrix multiplication </span>
        <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="n">i</span><span class="p">])]</span> <span class="c1"># output from activation</span>
        <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">i</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)})</span> <span class="c1"># add bias node and update x</span>

                                  <span class="c1"># the output layer l - no adding of the bias node</span>
        <span class="n">s</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">we</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>    <span class="c1"># signal   </span>
        <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">q</span><span class="p">])</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">arch</span><span class="p">[</span><span class="n">l</span><span class="p">])]</span> <span class="c1"># output</span>
        <span class="n">x</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">l</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>          <span class="c1"># update x</span>
          
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>For brevity, we adopt a convention where we do not pass the input bias node of the input in the argument. It is inserted inside the function with <strong>np.insert(x_in,0,1)</strong>. As usual, we use <strong>np.dot</strong> for matrix multiplication.</p>
<p>Next, we test how <strong>feed_forward</strong> works on a sample input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xi</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">xi</span><span class="p">,</span><span class="n">func</span><span class="o">.</span><span class="n">step</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0: array([ 1,  2, -1]), 1: array([1, 1, 0, 0, 0]), 2: [1]}
</pre></div>
</div>
</div>
</div>
<p>The output from this network is obtained as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
<div class="section" id="digression-on-linear-networks">
<h3>Digression on linear networks<a class="headerlink" href="#digression-on-linear-networks" title="Permalink to this headline">¶</a></h3>
<p>Let us now make the following observation. Suppose we have a network with a linear activation function <span class="math notranslate nohighlight">\(f(s)=c s\)</span>. Then the last formula from the feed-forward derivation becomes</p>
<div class="math notranslate nohighlight">
\[ 
x^i_\alpha  = c \sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l , 
\]</div>
<p>or, in the matrix notation,</p>
<div class="math notranslate nohighlight">
\[
x^i = c x^{i-1} w^i.
\]</div>
<p>Iterating this, we get for the signal in the output layer</p>
<div class="math notranslate nohighlight">
\[
x^l = c x^{l-1} w^i = c^2 x^{l-2} w^{l-1} w^l =\dots= c^l x^0 w^1 w^2 \dots w^l = 
x^0 W,
\]</div>
<p>where <span class="math notranslate nohighlight">\(W=c^l w^1 w^2 \dots w^l\)</span>. hence such a network is <strong>equivalent</strong> to a single-layer network with the weight matrix <span class="math notranslate nohighlight">\(W\)</span> as specified above.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For that reason, it does not make sense to consider multiple layer networks with linear activation function.</p>
</div>
</div>
</div>
<div class="section" id="visualization">
<h2>Visualization<a class="headerlink" href="#visualization" title="Permalink to this headline">¶</a></h2>
<p>For visualization of simple networks, in the <strong>draw</strong> module of he <strong>neural</strong> library we provide some drawing functions which show the weights, as well as the signals. Function <strong>plot_net_w</strong> draws the positive weights in red and the negative ones in blue, with the widths reflecting their magnitude. The last parameter, here 0.5, rescales  the widths such that the graphics looks nice. Function <strong>plot_net_w_x</strong>  prints in addition the values of the signal leaving the nodes of each layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="mf">0.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_39_0.png" src="../_images/more_layers_39_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w_x</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_40_0.png" src="../_images/more_layers_40_0.png" />
</div>
</div>
</div>
<div class="section" id="classifier-with-three-neuron-layers">
<h2>Classifier with three neuron layers<a class="headerlink" href="#classifier-with-three-neuron-layers" title="Permalink to this headline">¶</a></h2>
<p>We are now ready to explicitly construct an example of a binary classifier of points in a concave region: a triagle with a triangular hollow of <a class="reference internal" href="#tri-fig"><span class="std std-numref">Fig. 9</span></a>.
The network architecture is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">arch</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net</span><span class="p">(</span><span class="n">arch</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_43_0.png" src="../_images/more_layers_43_0.png" />
</div>
</div>
<p>The geometric conditions and the corresponding weights for the first neuron layer are</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></th>
<th class="head"><p>inequality condition</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{0\alpha}^1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{1\alpha}^1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{2\alpha}^1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1&gt;0.1\)</span></p></td>
<td><p>-0.1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><span class="math notranslate nohighlight">\(x_2&gt;0.1\)</span></p></td>
<td><p>-0.1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1+x_2&lt;1\)</span></p></td>
<td><p>1</p></td>
<td><p>-1</p></td>
<td><p>-1</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1&gt;0.25\)</span></p></td>
<td><p>-0.25</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><span class="math notranslate nohighlight">\(x_2&gt;0.25\)</span></p></td>
<td><p>-0.25</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><span class="math notranslate nohighlight">\(x_1+x_2&lt;0.8\)</span></p></td>
<td><p>0.8</p></td>
<td><p>-1</p></td>
<td><p>-1</p></td>
</tr>
</tbody>
</table>
<p>Conditions 1-3 provide boundaries for the bigger triangle, and 4-6 for the smaller one contained in the bigger one.
In the second neuron layer we need to realize two AND gates for conditions 1-3 and 4-6, respectively, hence we take</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{0\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{1\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{2\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{3\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{4\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{5\alpha}^2\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{6\alpha}^2\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>-1</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>-1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
<td><p>0.4</p></td>
</tr>
</tbody>
</table>
<p>Finally, in the output layer we take the <span class="math notranslate nohighlight">\(p \wedge \! \sim \! q\)</span>  gate, hence</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(\alpha\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{0\alpha}^3\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{1\alpha}^3\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(w_{2\alpha}^3\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>-1</p></td>
<td><p>1.2</p></td>
<td><p>-0.6</p></td>
</tr>
</tbody>
</table>
<p>Putting all this together, the weight dictionary is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">=</span><span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span>
   <span class="mi">2</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mf">0.4</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">]]),</span>
   <span class="mi">3</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mf">1.2</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.6</span><span class="p">]])}</span>
</pre></div>
</div>
</div>
</div>
<p>Feeding forward a sample input yields</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xi</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">]</span>
<span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">xi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">draw</span><span class="o">.</span><span class="n">plot_net_w_x</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/more_layers_47_0.png" src="../_images/more_layers_47_0.png" />
</div>
</div>
<p>We have just found that point [0.2,0.3] is within our region (1 from the output layer). Actually, we we have more information from the intermediate layers. From the second neuron layer we know that the point belongs to the bigger triangle (1 from the lower neuron) and does not belong to the smaller triangle (0 from the upper neuron). From the first neuron layer we may read out the conditions from the six inequalities.</p>
<p>Next, we test how our network works for other points. First, we define a function generating a  random point in the square <span class="math notranslate nohighlight">\([0,1]\times [0,1]\)</span> and pass it through the network. We assign to it label 1 if it belongs to the requested triangle with the hollow, and 0 otherwise. Subsequently, we create a large sample of such points and generate the graphics, using pink for label 1 and blue for label 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">po</span><span class="p">():</span>
    <span class="n">xi</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()]</span> <span class="c1"># random point from the [0,1]x[0,1] square</span>
    <span class="n">x</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">arch</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">xi</span><span class="p">)</span>             <span class="c1"># feed forward</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">xi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xi</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>               <span class="c1"># the point&#39;s coordinates and label</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samp</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">po</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samp</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.71435539 0.05281912 0.        ]
 [0.54490285 0.59347251 0.        ]
 [0.95485103 0.77741598 0.        ]
 [0.71722458 0.42208055 0.        ]
 [0.54892879 0.62149029 0.        ]]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/more_layers_52_0.png" src="../_images/more_layers_52_0.png" />
</div>
</div>
<p>We can see that our little machine works perfectly well!</p>
<p>At this point the reader might rightly say that the preceding results are trivial: in essence, we have just been implementing some geometric conditions and their conjunctions.</p>
<p>However, as in the case of single-layer networks, there is an important argument against this apparent triviality. Imagine again we have a data sample with labels, and only this, as in the example of the single MCP neuron of chapter <a class="reference internal" href="mcp.html#mcp-lab"><span class="std std-ref">MCP Neuron</span></a>. Then we do not have the dividing conditions to begin with and need some efficient way to find them. This is exactly what teaching of classifiers will do: its sets the weights in such a way that the proper conditions are implicitly built in. After the material of this chapter, the reader should be convinced that this is perfectly possible and there is nothing magical about it! In the next chapter we will show how to do it.</p>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="warning admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(~\)</span></p>
<ul class="simple">
<li><p>Design a network and run the code from this lecture for a convex region of your choice.</p></li>
<li><p>Design and program a classifier for four categories of points belonging to regions formed by 2 intersecting lines (hint: include four output cells).</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="perceptron.html" title="previous page">Perceptron</a>
    <a class='right-next' id="next-link" href="backprop.html" title="next page">Back propagation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>