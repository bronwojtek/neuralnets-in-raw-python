
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Self Organizing Maps &#8212; Explaining neural networks in raw Python: lectures in Jupyter</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../_static/koh.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Concluding remarks" href="conclusion.html" />
    <link rel="prev" title="Unsupervised learning" href="unsupervised.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/koh.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Explaining neural networks in raw Python: lectures in Jupyter</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mcp.html">
   MCP Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="memory.html">
   Models of memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="more_layers.html">
   More layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backprop.html">
   Back propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="interpol.html">
   Interpolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rectification.html">
   Rectification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Self Organizing Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="conclusion.html">
   Concluding remarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/som.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/bronwojtek/neuralnets-in-raw-python//issues/new?title=Issue%20on%20page%20%2Fdocs/som.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/bronwojtek/neuralnets-in-raw-python/edit/master/nn_book/docs/som.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bronwojtek/neuralnets-in-raw-python/master?urlpath=tree/nn_book/docs/som.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/bronwojtek/neuralnets-in-raw-python/blob/master/nn_book/docs/som.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kohonen-s-algorithm">
   Kohonen’s algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dim-data-and-1-dim-neuron-grid">
     2-dim. data and 1-dim. neuron grid
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dim-color-map">
     2 dim. color map
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#u-matrix">
   <span class="math notranslate nohighlight">
    \(U\)
   </span>
   -matrix
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapping-colors-on-a-line">
     Mapping colors on a line
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#large-reduction-of-dimensionality">
     Large reduction of dimensionality
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mapping-2-dim-data-into-a-2-dim-grid">
   Mapping 2-dim. data into a 2-dim. grid
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#topology">
   Topology
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lateral-inhibition">
   Lateral inhibition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="self-organizing-maps">
<h1>Self Organizing Maps<a class="headerlink" href="#self-organizing-maps" title="Permalink to this headline">¶</a></h1>
<p>A very important and ingenious application of unsupervised learning are the so-called <strong>Kohonen networks</strong> (<a class="reference external" href="https://en.wikipedia.org/wiki/Teuvo_Kohonen">Teuvo Kohonen</a>, a class of <strong>self-organizing mappings (SOM)</strong>. Consider firs a mapping <span class="math notranslate nohighlight">\(f\)</span> between a <strong>discrete</strong> <span class="math notranslate nohighlight">\(k\)</span>-dimensional set (we call it a <strong>grid</strong> in this chapter) of neurons and <span class="math notranslate nohighlight">\(n\)</span>-dimensional input data <span class="math notranslate nohighlight">\(D\)</span> (continuous or discrete),</p>
<div class="math notranslate nohighlight">
\[
f: N \to D
\]</div>
<p>(note that <strong>this is not a Kohonen mapping yet!</strong>).
Since <span class="math notranslate nohighlight">\(N\)</span> is discrete, each neuron carries an index consisting of <span class="math notranslate nohighlight">\(k\)</span> natural numbers, denoted as <span class="math notranslate nohighlight">\(\bar {i} = (i_1, i_2, ..., i_k)\)</span>. Typically, the dimensions in Kohonen’s networks satisfy <span class="math notranslate nohighlight">\(n \ge k\)</span>. When <span class="math notranslate nohighlight">\(n &gt; k\)</span>, one talks about <strong>reduction of dimensionality</strong>, as then the input space <span class="math notranslate nohighlight">\(D\)</span> has more dimensions than the dimensionaiy of the grid of neurons <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Two examples of such networks are visualized in <a class="reference internal" href="#koh-fig"><span class="std std-numref">Fig. 13</span></a>. The left panel shows a 2-dim. input space <span class="math notranslate nohighlight">\(D\)</span>, and a one dimensional grid on neurons labeled with <span class="math notranslate nohighlight">\(i\)</span>. The input point <span class="math notranslate nohighlight">\((x_1,x_2)\)</span> enters all the neurons in the grid, and one of the neurons (the one with best-suited weights) becomes the <strong>winner</strong> (red dot). The gray oval indicates the <strong>neighborhood</strong> of the winner, to be defined accurately in the following.</p>
<p>The right panel shows an analogous situation for the case of a 3-dim. input and 2-dim. grid of neurons, now labeled with a double index <span class="math notranslate nohighlight">\(\bar {i} = (i_1, i_2)\)</span>. Here, for clarity, we only indicate the edges entering the winner, but they also enter all the other neurons in the grid, similarly to the left panel.</p>
<figure class="align-default" id="koh-fig">
<a class="reference internal image-reference" href="../_images/koha.png"><img alt="../_images/koha.png" src="../_images/koha.png" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Example of Kohonen’s networks. Left: 1-dim. grid of neurons <span class="math notranslate nohighlight">\(N\)</span> and 2-dim. input space <span class="math notranslate nohighlight">\(D\)</span>. Right: 2-dim. grid of neurons <span class="math notranslate nohighlight">\(N\)</span> and 3-dim. input space <span class="math notranslate nohighlight">\(D\)</span>. The red dot indicates the winner, and the gray oval marks its neighborhood.</span><a class="headerlink" href="#koh-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Next, one defines the neuron <strong>proximity function</strong>, <span class="math notranslate nohighlight">\(\phi (\bar {i}, \bar {j})\)</span>, which assigns, to a pair of neurons, a real number depending on their relative position in the grid. This function must decrease with the distance between the neuron indices. A popular choice is a Gaussian,</p>
<div class="math notranslate nohighlight">
\[ \phi(\bar{i}, \bar{j})=\exp\left [ -\frac{(i_1-j_1)^2+...+(i_k-j_k)^2}{2 \delta^2} \right ] ,\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta\)</span> is the <strong>neighborhood radius</strong>. For a 1-dim. grid we have <span class="math notranslate nohighlight">\( \phi(i,j)=\exp\left [ -\frac{(i-j)^2}{2 \delta^2} \right ]\)</span>.</p>
<section id="kohonen-s-algorithm">
<h2>Kohonen’s algorithm<a class="headerlink" href="#kohonen-s-algorithm" title="Permalink to this headline">¶</a></h2>
<p>The set up for Kohonen’s algorithm is similar to the unsupervised learning discussed in the previous chapter. Each neuron <span class="math notranslate nohighlight">\(\bar{i}\)</span> obtains weights <span class="math notranslate nohighlight">\(f\left(\bar{i}\right)\)</span>, which are elements of <span class="math notranslate nohighlight">\(D\)</span>, i.e. form <span class="math notranslate nohighlight">\(n\)</span>-dimensional vectors. One may simply think of this procedure as placing the neurons in some locations in <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p>When an input point <span class="math notranslate nohighlight">\(P\)</span> from <span class="math notranslate nohighlight">\(D\)</span> is fed into the network, one looks for the closest neuron, which becomes the <strong>winner</strong>, exactly as in the unsupervised learning algorithm from section <a class="reference internal" href="unsupervised.html#inn-sec"><span class="std std-ref">Interpretation via neural networks</span></a>. However, now comes a <strong>crucial difference</strong>: Not only the winner is attracted (updated) a bit towards <span class="math notranslate nohighlight">\(P\)</span>, but also its neighbors, to a lesser and lesser extent the farther they are from the winner, as quantified by the proximity function.</p>
<div class="important admonition">
<p class="admonition-title">Winner-take-most strategy</p>
<p>Kohonen’s algorithm involves the “winner take most” strategy, where not only the winner neuron is updated (as in the winner-take-all case), but also its neighbors. The neighbors update is strongest for the nearest neighbors, and gradually weakens with the distance from the winner, as given by the proximity function.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Kohnen’s algorithm</p>
<ol class="simple">
<li><p>Initialize (for instance randomly) <span class="math notranslate nohighlight">\(n\)</span>-dimensional weight vectors <span class="math notranslate nohighlight">\(w_i\)</span>, <span class="math notranslate nohighlight">\(i-1,\dots,m\)</span> for all the <span class="math notranslate nohighlight">\(m\)</span> neurons in the grid. Set an an initial neighborhood radius <span class="math notranslate nohighlight">\( \delta \)</span> and an initial learning speed <span class="math notranslate nohighlight">\( \varepsilon \)</span>.</p></li>
<li><p>Choose (for instance, randomly) a data point <span class="math notranslate nohighlight">\(P\)</span> with coordinates <span class="math notranslate nohighlight">\(x\)</span> from the input space (possibly with an appropriate probability distribution).</p></li>
<li><p>Find the neuron (the winner) for which the distance from <span class="math notranslate nohighlight">\(P\)</span> is the smallest. Denote its index as <span class="math notranslate nohighlight">\( \bar {l} \)</span>.</p></li>
<li><p>The weights of the winner and its neighbors are updated according to the <strong>winner-take-most</strong> recipe:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[w_{\bar{i}} \to w_{\bar{i}} + \varepsilon \phi(\bar{i}, \bar{l})(x - w_{\bar{i}}), \hspace{1cm} i=1, \dots , m. 
\]</div>
<ol class="simple">
<li><p>Loop from <span class="math notranslate nohighlight">\(1.\)</span> for a specified number of points.</p></li>
<li><p>Repeat from <span class="math notranslate nohighlight">\(1.\)</span> in rounds, until a satisfactory result is obtained or a stopping criterion is reached. In each round  <strong>reduce</strong> <span class="math notranslate nohighlight">\( \varepsilon \)</span> and <span class="math notranslate nohighlight">\( \delta \)</span> according to a chosen policy.</p></li>
</ol>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The way the reduction of <span class="math notranslate nohighlight">\( \varepsilon \)</span> and <span class="math notranslate nohighlight">\( \delta \)</span> is done is very important for the desired outcome of the algorithm (see exercises).</p>
</div>
<section id="dim-data-and-1-dim-neuron-grid">
<h3>2-dim. data and 1-dim. neuron grid<a class="headerlink" href="#dim-data-and-1-dim-neuron-grid" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num</span><span class="o">=</span><span class="mi">100</span> <span class="c1"># number of neurons</span>
</pre></div>
</div>
</div>
</div>
<p>and the Gaussian proximity function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">phi</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>                       <span class="c1"># proximity function</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">k</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># Gaussian</span>
</pre></div>
</div>
</div>
</div>
<p>This function looks as follows around the middle neuron (<span class="math notranslate nohighlight">\(k=50\)</span>) and for the width parameter <span class="math notranslate nohighlight">\(\delta=5\)</span>:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_16_0.png" src="../_images/som_16_0.png" />
</div>
</div>
<p>As a feature of a Gaussian, at <span class="math notranslate nohighlight">\(|k-i|=\delta\)</span> the function drops to <span class="math notranslate nohighlight">\(~60\%\)</span> of the central value, and at <span class="math notranslate nohighlight">\(|k-i|=3\delta\)</span> to <span class="math notranslate nohighlight">\(~1\%\)</span>, a tiny fraction. Hence <span class="math notranslate nohighlight">\(\delta\)</span> controls the size of the neighborhood of the winner. The neurons farther away from the winner than, say, <span class="math notranslate nohighlight">\(3\delta\)</span> are practically left uncharged.</p>
<p>We initiate the network by by placing the grid inside the circle, with a random location of each neuron. As said, this amounts to assigning weights to the neuron equal to its location. An auxiliary line is drawn to guide the eye sequentially along the neuron indices: <span class="math notranslate nohighlight">\(1,2,3,\dots m\)</span>. The line has no other meaning.</p>
<p>The weights (neuron locations) are stored in array <strong>W</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">func</span><span class="o">.</span><span class="n">point_c</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">)])</span> <span class="c1"># random initialization of weights</span>
</pre></div>
</div>
</div>
</div>
<p>As a result of the initial randomness, the neurons are, of course, “chaotically” distributed:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_20_0.png" src="../_images/som_20_0.png" />
</div>
</div>
<p>Next, we initialize the parameters <strong>eps</strong> amd <strong>delta</strong> and run the algorithm. Its structure is analogous to the previously discussed codes and is a straightforward implementation of the steps spelled out in the previous section. For that reason, we only provide the comments in the code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>   <span class="c1"># initial learning speed </span>
<span class="n">de</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># initial neighborhood distance</span>
<span class="n">ste</span><span class="o">=</span><span class="mi">0</span>    <span class="c1"># inital number of caried out steps</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Kohonen&#39;s algorithm</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">150</span><span class="p">):</span>              <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="o">*</span><span class="mf">.98</span>                   <span class="c1"># dicrease learning speed</span>
    <span class="n">de</span><span class="o">=</span><span class="n">de</span><span class="o">*</span><span class="mf">.95</span>                     <span class="c1"># ... and the neighborhood distance</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>          <span class="c1"># loop over points</span>
        <span class="n">p</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">point_c</span><span class="p">()</span>          <span class="c1"># random point</span>
        <span class="n">ste</span><span class="o">=</span><span class="n">ste</span><span class="o">+</span><span class="mi">1</span>                 <span class="c1"># count steps</span>
        <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">eucl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">W</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">)]</span> 
         <span class="c1"># array of squares of Euclidean disances between p and the neuron locations</span>
        <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="c1"># index of the winner</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>      <span class="c1"># loop over all the neurons</span>
            <span class="n">W</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="n">phi</span><span class="p">(</span><span class="n">ind_min</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">de</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">W</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> 
             <span class="c1"># update of the neuron locations (weights), depending on proximity</span>
</pre></div>
</div>
</div>
</div>
<p>As the above algorithm progresses (see <a class="reference internal" href="#kohstory-fig"><span class="std std-numref">Fig. 14</span></a>) the neuron grid first disentangles, and then gradually fills the whole space <span class="math notranslate nohighlight">\(D\)</span> (circle) in such a way that the neurons with adjacent indices are located close to each other.
Figuratively speaking, a new point <span class="math notranslate nohighlight">\(P\)</span> attracts towards itself the nearest neuron (the winner), but also, to a weaker extent, its neighbors. At the beginning of the algorithm the neighborhood distance <strong>de</strong> is large, so large chunks of the neighboring neurons in the input grid are pulled together towards <span class="math notranslate nohighlight">\(P\)</span>, and the arrangement looks as in the top right corner of <a class="reference internal" href="#kohstory-fig"><span class="std std-numref">Fig. 14</span></a>. At later stages <strong>de</strong> reduces, so only the winner and possibly its very immediate neighbors are attracted to a new point.
After completion (bottom right panel), individual neurons “specialize” (are close to) in a certain data area.</p>
<p>In the present example, after about 20000 steps the result practically stops to change.</p>
<figure class="align-default" id="kohstory-fig">
<a class="reference internal image-reference" href="../_images/kaall.png"><img alt="../_images/kaall.png" src="../_images/kaall.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Progress of Kohonen’s algorithm. The line, drawn to guide the eye, connects neurons with adjacent indices.</span><a class="headerlink" href="#kohstory-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="note admonition">
<p class="admonition-title">Kohonen’s network as a classifier</p>
<p>Having the trained network, we may use it as a classifier similarly as in chapter {\ref}<code class="docutils literal notranslate"><span class="pre">un-lab</span></code>. We label a point from <span class="math notranslate nohighlight">\(D\)</span> with the index of the nearest neuron. One can interpret this as a Voronoi construction, see section <a class="reference internal" href="unsupervised.html#vor-lab"><span class="std std-ref">Voronoi areas</span></a>.</p>
</div>
<p>The plots in <a class="reference internal" href="#kohstory-fig"><span class="std std-numref">Fig. 14</span></a> are made in coordinates <span class="math notranslate nohighlight">\((x_1,x_2)\)</span>, that is, from the “point of view” of the input <span class="math notranslate nohighlight">\(D\)</span>-space. One may also look at the result from the point of view of the <span class="math notranslate nohighlight">\(N\)</span>-space, i.e. plot <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> as functions of the neuron index <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Caution</p>
<p>When presenting results of Kohonen’s algorithm, one sometimes makes plots in <span class="math notranslate nohighlight">\(D\)</span>-space, and sometimes in <span class="math notranslate nohighlight">\(N\)</span>-space, which may lead to some confusion.</p>
</div>
<p>The plots in the <span class="math notranslate nohighlight">\(N\)</span>-space, fully equivalent in information to the plot in, e.g., the bottom right panel of <a class="reference internal" href="#kohstory-fig"><span class="std std-numref">Fig. 14</span></a>, are following:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_29_0.png" src="../_images/som_29_0.png" />
</div>
</div>
<p>We note that the jumps in the above plotted curves are small, since the subsequent neurons are close to each other. This feature can be presented quantitatively as in the histogram below, where we can see that the average distance between the neurons is about 0.07, and the spread is between 0.05 and 0.10.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dd</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
        <span class="c1"># array of distances between subsequent neurons in the grid</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;distance&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;distribution&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">dd</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>   <span class="c1"># histogram</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_31_0.png" src="../_images/som_31_0.png" />
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Remarks</p>
<ul class="simple">
<li><p>We took a situation in which the data space with the dimension <span class="math notranslate nohighlight">\(n = 2\)</span> is “sampled” by a discrete set of neurons forming  <span class="math notranslate nohighlight">\(k=1\)</span>-dimensional grid. Hence we encounter dimensional reduction.</p></li>
<li><p>The outcome of the algorithm is a network in which a given neuron “focuses” on data from its vicinity. In a general case, where the data can be non-uniformly distributed, the neurons would fill the area containing more data more densely.</p></li>
<li><p>The policy of choosing initial <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\varepsilon \)</span> parameters and reducing them appropriately in subsequent rounds is based on experience and is non-trivial. The results depend significantly on this choice.</p></li>
<li><p>The final result, even with the same <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\varepsilon \)</span> strategy, is not unequivocal, i.e. running the algorithm with a different initialization of the weights (initial positions of neurons) yields different outcomes, usually equally “good”.</p></li>
<li><p>Finally, the progress and the result of the algorithm is reminiscent of the construction of the <a class="reference external" href="https://en.wikipedia.org/wiki/Peano_curve">Peano curve</a> in mathematics, which fills densely an area with a line.
As we increase the number of neurons, the analogy gets closer and closer.</p></li>
</ul>
</div>
</section>
<section id="dim-color-map">
<h3>2 dim. color map<a class="headerlink" href="#dim-color-map" title="Permalink to this headline">¶</a></h3>
<p>Now we pass to a case of 3-dim. data and 2-dim. neuron grid, which is a situation from the right panel of <a class="reference internal" href="#koh-fig"><span class="std std-numref">Fig. 13</span></a> (hence also with dimensionality reduction). As we know, an RGB color is described with three numbers <span class="math notranslate nohighlight">\([r,g,b]\)</span> from <span class="math notranslate nohighlight">\([0,1]\)</span>, so it can nicely serve as input in our example.</p>
<p>The distance squared between two colors (this is just a distance between two points in the 3-dim. space) is taken in the Euclidean form:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dist3</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span><span class="n">p2</span><span class="p">):</span> 
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square of the Euclidean distance between points p1 and p2</span>
<span class="sd">    in 3 dimensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">p1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">p1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">p1</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">p2</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>The proximity function is now a Gaussian in two dimensions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">phi2</span><span class="p">(</span><span class="n">ix</span><span class="p">,</span><span class="n">iy</span><span class="p">,</span><span class="n">kx</span><span class="p">,</span><span class="n">ky</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>  <span class="c1"># proximity function for 2-dim. grid</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">ix</span><span class="o">-</span><span class="n">kx</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">iy</span><span class="o">-</span><span class="n">ky</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># Gaussian</span>
</pre></div>
</div>
</div>
</div>
<p>We also decide to normalize the RGB colors such that <span class="math notranslate nohighlight">\(r^2+g^2+b^2=1\)</span>. This makes the perceived intensity of colors similar (this normalization could be dropped, as irrelevant for the method to work).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rgbn</span><span class="p">():</span>
    <span class="n">r</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="c1"># random RGB</span>
    <span class="n">norm</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">r</span><span class="o">*</span><span class="n">r</span><span class="o">+</span><span class="n">g</span><span class="o">*</span><span class="n">g</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>                                      <span class="c1"># norm</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">r</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">b</span><span class="p">]</span><span class="o">/</span><span class="n">norm</span><span class="p">)</span>                                  <span class="c1"># normalized RGB</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we generate and plot a sample of <strong>ns</strong> points with (normalized) RGB colors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ns</span><span class="o">=</span><span class="mi">40</span>                            <span class="c1"># number of colors in the sample</span>
<span class="n">samp</span><span class="o">=</span><span class="p">[</span><span class="n">rgbn</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">)]</span> <span class="c1"># random sample</span>

<span class="n">pls</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">samp</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">);</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_41_0.png" src="../_images/som_41_0.png" />
</div>
</div>
<p>We use a 2-dim. <strong>size</strong> x <strong>size</strong> grid of neurons. Each neuron’s position (that is its color) in the 3-dim. <span class="math notranslate nohighlight">\(D\)</span>-space is initialized randomly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">size</span><span class="o">=</span><span class="mi">40</span>                        <span class="c1"># neuron array of size x size (40 x 40)</span>
<span class="n">tab</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>    <span class="c1"># create array tab with zeros  </span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>          <span class="c1"># i index in the grid    </span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>      <span class="c1"># j index in the grid</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>     <span class="c1"># RGB: k=0-red, 1-green, 2-blue</span>
            <span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">k</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="c1"># random number form [0,1]</span>
            <span class="c1"># 3 RGB components for neuron in the grid positin (i,j)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_44_0.png" src="../_images/som_44_0.png" />
</div>
</div>
<p>Now we are ready to run Kohonen’s algorithm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>   <span class="c1"># initial parameters</span>
<span class="n">de</span> <span class="o">=</span> <span class="mi">20</span>  
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">150</span><span class="p">):</span>    <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="o">*</span><span class="mf">.995</span>      
    <span class="n">de</span><span class="o">=</span><span class="n">de</span><span class="o">*</span><span class="mf">.96</span>           <span class="c1"># de shrinks a bit faster than eps     </span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span> <span class="c1"># loop over the points in the data sample       </span>
        <span class="n">p</span><span class="o">=</span><span class="n">samp</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>       <span class="c1"># point from the sample</span>
        <span class="n">dist</span><span class="o">=</span><span class="p">[[</span><span class="n">dist3</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span> 
                        <span class="c1"># distance of p from all neurons</span>
        <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="c1"># the winner index</span>
        <span class="n">ind_1</span><span class="o">=</span><span class="n">ind_min</span><span class="o">//</span><span class="n">size</span>       <span class="c1"># a trick to get a 2-dim index</span>
        <span class="n">ind_2</span><span class="o">=</span><span class="n">ind_min</span><span class="o">%</span><span class="k">size</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span> 
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
                <span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="n">phi2</span><span class="p">(</span><span class="n">ind_1</span><span class="p">,</span><span class="n">ind_2</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">de</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span> <span class="c1"># update         </span>
</pre></div>
</div>
</div>
</div>
<p>A word of explanation is in place here, concerning the numpy <strong>argmin</strong> function. For a 2-dim. array it provides the index of the minimum in the corresponding <strong>flattened</strong> array (cf. section <a class="reference internal" href="memory.html#het-lab"><span class="std std-ref">Heteroassociative memory</span></a>). Hence, to get the indices in the two dimensions, we need to apply the operations <strong>//</strong> (integer division) and <strong>%</strong> (remainder). For instance, in an array <strong>ind_min=53</strong>, then <strong>ind_1=ind_min//size=53//10=5</strong> and <strong>ind_2=ind_min%size=53//10=3</strong>.</p>
<p>As a result of the above code, we get an arrangement of our color sample in two dimensions in such a way that the neighboring areas in the grid have a similar color “specializing” on the color of a given sample point (note the plot is in the <span class="math notranslate nohighlight">\(N\)</span>-space):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Kohonen color map&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> 

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$i$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$j$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_50_0.png" src="../_images/som_50_0.png" />
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Remarks</p>
<ul class="simple">
<li><p>The areas for the individual colors of the sample have a comparable area. Generally, the area is proportional to the frequency of the data point in the sample.</p></li>
<li><p>To get sharper boundaries between the regions, <strong>de</strong> would have to shrink even faster compared to <strong>eps</strong>. Then, in the final stage of learning, the neuron update process takes place within a smaller neighborhood radius and more resolution in the boundaries can be achieved.</p></li>
</ul>
</div>
</section>
</section>
<section id="u-matrix">
<h2><span class="math notranslate nohighlight">\(U\)</span>-matrix<a class="headerlink" href="#u-matrix" title="Permalink to this headline">¶</a></h2>
<p>A convenient way to present the results of Kohonen’s algorithm when the grid is 2-dimensional is via the <strong>unified distance matrix</strong> (shortly <strong><span class="math notranslate nohighlight">\(U\)</span>-matrix</strong>). The idea is to plot a 2-dimensional grayscale map in <span class="math notranslate nohighlight">\(N\)</span>-space with the intensity given by the averaged distance (in <span class="math notranslate nohighlight">\(D\)</span>-space) of the given neuron to its immediate neighbors, and not a neuron property itself (such as its color in the figure above). This is particularly useful when the dimension of the input space is large, hence it is difficult to visualize the results directly.</p>
<p>The definition of a <span class="math notranslate nohighlight">\(U\)</span>-matrix element <span class="math notranslate nohighlight">\(U_{ij}\)</span> is explained in <a class="reference internal" href="#udm-fig"><span class="std std-numref">Fig. 15</span></a>. Let <span class="math notranslate nohighlight">\(d\)</span> be the distance in <span class="math notranslate nohighlight">\(D\)</span>-space and <span class="math notranslate nohighlight">\([i,j]\)</span> denote the neuron of indices <span class="math notranslate nohighlight">\(i,j\)</span> . We take</p>
<div class="math notranslate nohighlight">
\[
U_{ij}=\sqrt{d\left([i,j],[i+1,j]\right)^2+d\left([i,j],[i-1,j]\right)^2+
        d\left([i,j],[i,j+1]\right)^2+d\left([i,j],[i,j-1]\right)^2 }.
\]</div>
<figure class="align-default" id="udm-fig">
<a class="reference internal image-reference" href="../_images/udm.png"><img alt="../_images/udm.png" src="../_images/udm.png" style="width: 150px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Construction of <span class="math notranslate nohighlight">\(U_{ij}\)</span>: a geometric average of the distances along the indicated links.</span><a class="headerlink" href="#udm-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The Python implementation of the above definition is following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">udm</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">size</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span>    <span class="c1"># initiaize U-matrix with elements set to 0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>        <span class="c1"># loops over the neurons in the grid</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">udm</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dist3</span><span class="p">(</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="n">dist3</span><span class="p">(</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span>
                            <span class="n">dist3</span><span class="p">(</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">])</span><span class="o">+</span><span class="n">dist3</span><span class="p">(</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">j</span><span class="p">]))</span>
                                 <span class="c1"># U-matrix as explained above</span>
</pre></div>
</div>
</div>
</div>
<p>The result, corresponding one-to-one to the color map above, can be presented in a contour plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;U-matrix&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span> 

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span> <span class="c1"># loops over indices, excluding the boundaries of the grid</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="o">-</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="n">udm</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> 
                        <span class="c1"># color format: [R,G,B,intensity], 2 just scales up</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$i$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$j$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_58_0.png" src="../_images/som_58_0.png" />
</div>
</div>
<p>The white regions in the above figure show the clusters (they correspond one-to-one to the regions of the same color in the previously shown color map). There, the elements <span class="math notranslate nohighlight">\(U_{ij} \simeq 0\)</span>. The clusters are separated with darker boundaries. The higher the dividing ridge between clusters, the darker the intensity.</p>
<p>The result may also be visualized with a 3-dim. plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
<span class="n">axes1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">gca</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="n">xx_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">xx_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xx_1</span><span class="p">,</span> <span class="n">xx_2</span><span class="p">)</span>

<span class="n">Z</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">udm</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="o">-</span><span class="mi">2</span><span class="p">)]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="o">-</span><span class="mi">2</span><span class="p">)])</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">.5</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span><span class="n">x_2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$i$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$j$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;U-matrix&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_60_0.png" src="../_images/som_60_0.png" />
</div>
</div>
<p>We can now classify a given (new) data point according to the obtained map. We generate a new (normalized) RGB color:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nd</span><span class="o">=</span><span class="n">rgbn</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_63_0.png" src="../_images/som_63_0.png" />
</div>
</div>
<p>It is useful to obtain a map of distances of our grid neurons from this point:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tad</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
        <span class="n">tad</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="n">dist3</span><span class="p">(</span><span class="n">nd</span><span class="p">,</span><span class="n">tab</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
        

<span class="n">ind_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">tad</span><span class="p">)</span> <span class="c1"># winner</span>
<span class="n">in_x</span><span class="o">=</span><span class="n">ind_m</span><span class="o">//</span><span class="n">size</span>      
<span class="n">in_y</span><span class="o">=</span><span class="n">ind_m</span><span class="o">%</span><span class="k">size</span> 

<span class="n">da</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tad</span><span class="p">[</span><span class="n">in_x</span><span class="p">][</span><span class="n">in_y</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Closest neuron grid indices: (&quot;</span><span class="p">,</span><span class="n">in_x</span><span class="p">,</span><span class="s2">&quot;,&quot;</span><span class="p">,</span><span class="n">in_y</span><span class="p">,</span><span class="s2">&quot;)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Distance: &quot;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">da</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Closest neuron grid indices: ( 30 , 6 )
Distance:  0.012
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_66_0.png" src="../_images/som_66_0.png" />
</div>
</div>
<p>The lightest region in the above figure indicates the cluster, to which the new point belongs. The darker the region, the larger is the distance from the corresponding neuron.</p>
<p>One should stress that we have obtained a classifier which not only assigns a closest cluster to a probed point, but also provides its distances from all other clusters.</p>
<section id="mapping-colors-on-a-line">
<h3>Mapping colors on a line<a class="headerlink" href="#mapping-colors-on-a-line" title="Permalink to this headline">¶</a></h3>
<p>In this subsection we present an example of a mapping of 3-dim. data into a 1-dim. neuron grid, hence a reduction of three dimensions into one. This proceeds exactly along the lines of the previous subsection, so we are very brief in comments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ns</span><span class="o">=</span><span class="mi">8</span>
<span class="n">samp</span><span class="o">=</span><span class="p">[</span><span class="n">rgbn</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">)]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Sample colors&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">samp</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">400</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_70_0.png" src="../_images/som_70_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">si</span><span class="o">=</span><span class="mi">50</span>                    <span class="c1"># 1-dim. grid of si neurons, 3 RGB components</span>
<span class="n">tab2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">si</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>    <span class="c1"># neuron gri</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">si</span><span class="p">):</span>      
    <span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">rgbn</span><span class="p">()</span>    <span class="c1"># random initialization</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_72_0.png" src="../_images/som_72_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>    
<span class="n">de</span> <span class="o">=</span> <span class="mi">20</span>   
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span> 
    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="o">*</span><span class="mf">.99</span>      
    <span class="n">de</span><span class="o">=</span><span class="n">de</span><span class="o">*</span><span class="mf">.96</span>        
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>       
        <span class="n">p</span><span class="o">=</span><span class="n">samp</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
        <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">dist3</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">si</span><span class="p">)]</span> 
        <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>          
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">si</span><span class="p">):</span>
            <span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="n">phi</span><span class="p">(</span><span class="n">ind_min</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">de</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_75_0.png" src="../_images/som_75_0.png" />
</div>
</div>
<p>As expected, we note smooth transitions between colors. The formation of clusters can be seen with the <span class="math notranslate nohighlight">\(U\)</span>-matrix, which now is, of course, one-dimensional:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ta2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">si</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">si</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ta2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dist3</span><span class="p">(</span><span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span><span class="o">+</span><span class="n">dist3</span><span class="p">(</span><span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">tab2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_78_0.png" src="../_images/som_78_0.png" />
</div>
</div>
<p>The minima (there are 8 of them, equal to the multiplicity of the sample) indicate the clusters. The height of the separating peaks shows how much the neighboring colors differ. Again, we see a nicely produced classifier, this time with two dimensions “hidden away”, as we reduce from three to one.</p>
</section>
<section id="large-reduction-of-dimensionality">
<h3>Large reduction of dimensionality<a class="headerlink" href="#large-reduction-of-dimensionality" title="Permalink to this headline">¶</a></h3>
<p>In many situations the input space may have a very large dimension. In the <a class="reference external" href="https://en.wikipedia.org/wiki/Self-organizing_map">Wikipedia example</a> quoted here, one takes articles from various fields and computes frequencies of used words (for instance, in a given article how  many times the word “goalkeeper” has been used, divided by the total number of words in the article). Essentially, the dimensionality of <span class="math notranslate nohighlight">\(D\)</span> is of the order of the number of all English words, a huge number <span class="math notranslate nohighlight">\(\sim 10^5\)</span>! Then, with a properly defined distance depending on these frequencies, one uses Kohonen’s algorithm to carry out a reduction into a 2-dim. grid of neurons. The resulting <span class="math notranslate nohighlight">\(U\)</span>-matrix can be drawn as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_83_0.jpg" src="../_images/som_83_0.jpg" />
</div>
</div>
<p>Not surprisingly, we notice that articles on sports are special and form a very well defined cluster. The reason is that the sport’s jargon is very specific. The Media are also distinguished, whereas other fields are more-less uniformly distributed. The example shows how we can, with a very simple method, comprehend data in a multidimensional space and see specific correlations/clusters. Whereas some conclusions may be obvious, such as the fact that sport has a unique jargon, other are less transparent, for instance the emergence of the media cluster and lack of well-defined clusters for other fields, e.g. for mathematics.</p>
</section>
</section>
<section id="mapping-2-dim-data-into-a-2-dim-grid">
<h2>Mapping 2-dim. data into a 2-dim. grid<a class="headerlink" href="#mapping-2-dim-data-into-a-2-dim-grid" title="Permalink to this headline">¶</a></h2>
<p>Finally, we come to a very important case of mapping 2-dim. data in a 2-dim. grid, i.e. with no dimensionality reduction. In particular, this case is realized in our vision system between the retina and the visual cortex.</p>
<p>The algorithm proceeds analogously to the previous cases. We initialize an <span class="math notranslate nohighlight">\(n \times n\)</span> grid of neurons and place them randomly in the square <span class="math notranslate nohighlight">\([0,1]\times [0,1]\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="o">=</span><span class="mi">10</span>
<span class="n">sam</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">func</span><span class="o">.</span><span class="n">point</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>The lines, again drawn to guide the eye, join the adjacent index pairs in the grid: [i,j] and [i+1,j], or [i,j] and [i,j+1] (the neurons in the interior of the grid have 4 nearest neighbors, those at the boundary 3, except for the corners, which have only 2).</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_89_0.png" src="../_images/som_89_0.png" />
</div>
</div>
<p>We note a total initial “chaos”, as the neurons are located randomly. Now comes Kohonen’s miracle:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eps</span><span class="o">=</span><span class="mf">.5</span>   <span class="c1"># initial learning speed</span>
<span class="n">de</span> <span class="o">=</span> <span class="mi">3</span>   <span class="c1"># initial neighborhood distance</span>
<span class="n">nr</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># number of rounds</span>
<span class="n">rep</span><span class="o">=</span> <span class="mi">300</span> <span class="c1"># number of points in each round</span>
<span class="n">ste</span><span class="o">=</span><span class="mi">0</span>    <span class="c1"># inital number of caried out steps</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># completely analogous to the previous codes of this chapter</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nr</span><span class="p">):</span>   <span class="c1"># rounds</span>
    <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="o">*</span><span class="mf">.97</span>      
    <span class="n">de</span><span class="o">=</span><span class="n">de</span><span class="o">*</span><span class="mf">.98</span>         
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rep</span><span class="p">):</span>    <span class="c1"># repeat for rep points</span>
        <span class="n">ste</span><span class="o">=</span><span class="n">ste</span><span class="o">+</span><span class="mi">1</span>
        <span class="n">p</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">point</span><span class="p">()</span> 
        <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">eucl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">sam</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">)]</span> 
        <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> 
        <span class="n">ind_i</span><span class="o">=</span><span class="n">ind_min</span><span class="o">%</span><span class="k">n</span>
        <span class="n">ind_j</span><span class="o">=</span><span class="n">ind_min</span><span class="o">//</span><span class="n">n</span>       
        
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span> 
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
                <span class="n">sam</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="o">*</span><span class="n">j</span><span class="p">]</span><span class="o">+=</span><span class="n">eps</span><span class="o">*</span><span class="n">phi2</span><span class="p">(</span><span class="n">ind_i</span><span class="p">,</span><span class="n">ind_j</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">de</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">p</span><span class="o">-</span><span class="n">sam</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="o">*</span><span class="n">j</span><span class="p">])</span> 
</pre></div>
</div>
</div>
</div>
<p>Here is the history of a simulation:</p>
<figure class="align-default" id="kohstory2-fig">
<a class="reference internal image-reference" href="../_images/kball.png"><img alt="../_images/kball.png" src="../_images/kball.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Progress of Kohonen’s algorithm. The lines, drawn to guide the eye, connects neurons with adjacent indices.</span><a class="headerlink" href="#kohstory2-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As the algorithm progresses, the initial “chaos” gradually changes into a nearly perfect order, with the grid placed uniformly in the square of the data, with only slight displacements from a regular arrangement. On the way, near 40 steps, we notice a phenomenon called “twist”, where the grid is crumpled. In the twist region, many neurons, also of distant indices, have a close location in <span class="math notranslate nohighlight">\((x_1,x_2)\)</span>.</p>
</section>
<section id="topology">
<h2>Topology<a class="headerlink" href="#topology" title="Permalink to this headline">¶</a></h2>
<p>Recall the Voronoi construction of categories introduced in section <a class="reference internal" href="unsupervised.html#vor-lab"><span class="std std-ref">Voronoi areas</span></a>. One can use it now again, treating the neurons from a grid as the Voronoi points. The Voronoi construction provides a mapping <span class="math notranslate nohighlight">\(v\)</span> from the data space <span class="math notranslate nohighlight">\(D\)</span> to the neuron space <span class="math notranslate nohighlight">\(N\)</span>,</p>
<div class="math notranslate nohighlight">
\[ 
v: D \to N 
\]</div>
<p>(note that this goes in the opposite direction than function <span class="math notranslate nohighlight">\(f\)</span> defined at the beginning of this chapter).</p>
<p>The procedure is as follows:
We take the final outcome of the algorith, such as in the  bottom right panel of <a class="reference internal" href="#kohstory2-fig"><span class="std std-numref">Fig. 16</span></a>, construct the Voronoi areas for all the neurons, and thus obtain a mapping <span class="math notranslate nohighlight">\(v\)</span> for all the points in the <span class="math notranslate nohighlight">\((x_1,x_2)\)</span> square. The reader may notice that there is an ambiguity for points lying exactly at the boundaries between the neighboring areas, but this can be taken care of by using an additional prescription (for instance, selecting a neuron lying at a direction which has the lowest azimuthal angle, etc.)</p>
<p>Now a key observation:</p>
<div class="important admonition">
<p class="admonition-title">Topological property</p>
<p>For situations without twists, such as in the bottom right panel of <a class="reference internal" href="#kohstory2-fig"><span class="std std-numref">Fig. 16</span></a>, mapping <span class="math notranslate nohighlight">\(v\)</span> has the property that when <span class="math notranslate nohighlight">\(d_1\)</span> and <span class="math notranslate nohighlight">\(d_2\)</span> from <span class="math notranslate nohighlight">\(D\)</span> are close to each other, then also their corresponding neurons are close, i.e. the indices <span class="math notranslate nohighlight">\(v(d_1)\)</span> and <span class="math notranslate nohighlight">\(v(d_2)\)</span> are close.</p>
</div>
<p>This observation is straightforward to prove: Since <span class="math notranslate nohighlight">\(d_1\)</span> and <span class="math notranslate nohighlight">\(d_2\)</span> are close (and we mean very close, closer than the grid spacing), they must belong either to</p>
<ul class="simple">
<li><p>the same Voronoi area, where <span class="math notranslate nohighlight">\(v(d_1)=v(d_2)\)</span>, or</p></li>
<li><p>a pair of neighboring Voronoi areas.</p></li>
</ul>
<p>Since for the considered situation (without twists) the neighboring areas have the grid indices differing by 1, the conclusion that <span class="math notranslate nohighlight">\(v(d_1)\)</span> and <span class="math notranslate nohighlight">\(v(d_2)\)</span> are close follows immediately.</p>
<p>Note that this feature of Kohonen’s maps is far from trivial and does not hold for a general mapping. Imagine for instance that we stop our simulations for <a class="reference internal" href="#kohstory2-fig"><span class="std std-numref">Fig. 16</span></a> after 40 steps (top central panel) and are left with a “twisted” grid. In the vicinity of the twist, the indices of the adjacent Voronoi areas differ largely, and the advertised topological property no longer holds.</p>
<p>The discussed topological property has mathematically general and far-reaching consequences. First, it allows to carry over “shapes” from <span class="math notranslate nohighlight">\(D\)</span> to <span class="math notranslate nohighlight">\(N\)</span>. We illustrate it on an example.</p>
<p>Imagine that we have a circle <span class="math notranslate nohighlight">\(C\)</span> in <span class="math notranslate nohighlight">\(D\)</span>-space, of radius <strong>rad</strong> centered at <strong>cent</strong>. We need to find the winners in the <span class="math notranslate nohighlight">\(N\)</span> space for any point in <span class="math notranslate nohighlight">\(C\)</span>. For this purpose we go around <span class="math notranslate nohighlight">\(C\)</span> in <strong>npoi</strong> points equally spaced in the azimuthal angle, and for each one find a winner.</p>
<p><span class="math notranslate nohighlight">\(C\)</span> is parametrized with polar coordinates:</p>
<div class="math notranslate nohighlight">
\[
x_1=r \cos \left( \frac{2\pi \phi}{N}  \right)+c_1, \;\;\;
x_2=r \sin \left( \frac{2\pi \phi}{N}  \right)+c_2.
\]</div>
<p>Going to the mathematical notation to Python we use
<span class="math notranslate nohighlight">\(r=\)</span><strong>rad</strong>, <span class="math notranslate nohighlight">\(\phi\)</span>=<strong>ph</strong>, <span class="math notranslate nohighlight">\(N=\)</span><strong>npoi</strong>, <span class="math notranslate nohighlight">\((c_1,c_2)=\)</span><strong>[cent]</strong>.
The loop over <strong>ph</strong> goes around the circle.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rad</span><span class="o">=</span><span class="mf">0.35</span>                      <span class="c1"># radius of a circle</span>
<span class="n">cent</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>      <span class="c1"># center of the circle</span>
<span class="n">npoi</span><span class="o">=</span><span class="mi">400</span>                      <span class="c1"># number of points in the circle</span>

<span class="n">wins</span><span class="o">=</span><span class="p">[]</span>                       <span class="c1"># table of winners</span>

<span class="k">for</span> <span class="n">ph</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">npoi</span><span class="p">):</span>        <span class="c1"># go around the circle</span>
    <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">rad</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="n">npoi</span><span class="o">*</span><span class="n">ph</span><span class="p">),</span><span class="n">rad</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="n">npoi</span><span class="o">*</span><span class="n">ph</span><span class="p">)])</span><span class="o">+</span><span class="n">cent</span>
                              <span class="c1"># the circle in polar coordinates</span>
    <span class="n">dist</span><span class="o">=</span><span class="p">[</span><span class="n">func</span><span class="o">.</span><span class="n">eucl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">sam</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">)]</span> 
      <span class="c1"># distances from the point on the circle to the neurons in the nxn grid</span>
    <span class="n">ind_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="c1"># winner</span>
    <span class="n">wins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind_min</span><span class="p">)</span>      <span class="c1"># add winner to the table</span>
        
<span class="n">ci</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span>            <span class="c1"># remove duplicates from the table      </span>
</pre></div>
</div>
</div>
</div>
<p>The result of Kohonen’s algorithm is as follows:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_103_0.png" src="../_images/som_103_0.png" />
</div>
</div>
<p>The red neurons are the winners for certain sections of the circle. When we draw these winners alone in the <span class="math notranslate nohighlight">\(N\)</span> space (keep in mind we are going from <span class="math notranslate nohighlight">\(D\)</span> to <span class="math notranslate nohighlight">\(N\)</span>), we get</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.3</span><span class="p">,</span><span class="mf">2.3</span><span class="p">),</span><span class="n">dpi</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ci</span><span class="o">//</span><span class="mi">10</span><span class="p">,</span><span class="n">ci</span><span class="o">%</span><span class="k">10</span>,c=&#39;red&#39;,s=5)

        
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$i$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$j$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">11</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/som_105_0.png" src="../_images/som_105_0.png" />
</div>
</div>
<p>This looks pretty much as a (rough and discrete) circle. Note that in our example we only have <span class="math notranslate nohighlight">\(n^2=100\)</span> pixels to our disposal - a very low resolution. The image would look better and better with an increasing <span class="math notranslate nohighlight">\(n\)</span>. At some point one would reach the 10M pixel resolution of typical camera, and then the image would seem smooth! We have carried over our circle from <span class="math notranslate nohighlight">\(D\)</span> into <span class="math notranslate nohighlight">\(N\)</span>.</p>
<div class="important admonition">
<p class="admonition-title">Vision</p>
<p>The topological property, such as the one in the discussed Kohonen mappings, has a prime importance in our vision system and the perception of objects. Shapes are carried over from the retina to the visual cortex and are not “warped up” on the way!</p>
</div>
<p>Another key topological feature is the preservation of <strong>connectedness</strong>. If an area <span class="math notranslate nohighlight">\(A\)</span> in <span class="math notranslate nohighlight">\(D\)</span> is connected (so to speak, is in one piece), then its image <span class="math notranslate nohighlight">\(v(A)\)</span> in <span class="math notranslate nohighlight">\(N\)</span> is also connected (we ignore the desired rigor here as to what “connected” means in a discrete space and rely on intuition). So things do not get “torn into pieces” when transforming from <span class="math notranslate nohighlight">\(D\)</span> to <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Note that the discussed topological features need not be present when the dimensionality is reduced, as in our previous examples. Take for instance the bottom right panel of <a class="reference internal" href="#kohstory-fig"><span class="std std-numref">Fig. 14</span></a>. There, many neighboring pairs of the Voronoi areas correspond to distant indices, so it is no longer true that <span class="math notranslate nohighlight">\(v(d_1)\)</span> and <span class="math notranslate nohighlight">\(v(d_2)\)</span> in <span class="math notranslate nohighlight">\(N\)</span> are close for close <span class="math notranslate nohighlight">\(d_1\)</span> and <span class="math notranslate nohighlight">\(d_2\)</span> in <span class="math notranslate nohighlight">\(D\)</span>, as these points may belong to different Voronoi areas with <strong>distant</strong> indices.</p>
<p>For that case, our example with the circle looks like this:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_111_0.png" src="../_images/som_111_0.png" />
</div>
</div>
<p>When we go subsequently along the <strong>grid indices</strong> (i.e. along the blue connecting line), taking <span class="math notranslate nohighlight">\(i=1,2,\dots,100\)</span>, we obtain the plot below. We can see the image of our circle (red dots) as a bunch of <strong>disconnected</strong> red sections. The circle is torn into pieces, the <strong>topology is not preserved!</strong></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_113_0.png" src="../_images/som_113_0.png" />
</div>
</div>
<p>Here is the summarizing statement (NB not made sufficiently clear in the literature):</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Topological features of Kohonen’s maps hold for equal dimensionalities of the input space and the neuron grid, <span class="math notranslate nohighlight">\(n=k\)</span>, and in general do not hold for the reduced dimensionality cases, <span class="math notranslate nohighlight">\(k&lt;n\)</span>.</p>
</div>
</section>
<section id="lateral-inhibition">
<span id="lat-lab"></span><h2>Lateral inhibition<a class="headerlink" href="#lateral-inhibition" title="Permalink to this headline">¶</a></h2>
<p>In the last topic of these lectures, we return to the issue of how the competition for the “winner” is realized in ANNs. Up to now (cf. section <a class="reference internal" href="unsupervised.html#inn-sec"><span class="std std-ref">Interpretation via neural networks</span></a>), we have just been using the minimum (or maximum, when the signal was extended to a hyperphere) in the output, though this is embarrassingly outside of the neural framework. Such an inspection of which neuron yields the strongest signal would require an “external wizard”, or some sort of a control unit. Mathematically, it is easy to imagine, but the challenge is to build it from neurons within the rules of the game.</p>
<p>Actually, if the neurons in a layer “talk” to one another, we can have a “contest” from which a winner may emerge. In particular, an architecture as in <a class="reference internal" href="#lat-fig"><span class="std std-numref">Fig. 17</span></a> allows for an arrangement of competition and a natural realization of a <strong>winner-take-most</strong> mechanism.</p>
<p>The type of models as presented below is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield networks</a>. Note that we depart here from the <strong>feed-forward</strong> limitation of <a class="reference internal" href="intro.html#ffnn-fig"><span class="std std-numref">Fig. 3</span></a> and allow for a recursive, or feed-back character.</p>
<figure class="align-default" id="lat-fig">
<a class="reference internal image-reference" href="../_images/lat3.png"><img alt="../_images/lat3.png" src="../_images/lat3.png" style="width: 220px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Network with inter-neuron couplings used for modeling lateral inhibition. All the neurons are connected to one another in both directions (lines without arrows).</span><a class="headerlink" href="#lat-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Neuron number <span class="math notranslate nohighlight">\(i\)</span> receives the signal <span class="math notranslate nohighlight">\(s_i = x w_i\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is the input (the same for all the neurons), and <span class="math notranslate nohighlight">\(w_i\)</span> is the weight of neuron <span class="math notranslate nohighlight">\(i\)</span>. The neuron produces output <span class="math notranslate nohighlight">\(y_i\)</span>, where now a part of it is sent to neurons <span class="math notranslate nohighlight">\(j\)</span> as <span class="math notranslate nohighlight">\(F_{ji} y_i\)</span>. Here <span class="math notranslate nohighlight">\(F_{ij}\)</span> denotes the coupling strength (we assume <span class="math notranslate nohighlight">\(F_{ii}=0\)</span> - no self coupling). Reciprocally, neuron <span class="math notranslate nohighlight">\(i\)</span> also receives output from neurons <span class="math notranslate nohighlight">\(j\)</span> in the form <span class="math notranslate nohighlight">\(F_{ij} y_j\)</span>. The summation over all the neurons yields</p>
<div class="math notranslate nohighlight">
\[ 
y_i = s_i + \sum_{j\neq i} F_{ij} y_j, 
\]</div>
<p>which in the matrix notation becomes <span class="math notranslate nohighlight">\( y = s + F y\)</span>, or <span class="math notranslate nohighlight">\(y(I-F)=s\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix. Solving for <span class="math notranslate nohighlight">\(y\)</span> gives formally</p>
<div class="math notranslate nohighlight" id="equation-eq-lat">
<span class="eqno">(4)<a class="headerlink" href="#equation-eq-lat" title="Permalink to this equation">¶</a></span>\[y= (I-F)^{-1} s.\]</div>
<p>One needs to model appropriately the coupling matrix <span class="math notranslate nohighlight">\(F\)</span>. We take</p>
<p><span class="math notranslate nohighlight">\( F_ {ii} = \)</span> 0,</p>
<p><span class="math notranslate nohighlight">\( F_ {ij} = - a \exp (- | i-j | / b) ~~ \)</span> for <span class="math notranslate nohighlight">\( i \neq j \)</span>, <span class="math notranslate nohighlight">\( ~~ a, b&gt; 0 \)</span>,</p>
<p>i.e. assume attenuation (negative feedback), which is strongest for close neighbors and decreases with distance. The decrease is controlled by a characteristic scale <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>The Python implementation is straightforward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ns</span> <span class="o">=</span> <span class="mi">30</span><span class="p">;</span>       <span class="c1"># number of neurons</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>         <span class="c1"># parameter controlling the decrease of damping with distance</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>         <span class="c1"># magnitude of damping</span>

<span class="n">F</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="n">a</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">j</span><span class="p">)</span><span class="o">/</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">)]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">)])</span> 
                    <span class="c1"># exponential fall-off</span>
    
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">F</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>       <span class="c1"># no self-coupling</span>
    
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_122_0.png" src="../_images/som_122_0.png" />
</div>
</div>
<p>We assume a bell-shaped Lorentzian input signal <span class="math notranslate nohighlight">\(s\)</span>, with a maximum in the middle neuron. The width is controlled with <strong>D</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span><span class="o">=</span><span class="mi">3</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">D</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="n">ns</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">D</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">)])</span> <span class="c1"># Lorentzian function</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we solve Eq. <a class="reference internal" href="#equation-eq-lat">(4)</a> via inverting the <span class="math notranslate nohighlight">\((I-F)\)</span> matrix, performed with the numpy <strong>linalg.inv</strong> function. Recall that <strong>dot</strong> multiplies matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">invF</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span><span class="o">-</span><span class="n">F</span><span class="p">)</span> <span class="c1"># matrix inversion</span>
<span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">invF</span><span class="p">,</span><span class="n">s</span><span class="p">)</span>                      <span class="c1"># multiplication</span>
<span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">/</span><span class="n">y</span><span class="p">[</span><span class="mi">15</span><span class="p">]</span>                             <span class="c1"># normalization (inessential) </span>
</pre></div>
</div>
</div>
</div>
<p>What follows is actually quite remarkable: the output signal <span class="math notranslate nohighlight">\(y\)</span> becomes much narrower from the input signal <span class="math notranslate nohighlight">\(s\)</span>. This may be interpreted as a realization of the “winner-take-all” scenario. The winner “damped” he guys around him, so he puts himself on airs! The effect is smooth, with the signal visibly sharpened.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/som_128_0.png" src="../_images/som_128_0.png" />
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Lateral inhibition</p>
<p>The damping of the response of neighboring neurons is called <strong>lateral inhibition</strong>. It was discovered in neurobiological networks <span id="id1">[<a class="reference internal" href="conclusion.html#id14">HR72</a>]</span>.</p>
</div>
<p>The presented model is certainly too simplistic to be realistic from the point of view of biological networks. Also, it yields unnatural negative signal outside of the central peak (which we can remove with rectification). Nevertheless, the setup shows a possible way to achieve the “winner competition”, essential for unsupervised learning: One needs to allow for the competing neurons to interact.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Actually, <strong>pyramidal neurons</strong>, present i.a. in the neocortex, have as many as a few thousand dendritic spines and do realize a scenario with numerous synaptic connections. They are believed <a class="reference external" href="https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/">Quantamagazine</a> to play a crucial role in learning and cognition processes.</p>
</div>
<figure class="align-default" id="pyr-fig">
<a class="reference internal image-reference" href="../_images/smi32-pic.jpg"><img alt="../_images/smi32-pic.jpg" src="../_images/smi32-pic.jpg" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Image of pyramidal neurons (from <a class="reference external" href="http://brainmaps.org/index.php?p=screenshots">brainmaps.org</a>)</span><a class="headerlink" href="#pyr-fig" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="warning admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(~\)</span></p>
<ol class="simple">
<li><p>Construct a Kohonen mapping form a <strong>disjoint</strong> 2D shape into a 2D grid of neurons.</p></li>
<li><p>Construct a Kohonen mapping for a case where the points in the input space are not distributed uniformly, but denser in some regions.</p></li>
<li><p>Create, for a number of countries, fictitious flags which have two colors (hence are described with 6 RGB numbers). Construct a Kohonen map into a 2-dim. grid. Plot the resulting <span class="math notranslate nohighlight">\(U\)</span>-matrix and draw conclusions.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lateral_inhibition">Lateral inhibition</a> has “side-effects” seen in optical delusions. Describe the <a class="reference external" href="https://en.wikipedia.org/wiki/Mach_bands">Mach illusion</a>, programming it in Python.</p></li>
</ol>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="unsupervised.html" title="previous page">Unsupervised learning</a>
    <a class='right-next' id="next-link" href="conclusion.html" title="next page">Concluding remarks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Wojciech Broniowski<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>